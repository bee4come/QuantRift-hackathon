"""
ç©å®¶æ•°æ®ç®¡ç†å™¨ - å¼‚æ­¥å‡†å¤‡å’Œç¼“å­˜Player-Packæ•°æ®

æ¶æ„:
1. ç”¨æˆ·æœç´¢ç©å®¶åï¼Œåå°ç«‹å³å¼€å§‹æ‹‰å–å’Œè®¡ç®—æ‰€æœ‰agentéœ€è¦çš„æ•°æ®
2. ç‚¹å‡»Agentå¡ç‰‡æ—¶ï¼Œæ£€æŸ¥æ•°æ®çŠ¶æ€ï¼Œç­‰å¾…å‡†å¤‡å®Œæˆåè°ƒç”¨agent
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta
from collections import defaultdict
import numpy as np
from enum import Enum

from .riot_client import riot_client
from src.core.statistical_utils import wilson_confidence_interval, winsorize
from src.utils.id_mappings import get_champion_name


class DataStatus(str, Enum):
    """æ•°æ®å‡†å¤‡çŠ¶æ€"""
    NOT_STARTED = "not_started"
    FETCHING_MATCHES = "fetching_matches"
    FETCHING_TIMELINES = "fetching_timelines"
    CALCULATING_METRICS = "calculating_metrics"
    COMPLETED = "completed"
    FAILED = "failed"


class PlayerDataJob:
    """ç©å®¶æ•°æ®å‡†å¤‡ä»»åŠ¡"""

    def __init__(self, puuid: str, region: str, game_name: str, tag_line: str, days: int = 365):
        self.puuid = puuid
        self.region = region
        self.game_name = game_name
        self.tag_line = tag_line
        self.days = days  # æ”¹ä¸ºdaysè€Œä¸æ˜¯count
        self.status = DataStatus.NOT_STARTED
        self.progress = 0.0  # 0.0 - 1.0
        self.error: Optional[str] = None
        self.started_at = datetime.utcnow()
        self.completed_at: Optional[datetime] = None
        self.player_pack: Optional[Dict[str, Any]] = None
        self.matches_data: List[Dict[str, Any]] = []  # ä¿å­˜åŸå§‹matchæ•°æ®
        self.timelines_data: List[Dict[str, Any]] = []  # ä¿å­˜timelineæ•°æ®ä¾›timeline_deep_diveä½¿ç”¨

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "puuid": self.puuid,
            "region": self.region,
            "days": self.days,  # æ”¹ä¸ºdays
            "status": self.status,
            "progress": self.progress,
            "error": self.error,
            "started_at": self.started_at.isoformat(),
            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
            "has_data": self.player_pack is not None
        }


class PlayerDataManager:
    """
    ç©å®¶æ•°æ®ç®¡ç†å™¨

    èŒè´£:
    1. å¼‚æ­¥æ‹‰å–Riot APIæ•°æ® (match + timeline)
    2. è®¡ç®—metricså¹¶ç”ŸæˆPlayer-Packæ ¼å¼æ•°æ®
    3. ç¼“å­˜ç»“æœä¾›agentä½¿ç”¨
    4. æä¾›æ•°æ®çŠ¶æ€æŸ¥è¯¢æ¥å£
    """

    def __init__(self, cache_dir: Path = None):
        self.jobs: Dict[str, PlayerDataJob] = {}  # {puuid: PlayerDataJob}
        # ä½¿ç”¨agentæœŸæœ›çš„ç›®å½•ç»“æ„
        self.cache_dir = cache_dir or Path("data/player_packs")
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Boots item IDs (for time_to_core calculation)
        self.boots_ids = {1001, 3006, 3009, 3020, 3047, 3111, 3117, 3158}

        # âš¡ å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶åŒæ—¶è¿›è¡Œçš„APIè¯·æ±‚æ•°é‡
        # 5ä¸ªAPI key Ã— 1800 req/10s = 9000 req/10s ç†è®ºä¸Šé™
        # ä½†è¦è€ƒè™‘ç½‘ç»œå»¶è¿Ÿï¼Œè®¾ç½®200å¹¶å‘æ¯”è¾ƒåˆç†
        self.semaphore = asyncio.Semaphore(200)

    async def prepare_player_data(
        self,
        puuid: str,
        region: str,
        game_name: str,
        tag_line: str,
        days: int = 365
    ) -> PlayerDataJob:
        """
        å¼‚æ­¥å‡†å¤‡ç©å®¶æ•°æ®

        Args:
            puuid: ç©å®¶PUUID
            region: æœåŠ¡å™¨åŒºåŸŸ
            game_name: æ¸¸æˆåç§°
            tag_line: æ ‡ç­¾
            days: æ‹‰å–è¿‡å»å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤365å¤©ï¼‰

        Returns:
            PlayerDataJobå¯¹è±¡ï¼ˆåå°ç»§ç»­å¤„ç†ï¼‰
        """
        # Check if there's an existing task for this player
        if puuid in self.jobs:
            job = self.jobs[puuid]
            # If task is in progress (not COMPLETED or FAILED), reuse it
            if job.status not in [DataStatus.COMPLETED, DataStatus.FAILED]:
                print(f"ğŸ”„ Task already in progress for {game_name}#{tag_line}, status: {job.status.value}")
                return job
            # If task completed with same time range within 1 minute, reuse cache
            elif (job.status == DataStatus.COMPLETED and
                  job.days == days and
                  job.completed_at and
                  (datetime.utcnow() - job.completed_at) < timedelta(minutes=1)):
                print(f"âœ… Reusing recent cache for {game_name}#{tag_line} (completed {(datetime.utcnow() - job.completed_at).seconds}s ago)")
                return job

        # Create new task (always fetch latest match list from Riot API)
        print(f"ğŸ†• Creating new data fetch task for {game_name}#{tag_line} (past {days} days)")
        job = PlayerDataJob(puuid, region, game_name, tag_line, days)
        self.jobs[puuid] = job

        # Start background task
        asyncio.create_task(self._fetch_and_calculate(job, game_name, tag_line))

        return job

    async def _fetch_and_calculate(self, job: PlayerDataJob, game_name: str, tag_line: str):
        """
        åå°ä»»åŠ¡ï¼šæ‹‰å–æ•°æ®å¹¶è®¡ç®—metrics
        """
        try:
            print(f"\nğŸ”„ Starting player data preparation: {game_name}#{tag_line}")
            print(f"   PUUID: {job.puuid[:30]}...")
            print(f"   Time range: Past {job.days} days")

            # é˜¶æ®µ1: æ‹‰å–match list (åŸºäºæ—¶é—´èŒƒå›´è‡ªåŠ¨æ£€æµ‹)
            job.status = DataStatus.FETCHING_MATCHES
            job.progress = 0.1

            match_ids = await self._fetch_all_match_ids(
                puuid=job.puuid,
                platform=job.region,
                days=job.days
            )

            if not match_ids:
                raise Exception(f"No matches found for {game_name}#{tag_line}")

            print(f"âœ… Retrieved {len(match_ids)} matches")
            job.progress = 0.3

            # âš¡ é˜¶æ®µ2-A: åªæ‹‰å–match details (pipelineä¼˜åŒ–ç¬¬ä¸€é˜¶æ®µ)
            job.status = DataStatus.FETCHING_MATCHES
            print(f"âš¡ Pipeline optimization: Fetching matches first, timelines in background")

            matches_data = []

            # ğŸš€ åˆ†æ‰¹å¹¶è¡Œå¤„ç†matches
            batch_size = 50
            total_batches = (len(match_ids) + batch_size - 1) // batch_size

            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(match_ids))
                batch_match_ids = match_ids[start_idx:end_idx]

                import time
                batch_start = time.time()
                print(f"   ğŸ“¦ Batch {batch_idx + 1}/{total_batches}: Fetching {len(batch_match_ids)} matches...")

                # å¹¶è¡Œæ‹‰å–æœ¬æ‰¹æ¬¡çš„matches
                batch_tasks = [self._fetch_match(match_id, job.region) for match_id in batch_match_ids]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                batch_duration = time.time() - batch_start

                # æ”¶é›†ç»“æœ
                batch_success = 0
                for result in batch_results:
                    if isinstance(result, Exception):
                        print(f"      âš ï¸  Skipping failed match: {result}")
                        continue
                    if result:
                        matches_data.append(result)
                        batch_success += 1

                print(f"      âœ… Batch successful {batch_success}/{len(batch_match_ids)} matches (took: {batch_duration:.1f}s)")

                # æ›´æ–°è¿›åº¦ï¼ˆ0.3-0.7åŒºé—´ï¼‰
                progress = 0.3 + (0.4 * (batch_idx + 1) / total_batches)
                job.progress = progress

            print(f"âœ… Match fetch complete: {len(matches_data)} matches")
            job.progress = 0.7

            # é˜¶æ®µ3: è®¡ç®—metricså¹¶ç”ŸæˆPlayer-Pack (ä½¿ç”¨é»˜è®¤time_to_core)
            job.status = DataStatus.CALCULATING_METRICS

            import time
            calc_start = time.time()
            print(f"\nâ±ï¸  Starting metrics calculation (time_to_core using default values)...")

            player_packs = self._generate_player_pack(
                puuid=job.puuid,
                game_name=job.game_name,
                tag_line=job.tag_line,
                matches_data=matches_data,
                timelines_data=[]  # âš¡ ç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨timeline
            )

            calc_duration = time.time() - calc_start
            print(f"â±ï¸  Calculation complete, took: {calc_duration:.2f} seconds")

            # âœ… ä¿å­˜æœ€æ–°çš„packåˆ°job.player_pack (ç”¨äºå‰ç«¯æ˜¾ç¤º)
            job.player_pack = player_packs[-1] if player_packs else {}
            job.matches_data = matches_data  # ğŸ’¾ ä¿å­˜matchesæ•°æ®ä¾›timelineåˆ†æä½¿ç”¨
            job.progress = 1.0
            job.status = DataStatus.COMPLETED
            job.completed_at = datetime.utcnow()

            # âœ… ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜ (agentæœŸæœ›çš„æ ¼å¼: packs_dir/{puuid}/pack_{patch}.json)
            player_dir = self.cache_dir / job.puuid
            player_dir.mkdir(parents=True, exist_ok=True)

            total_patches = len(player_packs)
            total_games = sum(pack['total_games'] for pack in player_packs)

            # âœ… ä¸ºæ¯ä¸ªpatchä¿å­˜å•ç‹¬æ–‡ä»¶
            for pack in player_packs:
                patch = pack['patch']
                cache_file = player_dir / f"pack_{patch}.json"

                # âœ… åªæœ‰å½“æ–°æ•°æ® >= ç°æœ‰æ•°æ®æ—¶æ‰è¦†ç›–ï¼ˆé˜²æ­¢å°countè¯·æ±‚è¦†ç›–å¤§æ•°æ®é›†ï¼‰
                should_save = True
                if cache_file.exists():
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            existing_pack = json.load(f)
                        existing_games = existing_pack.get('total_games', 0)
                        new_games = pack['total_games']

                        if new_games < existing_games:
                            should_save = False
                            print(f"â­ï¸  Skipping save pack_{patch}.json: Existing data more complete ({existing_games} games vs {new_games} games)")
                    except Exception as e:
                        print(f"âš ï¸  Cannot read existing pack_{patch}.json, will overwrite: {e}")

                if should_save:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(pack, f, indent=2, ensure_ascii=False)
                    print(f"âœ… Saved pack_{patch}.json: {pack['total_games']} games")

            # Save matches_data to disk for Timeline Deep Dive
            matches_file = player_dir / "matches_data.json"
            try:
                with open(matches_file, 'w', encoding='utf-8') as f:
                    json.dump(matches_data, f, indent=2, ensure_ascii=False)
                print(f"âœ… Saved matches_data.json: {len(matches_data)} match details")
            except Exception as e:
                print(f"âš ï¸  Failed to save matches_data.json: {e}")

            print(f"âœ… Data preparation complete (phase 1): {game_name}#{tag_line}")
            print(f"   Total games: {total_games}")
            print(f"   Patches: {total_patches}")
            print(f"   Cache location: {player_dir}")
            print(f"   âš¡ 65% of agents can now use the data")

            # âš¡ é˜¶æ®µ2-B: åå°æ‹‰å–timelineså¹¶æ›´æ–°time_to_core
            print(f"\nğŸ”„ Starting background task: Fetching timelines...")
            asyncio.create_task(
                self._fetch_timelines_background(
                    match_ids=match_ids,
                    region=job.region,
                    puuid=job.puuid,
                    player_dir=player_dir
                )
            )

        except Exception as e:
            print(f"âŒ Data preparation failed: {e}")
            job.status = DataStatus.FAILED
            job.error = str(e)
            job.completed_at = datetime.utcnow()

    async def _fetch_all_match_ids(self, puuid: str, platform: str, days: int = 365) -> List[str]:
        """æ ¹æ®æ—¶é—´èŒƒå›´æ‹‰å–æ‰€æœ‰match IDsï¼Œçªç ´counté™åˆ¶

        Args:
            puuid: Player PUUID
            platform: Platform code (e.g., 'na1')
            days: è¿‡å»å¤šå°‘å¤©ï¼ˆé»˜è®¤365å¤©ï¼‰

        Returns:
            List of match IDs within the time range
        """
        from datetime import datetime, timedelta

        # è®¡ç®—æ—¶é—´èŒƒå›´ï¼ˆUnixæ—¶é—´æˆ³ï¼Œç§’ï¼‰
        end_time = int(datetime.utcnow().timestamp())
        start_time = int((datetime.utcnow() - timedelta(days=days)).timestamp())

        print(f"   ğŸ“… Time range: Past {days} days")
        print(f"   ğŸ• Start: {datetime.fromtimestamp(start_time)}")
        print(f"   ğŸ• End: {datetime.fromtimestamp(end_time)}")

        all_match_ids = []
        start_index = 0
        batch_size = 100  # Riot APIå•æ¬¡æœ€å¤šè¿”å›100åœº

        while True:
            print(f"   ğŸ“¥ Fetching matches {start_index}-{start_index + batch_size}...")

            # ä½¿ç”¨æ—¶é—´èŒƒå›´æŸ¥è¯¢
            batch = await riot_client.get_match_history(
                puuid=puuid,
                platform=platform,
                count=batch_size,
                start=start_index,
                start_time=start_time,  # æ·»åŠ æ—¶é—´è¿‡æ»¤
                end_time=end_time,
                queue_id=420  # Ranked Solo/Duo
            )

            if not batch or len(batch) == 0:
                # æ²¡æœ‰æ›´å¤šæ¯”èµ›äº†
                print(f"   âœ… All available matches fetched: {len(all_match_ids)} matches")
                break

            all_match_ids.extend(batch)
            print(f"   âœ… Batch retrieved {len(batch)} matches, total {len(all_match_ids)} matches")

            # å¦‚æœè¿”å›æ•°é‡å°‘äºè¯·æ±‚æ•°é‡ï¼Œè¯´æ˜å·²ç»åˆ°æœ«å°¾äº†
            if len(batch) < batch_size:
                print(f"   â„¹ï¸  Reached end of player match history")
                break

            start_index += len(batch)

        return all_match_ids

    async def _fetch_match(self, match_id: str, platform: str):
        """åªæ‹‰å–å•åœºmatch details (pipelineä¼˜åŒ–ç¬¬ä¸€é˜¶æ®µ)

        Args:
            match_id: Match ID
            platform: Platform code (e.g., 'na1'), will be converted to regional routing internally
        """
        try:
            # Convert platform to regional routing
            PLATFORM_TO_REGION = {
                "na1": "americas", "br1": "americas", "la1": "americas", "la2": "americas",
                "euw1": "europe", "eun1": "europe", "tr1": "europe", "ru": "europe",
                "kr": "asia", "jp1": "asia",
                "oc1": "sea", "ph2": "sea", "sg2": "sea", "th2": "sea", "tw2": "sea", "vn2": "sea",
            }
            region = PLATFORM_TO_REGION.get(platform.lower(), "americas")

            # âš¡ ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            async with self.semaphore:
                match_data = await riot_client.get_match_details(match_id=match_id, region=region)
                return match_data

        except Exception as e:
            print(f"âš ï¸  Failed to fetch match {match_id}: {e}")
            return None

    async def _fetch_timeline(self, match_id: str, platform: str):
        """åªæ‹‰å–å•åœºtimeline (pipelineä¼˜åŒ–ç¬¬äºŒé˜¶æ®µ)

        Args:
            match_id: Match ID
            platform: Platform code (e.g., 'na1'), will be converted to regional routing internally
        """
        try:
            # Convert platform to regional routing
            PLATFORM_TO_REGION = {
                "na1": "americas", "br1": "americas", "la1": "americas", "la2": "americas",
                "euw1": "europe", "eun1": "europe", "tr1": "europe", "ru": "europe",
                "kr": "asia", "jp1": "asia",
                "oc1": "sea", "ph2": "sea", "sg2": "sea", "th2": "sea", "tw2": "sea", "vn2": "sea",
            }
            region = PLATFORM_TO_REGION.get(platform.lower(), "americas")

            # âš¡ ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            async with self.semaphore:
                timeline_data = await riot_client.get_match_timeline(match_id=match_id, region=region)
                return timeline_data

        except Exception as e:
            print(f"âš ï¸  Failed to fetch timeline {match_id}: {e}")
            return None

    def _generate_player_pack(
        self,
        puuid: str,
        game_name: str,
        tag_line: str,
        matches_data: List[Dict],
        timelines_data: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        ä»matchå’Œtimelineæ•°æ®ç”ŸæˆPlayer-Pack

        Returns:
            {
                "puuid": str,
                "generation_timestamp": str,
                "total_games": int,
                "by_cr": [
                    {
                        "champ_id": int,
                        "role": str,
                        "games": int,
                        "wins": int,
                        "losses": int,
                        "p_hat": float,
                        "p_hat_ci": [lower, upper],
                        "kda_adj": float,
                        "obj_rate": float,
                        "cp_25": float,
                        "build_core": [item_ids],
                        "avg_time_to_core": float,
                        "rune_keystone": int,
                        "effective_n": int,
                        "governance_tag": str
                    }
                ]
            }
        """
        import time
        t0 = time.time()

        # åˆ›å»ºtimelineæ˜ å°„
        timelines_map = {t['metadata']['matchId']: t for t in timelines_data}
        print(f"     â±ï¸  Creating timeline mapping: {time.time()-t0:.3f}s")

        # âœ… æŒ‰(patch, champ_id, role)èšåˆæ•°æ®
        patch_cr_data = defaultdict(lambda: defaultdict(list))

        # ğŸ“Š æ·»åŠ è¿‡æ»¤ç»Ÿè®¡
        filter_stats = {
            'total_matches': len(matches_data),
            'player_not_found': 0,
            'invalid_role': 0,
            'processed': 0
        }

        # ğŸ” è®°å½•å‰3ä¸ªè¢«è¿‡æ»¤çš„matchï¼ˆç”¨äºè°ƒè¯•ï¼‰
        filtered_matches_debug = []

        t1 = time.time()
        for match in matches_data:
            match_id = match['metadata']['matchId']
            timeline = timelines_map.get(match_id)

            # âœ… æå–patchç‰ˆæœ¬
            game_version = match['info'].get('gameVersion', '0.0.0.0')
            patch = '.'.join(game_version.split('.')[:2])  # "15.1.123.456" â†’ "15.1"

            # ğŸ”„ æ”¹ç”¨gameName#tagLineåŒ¹é…ï¼ˆæ›´å¯é ï¼‰
            player_data = None
            for p in match['info']['participants']:
                # åŒæ—¶æ”¯æŒPUUIDå’ŒgameName#tagLineåŒ¹é…
                puuid_match = p.get('puuid') == puuid
                # Case-insensitive name matching (Riot API may return different casing)
                name_match = (p.get('riotIdGameName', '').lower() == game_name.lower() and
                             p.get('riotIdTagline', '').lower() == tag_line.lower())

                if puuid_match or name_match:
                    player_data = p
                    break

            if not player_data:
                filter_stats['player_not_found'] += 1
                # ğŸ” è®°å½•å‰3ä¸ªè¢«è¿‡æ»¤çš„matchç”¨äºè°ƒè¯•
                if len(filtered_matches_debug) < 3:
                    filtered_matches_debug.append({
                        'match_id': match_id,
                        'reason': 'player_not_found',
                        'queue_id': match['info'].get('queueId'),
                        'target': f'{game_name}#{tag_line}',
                        'participants_names': [f"{p.get('riotIdGameName', '?')}#{p.get('riotIdTagline', '?')}"
                                              for p in match['info']['participants'][:3]]
                    })
                continue

            champ_id = player_data['championId']
            role = player_data['teamPosition']

            if not role or role == 'Invalid':
                filter_stats['invalid_role'] += 1
                continue

            # æå–å•åœºç»Ÿè®¡
            game_stats = self._extract_game_stats(
                player_data=player_data,
                match_data=match,
                timeline_data=timeline
            )

            # âœ… æŒ‰(patch, champ_id, role)å­˜å‚¨
            key = (champ_id, role)
            patch_cr_data[patch][key].append(game_stats)
            filter_stats['processed'] += 1

        print(f"     â±ï¸  Data extraction loop ({len(matches_data)} matches): {time.time()-t1:.3f}s")
        print(f"     ğŸ“Š Filter statistics:")
        print(f"        - Total matches: {filter_stats['total_matches']}")
        print(f"        - Player not found: {filter_stats['player_not_found']}")
        print(f"        - Invalid role: {filter_stats['invalid_role']}")
        print(f"        - âœ… Successfully processed: {filter_stats['processed']}")

        # ğŸ” è¾“å‡ºç©å®¶åŒ¹é…è°ƒè¯•ä¿¡æ¯
        if filtered_matches_debug:
            print(f"     ğŸ” Debug: First {len(filtered_matches_debug)} filtered matches player name comparison:")
            print(f"        Target player: {game_name}#{tag_line}")
            for i, fm in enumerate(filtered_matches_debug, 1):
                print(f"        Match {i} (ID: {fm['match_id'][:20]}..., QueueID: {fm['queue_id']}):")
                print(f"          Participant sample: {fm['participants_names']}")

        # âœ… ä¸ºæ¯ä¸ªpatchç”Ÿæˆä¸€ä¸ªpack
        t2 = time.time()
        packs = []

        for patch, cr_data in sorted(patch_cr_data.items()):
            # è®¡ç®—æ¯ä¸ª(champ_id, role)çš„èšåˆmetrics
            by_cr = []

            for (champ_id, role), games_stats in cr_data.items():
                if not games_stats:
                    continue

                games = len(games_stats)
                wins = sum(1 for g in games_stats if g['win'])
                losses = games - wins

                # Win rate with Wilson CI
                p_hat = wins / games if games > 0 else 0.0
                _, ci_lower, ci_upper = wilson_confidence_interval(wins, games)

                # KDA adjusted (winsorized)
                kda_values = [g['kda_adj'] for g in games_stats]
                kda_winsorized = winsorize(kda_values)
                kda_adj = np.mean(kda_winsorized) if kda_winsorized else 0.0

                # Objective rate
                obj_rate = np.mean([g['obj_rate'] for g in games_stats])

                # Combat power at 25min
                cp_25 = np.mean([g['cp_25'] for g in games_stats])

                # Build core: most common items
                item_counts = defaultdict(int)
                for g in games_stats:
                    for item_id in g['items_at_25']:
                        item_counts[item_id] += 1
                build_core = sorted(item_counts.keys(), key=lambda x: item_counts[x], reverse=True)[:3]

                # Average time to core
                avg_time_to_core = np.mean([g['time_to_core'] for g in games_stats])

                # Most common rune keystone
                rune_counts = defaultdict(int)
                for g in games_stats:
                    rune_counts[g['rune_keystone']] += 1
                rune_keystone = max(rune_counts.keys(), key=lambda x: rune_counts[x]) if rune_counts else 0

                # Governance tag
                if games >= 100:
                    governance_tag = "CONFIDENT"
                elif games >= 30:
                    governance_tag = "CAUTION"
                else:
                    governance_tag = "CONTEXT"

                by_cr.append({
                    "champ_id": champ_id,
                    "role": role,
                    "games": games,
                    "wins": wins,
                    "losses": losses,
                    "p_hat": round(p_hat, 4),
                    "p_hat_ci": [round(ci_lower, 4), round(ci_upper, 4)],
                    "kda_adj": round(kda_adj, 2),
                    "obj_rate": round(obj_rate, 3),
                    "cp_25": round(cp_25, 1),
                    "build_core": build_core,
                    "avg_time_to_core": round(avg_time_to_core, 2),
                    "rune_keystone": rune_keystone,
                    "effective_n": games,
                    "governance_tag": governance_tag
                })

            # âœ… åˆ›å»ºè¯¥patchçš„pack
            pack = {
                "puuid": puuid,
                "patch": patch,  # âœ… çœŸå®patchç‰ˆæœ¬
                "generation_timestamp": datetime.utcnow().isoformat(),
                "total_games": sum(entry['games'] for entry in by_cr),
                "by_cr": by_cr
            }
            packs.append(pack)

        print(f"     â±ï¸  Aggregation calculation + Pack generation: {time.time()-t2:.3f}s")

        return packs

    def _extract_game_stats(
        self,
        player_data: Dict,
        match_data: Dict,
        timeline_data: Optional[Dict]
    ) -> Dict[str, Any]:
        """ä»å•åœºæ¯”èµ›æå–ç»Ÿè®¡æ•°æ®"""
        # Basic stats
        win = player_data['win']
        kills = player_data['kills']
        deaths = player_data['deaths']
        assists = player_data['assists']

        # KDA adjusted
        kda_adj = (kills + 0.7 * assists) / (deaths + 1)

        # Objective participation rate
        team_baron = player_data.get('challenges', {}).get('teamBaronKills', 1)
        obj_rate = (
            player_data.get('baronKills', 0) +
            player_data.get('dragonKills', 0) +
            player_data.get('turretKills', 0)
        ) / max(1, team_baron)

        # Combat power at 25min (proxy)
        game_duration_min = match_data['info']['gameDuration'] / 60.0
        gold_earned = player_data['goldEarned']
        cp_25 = (gold_earned / game_duration_min * 25) if game_duration_min > 0 else gold_earned

        # Items at end
        items_at_25 = [
            player_data.get(f'item{i}', 0)
            for i in range(6)
            if player_data.get(f'item{i}', 0) != 0
        ]

        # Time to core
        time_to_core = 30.0  # default
        if timeline_data:
            time_to_core = self._calculate_time_to_core(
                timeline_data,
                player_data['participantId']
            )

        # Rune keystone
        rune_keystone = player_data['perks']['styles'][0]['selections'][0]['perk']

        return {
            'win': win,
            'kills': kills,
            'deaths': deaths,
            'assists': assists,
            'kda_adj': kda_adj,
            'obj_rate': obj_rate,
            'cp_25': cp_25,
            'items_at_25': items_at_25,
            'time_to_core': time_to_core,
            'rune_keystone': rune_keystone,
            'game_duration': game_duration_min
        }

    def _calculate_time_to_core(self, timeline_data: Dict, participant_id: int) -> float:
        """è®¡ç®—time to core (minutes)"""
        core_items_found = []

        for frame in timeline_data.get('info', {}).get('frames', []):
            timestamp_min = frame['timestamp'] / 60000.0

            for event in frame.get('events', []):
                if event.get('type') == 'ITEM_PURCHASED' and event.get('participantId') == participant_id:
                    item_id = event.get('itemId', 0)

                    if item_id not in self.boots_ids and item_id > 2000:
                        if item_id not in [item['id'] for item in core_items_found]:
                            core_items_found.append({'id': item_id, 'time': timestamp_min})

                            if len(core_items_found) >= 2:
                                return core_items_found[1]['time']

        # æœªæ‰¾åˆ°2ä»¶æ ¸å¿ƒè£…å¤‡
        if timeline_data['info']['frames']:
            return timeline_data['info']['frames'][-1]['timestamp'] / 60000.0
        return 30.0

    def get_status(self, puuid: str) -> Dict[str, Any]:
        """è·å–æ•°æ®å‡†å¤‡çŠ¶æ€"""
        if puuid not in self.jobs:
            return {"status": DataStatus.NOT_STARTED}

        return self.jobs[puuid].to_dict()

    async def wait_for_data(self, puuid: str, timeout: int = 120) -> Optional[Dict[str, Any]]:
        """
        ç­‰å¾…æ•°æ®å‡†å¤‡å®Œæˆ

        Args:
            puuid: ç©å®¶PUUID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Player-Packæ•°æ®ï¼Œæˆ–Noneï¼ˆå¦‚æœå¤±è´¥/è¶…æ—¶ï¼‰
        """
        if puuid not in self.jobs:
            return None

        job = self.jobs[puuid]

        # å¦‚æœå·²ç»å®Œæˆï¼Œç›´æ¥è¿”å›
        if job.status == DataStatus.COMPLETED and job.player_pack:
            return job.player_pack

        # ç­‰å¾…å®Œæˆ
        for _ in range(timeout):
            if job.status == DataStatus.COMPLETED:
                return job.player_pack
            elif job.status == DataStatus.FAILED:
                return None

            await asyncio.sleep(1)

        # è¶…æ—¶
        print(f"âš ï¸  Data wait timeout: {puuid}")
        return None

    def get_data(self, puuid: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å‡†å¤‡å¥½çš„æ•°æ®ï¼ˆåŒæ­¥ï¼Œä¸ç­‰å¾…ï¼‰

        Returns:
            Player-Packæ•°æ®ï¼Œæˆ–None
        """
        if puuid not in self.jobs:
            # å°è¯•ä»ç£ç›˜ç¼“å­˜åŠ è½½
            cache_file = self.cache_dir / puuid / "pack_current.json"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return None

        job = self.jobs[puuid]
        return job.player_pack if job.status == DataStatus.COMPLETED else None

    def get_packs_dir(self, puuid: str) -> Optional[str]:
        """
        è·å–player packsç›®å½•è·¯å¾„ï¼ˆç»™agentä½¿ç”¨ï¼‰

        Returns:
            packs_dirè·¯å¾„ï¼Œæˆ–None
        """
        player_dir = self.cache_dir / puuid
        if player_dir.exists():
            return str(player_dir)
        return None

    def get_role_stats(self, puuid: str) -> List[Dict[str, Any]]:
        """
        ä»Player-Packä¸­æå–roleç»Ÿè®¡æ•°æ®ï¼ˆä¼˜å…ˆä»summary.jsonï¼Œå¦åˆ™èšåˆæ‰€æœ‰packæ–‡ä»¶ï¼‰

        Returns:
            [
                {"role": "TOP", "games": 10, "wins": 6, "win_rate": 60.0, "avg_kda": 3.5},
                ...
            ]
        """
        player_dir = self.cache_dir / puuid

        if not player_dir.exists():
            return []

        try:
            # å…ˆå°è¯•ä»summary.jsonè¯»å–
            summary_file = player_dir / "summary.json"
            if summary_file.exists():
                with open(summary_file, 'r', encoding='utf-8') as f:
                    summary = json.load(f)
                by_cr_data = summary.get("by_cr", [])
            else:
                # å¦‚æœsummaryä¸å­˜åœ¨ï¼Œä»æ‰€æœ‰pack_*.jsonæ–‡ä»¶èšåˆ
                by_cr_data = []
                pack_files = sorted(player_dir.glob("pack_*.json"))
                for pack_file in pack_files:
                    with open(pack_file, 'r', encoding='utf-8') as f:
                        pack = json.load(f)
                        by_cr_data.extend(pack.get("by_cr", []))

            if not by_cr_data:
                return []

            # ä» by_cr èšåˆ role ç»Ÿè®¡
            role_stats_dict = defaultdict(lambda: {
                "games": 0,
                "wins": 0,
                "total_kda": 0.0
            })

            for cr in by_cr_data:
                role = cr.get("role", "UNKNOWN")
                role_stats_dict[role]["games"] += cr.get("games", 0)
                role_stats_dict[role]["wins"] += cr.get("wins", 0)

                # ä½¿ç”¨ kda_adj * games ä½œä¸ºåŠ æƒKDA
                if "kda_adj" in cr:
                    role_stats_dict[role]["total_kda"] += cr["kda_adj"] * cr.get("games", 0)

            # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
            role_stats = []
            for role, stats in role_stats_dict.items():
                games = stats["games"]
                wins = stats["wins"]
                win_rate = (wins / games * 100) if games > 0 else 0
                avg_kda = (stats["total_kda"] / games) if games > 0 else 0

                role_stats.append({
                    "role": role,
                    "games": games,
                    "wins": wins,
                    "win_rate": round(win_rate, 1),
                    "avg_kda": round(avg_kda, 2)
                })

            # æŒ‰æ¸¸æˆæ•°æ’åº
            role_stats.sort(key=lambda x: x["games"], reverse=True)
            return role_stats

        except Exception as e:
            print(f"âš ï¸  Failed to get role stats: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_best_champions(self, puuid: str, limit: int = 5) -> List[Dict[str, Any]]:
        """
        ä»Player-Packä¸­æå–æœ€ä½³è‹±é›„æ•°æ®ï¼ˆæŒ‰æ¸¸æˆæ•°æ’åºï¼‰

        Returns:
            [
                {"champ_id": 202, "games": 50, "wins": 30, "win_rate": 60.0, "avg_kda": 3.5},
                ...
            ]
        """
        player_dir = self.cache_dir / puuid
        if not player_dir.exists():
            return []

        try:
            # èšåˆæ‰€æœ‰packæ–‡ä»¶ä¸­çš„championæ•°æ®
            champion_stats = defaultdict(lambda: {
                "games": 0,
                "wins": 0,
                "total_kda": 0.0
            })

            pack_files = sorted(player_dir.glob("pack_*.json"))
            for pack_file in pack_files:
                with open(pack_file, 'r', encoding='utf-8') as f:
                    pack = json.load(f)
                    by_cr_data = pack.get("by_cr", [])

                    for cr in by_cr_data:
                        champ_id = cr.get("champ_id")
                        if not champ_id:
                            continue

                        games = cr.get("games", 0)
                        wins = cr.get("wins", 0)
                        kda_adj = cr.get("kda_adj", 0)

                        champion_stats[champ_id]["games"] += games
                        champion_stats[champ_id]["wins"] += wins
                        champion_stats[champ_id]["total_kda"] += kda_adj * games

            # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
            best_champions = []
            for champ_id, stats in champion_stats.items():
                games = stats["games"]
                wins = stats["wins"]
                win_rate = (wins / games * 100) if games > 0 else 0
                avg_kda = (stats["total_kda"] / games) if games > 0 else 0

                # è·å–è‹±é›„åç§°
                champion_name = get_champion_name(champ_id)

                best_champions.append({
                    "champ_id": champ_id,
                    "name": champion_name,
                    "games": games,
                    "wins": wins,
                    "win_rate": round(win_rate, 1),
                    "avg_kda": round(avg_kda, 2)
                })

            # æŒ‰æ¸¸æˆæ•°æ’åºï¼Œè¿”å›å‰Nä¸ª
            best_champions.sort(key=lambda x: x["games"], reverse=True)
            return best_champions[:limit]

        except Exception as e:
            print(f"âš ï¸  Failed to get best champions: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_recent_matches(self, puuid: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘çš„æ¯”èµ›åˆ—è¡¨ï¼ˆç”¨äºtimelineåˆ†æï¼‰

        ä¼˜å…ˆè¿”å›æœ‰timelineæ–‡ä»¶çš„matches

        Args:
            puuid: ç©å®¶PUUID
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            List[Dict]: æ¯”èµ›ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ£€æŸ¥timelineç›®å½•ï¼Œè·å–æœ‰timelineæ–‡ä»¶çš„match_ids
            player_dir = self.cache_dir / puuid
            timelines_dir = player_dir / "timelines"

            available_match_ids = set()
            if timelines_dir.exists():
                for timeline_file in timelines_dir.glob("*_timeline.json"):
                    match_id = timeline_file.stem.replace("_timeline", "")
                    available_match_ids.add(match_id)

            # Get matches data from job (memory) or matches_data.json (disk)
            job = self.jobs.get(puuid)
            matches_data = None

            if job and job.matches_data:
                # Use in-memory data if available
                matches_data = job.matches_data
            else:
                # Try to load from disk cache
                matches_file = player_dir / "matches_data.json"
                if matches_file.exists():
                    try:
                        with open(matches_file, 'r', encoding='utf-8') as f:
                            matches_data = json.load(f)
                        print(f"âœ… Loaded matches_data.json from disk: {len(matches_data)} matches")
                    except Exception as e:
                        print(f"âš ï¸  Failed to load matches_data.json: {e}")

            # If no matches data available, return empty list
            if not matches_data:
                print(f"âš ï¸  No matches data available for {puuid}")
                return []

            # Convert to frontend format, only include matches with timeline files
            matches = []
            for match in matches_data:
                try:
                    # æå–åŸºç¡€ä¿¡æ¯
                    match_id = match['metadata']['matchId']

                    # ğŸ” åªè¿”å›æœ‰timelineæ–‡ä»¶çš„matches
                    if match_id not in available_match_ids:
                        continue

                    game_creation = match['info']['gameCreation']
                    game_duration = match['info']['gameDuration']

                    # æ‰¾åˆ°å½“å‰ç©å®¶çš„æ•°æ®
                    participants = match['info']['participants']
                    player_data = None
                    for participant in participants:
                        if participant.get('puuid') == puuid:
                            player_data = participant
                            break

                    if not player_data:
                        continue

                    # æå–ç©å®¶æ•°æ®
                    champion_id = player_data.get('championId', 0)
                    champion_name = get_champion_name(champion_id)
                    role = player_data.get('teamPosition', 'UNKNOWN')
                    win = player_data.get('win', False)
                    kills = player_data.get('kills', 0)
                    deaths = player_data.get('deaths', 0)
                    assists = player_data.get('assists', 0)

                    matches.append({
                        'match_id': match_id,
                        'game_creation': game_creation,
                        'game_duration': game_duration,
                        'champion_id': champion_id,
                        'champion_name': champion_name,
                        'role': role,
                        'win': win,
                        'kills': kills,
                        'deaths': deaths,
                        'assists': assists
                    })

                except Exception as e:
                    print(f"âš ï¸  Failed to parse match: {e}")
                    continue

            return matches

        except Exception as e:
            print(f"âš ï¸  Failed to get recent matches: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def _fetch_timelines_background(
        self,
        match_ids: List[str],
        region: str,
        puuid: str,
        player_dir: Path
    ):
        """
        åå°ä»»åŠ¡ï¼šæ‹‰å–timelineså¹¶æ›´æ–°time_to_core

        è¿™ä¸ªä»»åŠ¡åœ¨ç¬¬ä¸€é˜¶æ®µå®Œæˆåè¿è¡Œï¼Œä¸é˜»å¡agentä½¿ç”¨æ•°æ®
        """
        try:
            import time
            bg_start = time.time()
            print(f"\nğŸ”„ Background task started: Fetching {len(match_ids)} timelines")

            timelines_data = []

            # åˆ†æ‰¹æ‹‰å–timelines
            batch_size = 50
            total_batches = (len(match_ids) + batch_size - 1) // batch_size

            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(match_ids))
                batch_match_ids = match_ids[start_idx:end_idx]

                batch_start = time.time()
                print(f"   ğŸ“¦ Background batch {batch_idx + 1}/{total_batches}: Fetching {len(batch_match_ids)} timelines...")

                # å¹¶è¡Œæ‹‰å–æœ¬æ‰¹æ¬¡çš„timelines
                batch_tasks = [self._fetch_timeline(match_id, region) for match_id in batch_match_ids]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                batch_duration = time.time() - batch_start

                # æ”¶é›†ç»“æœ
                batch_success = 0
                for result in batch_results:
                    if isinstance(result, Exception):
                        continue
                    if result:
                        timelines_data.append(result)
                        batch_success += 1

                print(f"      âœ… Background batch successful {batch_success}/{len(batch_match_ids)} timelines (took: {batch_duration:.1f}s)")

            bg_duration = time.time() - bg_start
            print(f"âœ… Background timeline fetch complete: {len(timelines_data)} timelines (total time: {bg_duration:.1f}s)")

            # ä¿å­˜timelineæ•°æ®åˆ°jobï¼ˆä¾›APIä½¿ç”¨ï¼‰
            job = self.jobs.get(puuid)
            if job:
                job.timelines_data = timelines_data
                print(f"ğŸ’¾ Timeline data saved to job: {len(timelines_data)} timelines")

            # ä¿å­˜timelineæ•°æ®åˆ°ç£ç›˜ï¼ˆä¾›timeline_deep_dive agentä½¿ç”¨ï¼‰
            timelines_dir = player_dir / "timelines"
            timelines_dir.mkdir(exist_ok=True)

            saved_count = 0
            skipped_count = 0  # ğŸ›¡ï¸ ç»Ÿè®¡è¢«è¿‡æ»¤çš„timelineæ•°é‡

            for timeline in timelines_data:
                try:
                    match_id = timeline['metadata']['matchId']
                    participants = timeline['metadata']['participants']

                    # ğŸ›¡ï¸ ã€å…³é”®éªŒè¯ã€‘ï¼šåªä¿å­˜åŒ…å«ç›®æ ‡ç©å®¶çš„timeline
                    if puuid not in participants:
                        skipped_count += 1
                        print(f"âš ï¸  Skipping timeline {match_id}: Does not contain target player")
                        print(f"     Target PUUID: {puuid[:40]}...")
                        print(f"     First participant: {participants[0][:40]}...")
                        continue

                    # âœ… éªŒè¯é€šè¿‡ï¼Œä¿å­˜timeline
                    timeline_file = timelines_dir / f"{match_id}_timeline.json"
                    with open(timeline_file, 'w', encoding='utf-8') as f:
                        json.dump(timeline, f, indent=2, ensure_ascii=False)
                    saved_count += 1

                except Exception as e:
                    print(f"âš ï¸  Failed to save timeline: {e}")

            print(f"ğŸ’¾ Timeline files saved to disk: {saved_count}/{len(timelines_data)} timelines")
            if skipped_count > 0:
                print(f"ğŸ›¡ï¸ Data security: Filtered out {skipped_count} timelines not belonging to target player")

            # æ›´æ–°player packsä¸­çš„time_to_core
            print(f"ğŸ”„ Updating time_to_core...")
            await self._update_time_to_core(puuid, player_dir, timelines_data)
            print(f"âœ… Background task complete, timeline_deep_dive agent can now use full data")

        except Exception as e:
            print(f"âš ï¸  Background timeline fetch failed (does not affect other agents): {e}")

    async def _update_time_to_core(
        self,
        puuid: str,
        player_dir: Path,
        timelines_data: List[Dict]
    ):
        """
        æ›´æ–°å·²ä¿å­˜çš„player packsï¼Œç”¨çœŸå®çš„time_to_coreæ›¿æ¢é»˜è®¤å€¼
        """
        try:
            # åˆ›å»ºtimelineæ˜ å°„: match_id -> timeline_data
            timelines_map = {t['metadata']['matchId']: t for t in timelines_data}
            print(f"   ğŸ“Š Available timeline data: {len(timelines_map)} timelines")

            # ä»jobä¸­è·å–åŸå§‹matches_data
            job = self.jobs.get(puuid)
            if not job or not job.matches_data:
                print(f"   âš ï¸  Original match data not found, cannot update time_to_core")
                return

            matches_data = job.matches_data
            print(f"   ğŸ“Š Available match data: {len(matches_data)} matches")

            # ä¸ºæ¯åœºæ¯”èµ›è®¡ç®—çœŸå®çš„time_to_core
            match_time_to_core = {}  # {match_id: {participant_id: time_to_core}}

            for match_data in matches_data:
                match_id = match_data['metadata']['matchId']
                if match_id not in timelines_map:
                    continue  # æ²¡æœ‰timelineæ•°æ®ï¼Œè·³è¿‡

                timeline_data = timelines_map[match_id]

                # ä¸ºè¿™åœºæ¯”èµ›çš„æ¯ä¸ªç©å®¶è®¡ç®—time_to_core
                for participant in match_data['info']['participants']:
                    if participant['puuid'] != puuid:
                        continue  # åªå¤„ç†ç›®æ ‡ç©å®¶

                    participant_id = participant['participantId']
                    time_to_core = self._calculate_time_to_core(timeline_data, participant_id)

                    if match_id not in match_time_to_core:
                        match_time_to_core[match_id] = {}
                    match_time_to_core[match_id][participant_id] = time_to_core

            print(f"   âœ… Calculation complete: {len(match_time_to_core)} matches time_to_core")

            # æ›´æ–°æ¯ä¸ªpackæ–‡ä»¶
            updated_packs = 0
            for pack_file in player_dir.glob("pack_*.json"):
                with open(pack_file, 'r', encoding='utf-8') as f:
                    pack = json.load(f)

                # é‡æ–°èšåˆtime_to_coreï¼ˆæŒ‰champion-roleåˆ†ç»„ï¼‰
                cr_time_to_core = defaultdict(list)  # {(champ_id, role): [time_to_core_values]}

                for match_data in matches_data:
                    match_id = match_data['metadata']['matchId']
                    game_version = match_data['info']['gameVersion']
                    patch = '.'.join(game_version.split('.')[:2])

                    # åªå¤„ç†å½“å‰packçš„patch
                    if patch != pack['patch']:
                        continue

                    if match_id not in match_time_to_core:
                        continue

                    for participant in match_data['info']['participants']:
                        if participant['puuid'] != puuid:
                            continue

                        champ_id = participant['championId']
                        role = participant.get('teamPosition', 'UTILITY')
                        participant_id = participant['participantId']

                        if participant_id in match_time_to_core[match_id]:
                            time_to_core = match_time_to_core[match_id][participant_id]
                            cr_time_to_core[(champ_id, role)].append(time_to_core)

                # æ›´æ–°packä¸­çš„avg_time_to_core
                modified = False
                for cr_entry in pack.get('by_cr', []):
                    champ_id = cr_entry['champ_id']
                    role = cr_entry['role']

                    if (champ_id, role) in cr_time_to_core:
                        times = cr_time_to_core[(champ_id, role)]
                        avg_time = np.mean(times)
                        old_time = cr_entry.get('avg_time_to_core', 30.0)
                        cr_entry['avg_time_to_core'] = round(avg_time, 2)

                        if abs(avg_time - old_time) > 0.1:  # æœ‰å®è´¨æ€§å˜åŒ–
                            modified = True

                # å¦‚æœæœ‰ä¿®æ”¹ï¼Œä¿å­˜packæ–‡ä»¶
                if modified:
                    with open(pack_file, 'w', encoding='utf-8') as f:
                        json.dump(pack, f, indent=2, ensure_ascii=False)
                    updated_packs += 1
                    print(f"   âœ… Updated {pack_file.name}: avg_time_to_core updated")

            print(f"   âœ… Background update complete: {updated_packs} pack files updated")

        except Exception as e:
            print(f"âš ï¸  Failed to update time_to_core: {e}")
            import traceback
            traceback.print_exc()


# å…¨å±€å•ä¾‹
player_data_manager = PlayerDataManager()
