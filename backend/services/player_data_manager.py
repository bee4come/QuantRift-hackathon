"""
ç©å®¶æ•°æ®ç®¡ç†å™¨ - å¼‚æ­¥å‡†å¤‡å’Œç¼“å­˜Player-Packæ•°æ®

æ¶æ„:
1. ç”¨æˆ·æœç´¢ç©å®¶åï¼Œåå°ç«‹å³å¼€å§‹æ‹‰å–å’Œè®¡ç®—æ‰€æœ‰agentéœ€è¦çš„æ•°æ®
2. ç‚¹å‡»Agentå¡ç‰‡æ—¶ï¼Œæ£€æŸ¥æ•°æ®çŠ¶æ€ï¼Œç­‰å¾…å‡†å¤‡å®Œæˆåè°ƒç”¨agent
"""
import asyncio
import json
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime, timedelta, timezone
from collections import defaultdict
import numpy as np
from enum import Enum

from .riot_client import riot_client
from src.core.statistical_utils import wilson_confidence_interval, winsorize
from src.utils.id_mappings import get_champion_name


class DataStatus(str, Enum):
    """æ•°æ®å‡†å¤‡çŠ¶æ€"""
    NOT_STARTED = "not_started"
    FETCHING_MATCHES = "fetching_matches"
    FETCHING_TIMELINES = "fetching_timelines"
    CALCULATING_METRICS = "calculating_metrics"
    COMPLETED = "completed"
    FAILED = "failed"


class PlayerDataJob:
    """ç©å®¶æ•°æ®å‡†å¤‡ä»»åŠ¡"""

    def __init__(self, puuid: str, region: str, game_name: str, tag_line: str, days: int = 365):
        self.puuid = puuid
        self.region = region
        self.game_name = game_name
        self.tag_line = tag_line
        self.days = days  # æ”¹ä¸ºdaysè€Œä¸æ˜¯count
        self.status = DataStatus.NOT_STARTED
        self.progress = 0.0  # 0.0 - 1.0
        self.error: Optional[str] = None
        self.started_at = datetime.utcnow()
        self.completed_at: Optional[datetime] = None
        self.player_pack: Optional[Dict[str, Any]] = None
        self.matches_data: List[Dict[str, Any]] = []  # ä¿å­˜åŸå§‹matchæ•°æ®
        self.timelines_data: List[Dict[str, Any]] = []  # ä¿å­˜timelineæ•°æ®ä¾›timeline_deep_diveä½¿ç”¨

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            "puuid": self.puuid,
            "region": self.region,
            "days": self.days,  # æ”¹ä¸ºdays
            "status": self.status,
            "progress": self.progress,
            "error": self.error,
            "started_at": self.started_at.isoformat(),
            "completed_at": self.completed_at.isoformat() if self.completed_at else None,
            "has_data": self.player_pack is not None
        }


class PlayerDataManager:
    """
    ç©å®¶æ•°æ®ç®¡ç†å™¨

    èŒè´£:
    1. å¼‚æ­¥æ‹‰å–Riot APIæ•°æ® (match + timeline)
    2. è®¡ç®—metricså¹¶ç”ŸæˆPlayer-Packæ ¼å¼æ•°æ®
    3. ç¼“å­˜ç»“æœä¾›agentä½¿ç”¨
    4. æä¾›æ•°æ®çŠ¶æ€æŸ¥è¯¢æ¥å£
    """

    def __init__(self, cache_dir: Path = None):
        self.jobs: Dict[str, PlayerDataJob] = {}  # {puuid: PlayerDataJob}
        # ä½¿ç”¨agentæœŸæœ›çš„ç›®å½•ç»“æ„
        self.cache_dir = cache_dir or Path("data/player_packs")
        self.cache_dir.mkdir(parents=True, exist_ok=True)

        # Boots item IDs (for time_to_core calculation)
        self.boots_ids = {1001, 3006, 3009, 3020, 3047, 3111, 3117, 3158}

        # âš¡ å¹¶å‘æ§åˆ¶ï¼šé™åˆ¶åŒæ—¶è¿›è¡Œçš„APIè¯·æ±‚æ•°é‡
        # 5ä¸ªAPI key Ã— 1800 req/10s = 9000 req/10s ç†è®ºä¸Šé™
        # ä½†è¦è€ƒè™‘ç½‘ç»œå»¶è¿Ÿï¼Œè®¾ç½®200å¹¶å‘æ¯”è¾ƒåˆç†
        self.semaphore = asyncio.Semaphore(200)

    async def prepare_player_data(
        self,
        puuid: str,
        region: str,
        game_name: str,
        tag_line: str,
        days: int = 365
    ) -> PlayerDataJob:
        """
        å¼‚æ­¥å‡†å¤‡ç©å®¶æ•°æ®

        Args:
            puuid: ç©å®¶PUUID
            region: æœåŠ¡å™¨åŒºåŸŸ
            game_name: æ¸¸æˆåç§°
            tag_line: æ ‡ç­¾
            days: æ‹‰å–è¿‡å»å¤šå°‘å¤©çš„æ•°æ®ï¼ˆé»˜è®¤365å¤©ï¼‰

        Returns:
            PlayerDataJobå¯¹è±¡ï¼ˆåå°ç»§ç»­å¤„ç†ï¼‰
        """
        # Check if there's an existing task for this player
        if puuid in self.jobs:
            job = self.jobs[puuid]
            # If task is in progress (not COMPLETED or FAILED), reuse it
            if job.status not in [DataStatus.COMPLETED, DataStatus.FAILED]:
                print(f"ğŸ”„ Task already in progress for {game_name}#{tag_line}, status: {job.status.value}")
                return job
            # If task completed with same time range within 1 minute, reuse cache
            elif (job.status == DataStatus.COMPLETED and
                  job.days == days and
                  job.completed_at and
                  (datetime.utcnow() - job.completed_at) < timedelta(minutes=1)):
                print(f"âœ… Reusing recent cache for {game_name}#{tag_line} (completed {(datetime.utcnow() - job.completed_at).seconds}s ago)")
                return job

        # Create new task (always fetch latest match list from Riot API)
        print(f"ğŸ†• Creating new data fetch task for {game_name}#{tag_line} (past {days} days)")
        job = PlayerDataJob(puuid, region, game_name, tag_line, days)
        self.jobs[puuid] = job

        # Start background task
        asyncio.create_task(self._fetch_and_calculate(job, game_name, tag_line))

        return job

    async def _fetch_and_calculate(self, job: PlayerDataJob, game_name: str, tag_line: str):
        """
        åå°ä»»åŠ¡ï¼šæ‹‰å–æ•°æ®å¹¶è®¡ç®—metrics
        """
        try:
            print(f"\nğŸ”„ Starting player data preparation: {game_name}#{tag_line}")
            print(f"   PUUID: {job.puuid[:30]}...")
            print(f"   Time range: Patch 14.1 (2024-01-09) to today")

            # é˜¶æ®µ1: æ‹‰å–match list (åŸºäºæ—¶é—´èŒƒå›´è‡ªåŠ¨æ£€æµ‹)
            job.status = DataStatus.FETCHING_MATCHES
            job.progress = 0.1

            match_ids = await self._fetch_all_match_ids(
                puuid=job.puuid,
                platform=job.region
            )

            if not match_ids:
                raise Exception(f"No matches found for {game_name}#{tag_line}")

            print(f"âœ… Retrieved {len(match_ids)} matches")
            job.progress = 0.3

            # âš¡ é˜¶æ®µ2-A: åªæ‹‰å–match details (pipelineä¼˜åŒ–ç¬¬ä¸€é˜¶æ®µ)
            job.status = DataStatus.FETCHING_MATCHES
            print(f"âš¡ Pipeline optimization: Fetching matches first, timelines in background")

            matches_data = []

            # ğŸš€ åˆ†æ‰¹å¹¶è¡Œå¤„ç†matches
            batch_size = 50
            total_batches = (len(match_ids) + batch_size - 1) // batch_size

            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(match_ids))
                batch_match_ids = match_ids[start_idx:end_idx]

                import time
                batch_start = time.time()
                print(f"   ğŸ“¦ Batch {batch_idx + 1}/{total_batches}: Fetching {len(batch_match_ids)} matches...")

                # å¹¶è¡Œæ‹‰å–æœ¬æ‰¹æ¬¡çš„matches
                batch_tasks = [self._fetch_match(match_id, job.region) for match_id in batch_match_ids]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                batch_duration = time.time() - batch_start

                # æ”¶é›†ç»“æœ
                batch_success = 0
                for result in batch_results:
                    if isinstance(result, Exception):
                        print(f"      âš ï¸  Skipping failed match: {result}")
                        continue
                    if result:
                        matches_data.append(result)
                        batch_success += 1

                print(f"      âœ… Batch successful {batch_success}/{len(batch_match_ids)} matches (took: {batch_duration:.1f}s)")

                # æ›´æ–°è¿›åº¦ï¼ˆ0.3-0.7åŒºé—´ï¼‰
                progress = 0.3 + (0.4 * (batch_idx + 1) / total_batches)
                job.progress = progress

            print(f"âœ… Match fetch complete: {len(matches_data)} matches")
            job.progress = 0.7

            # é˜¶æ®µ3: è®¡ç®—metricså¹¶ç”ŸæˆPlayer-Pack (ä½¿ç”¨é»˜è®¤time_to_core)
            job.status = DataStatus.CALCULATING_METRICS

            import time
            calc_start = time.time()
            print(f"\nâ±ï¸  Starting metrics calculation (time_to_core using default values)...")

            player_packs = self._generate_player_pack(
                puuid=job.puuid,
                game_name=job.game_name,
                tag_line=job.tag_line,
                matches_data=matches_data,
                timelines_data=[]  # âš¡ ç¬¬ä¸€é˜¶æ®µä¸ä½¿ç”¨timeline
            )

            calc_duration = time.time() - calc_start
            print(f"â±ï¸  Calculation complete, took: {calc_duration:.2f} seconds")

            # âœ… ä¿å­˜æœ€æ–°çš„packåˆ°job.player_pack (ç”¨äºå‰ç«¯æ˜¾ç¤º)
            job.player_pack = player_packs[-1] if player_packs else {}
            job.matches_data = matches_data  # ğŸ’¾ ä¿å­˜matchesæ•°æ®ä¾›timelineåˆ†æä½¿ç”¨
            job.progress = 1.0
            job.status = DataStatus.COMPLETED
            job.completed_at = datetime.utcnow()

            # âœ… ä¿å­˜åˆ°ç£ç›˜ç¼“å­˜ (agentæœŸæœ›çš„æ ¼å¼: packs_dir/{puuid}/pack_{patch}.json)
            player_dir = self.cache_dir / job.puuid
            player_dir.mkdir(parents=True, exist_ok=True)

            total_patches = len(player_packs)
            total_games = sum(pack['total_games'] for pack in player_packs)

            # âœ… Save pack files for each patch and queue_id combination
            # File naming: pack_{patch}_{queue_id}.json (e.g., pack_15.1_420.json for Solo/Duo)
            queue_id_names = {420: 'solo', 440: 'flex', 400: 'normal'}
            
            for pack in player_packs:
                patch = pack['patch']
                queue_id = pack.get('queue_id', 420)  # Default to Solo/Duo if not specified
                queue_name = queue_id_names.get(queue_id, str(queue_id))

                cache_file = player_dir / f"pack_{patch}_{queue_id}.json"

                # âœ… Only overwrite if new data >= existing data (prevent smaller requests from overwriting larger datasets)
                should_save = True
                if cache_file.exists():
                    try:
                        with open(cache_file, 'r', encoding='utf-8') as f:
                            existing_pack = json.load(f)
                        existing_games = existing_pack.get('total_games', 0)
                        new_games = pack['total_games']

                        if new_games < existing_games:
                            should_save = False
                            print(f"â­ï¸  Skipping save pack_{patch}_{queue_id}.json: Existing data more complete ({existing_games} games vs {new_games} games)")
                    except Exception as e:
                        print(f"âš ï¸  Cannot read existing pack_{patch}_{queue_id}.json, will overwrite: {e}")

                if should_save:
                    with open(cache_file, 'w', encoding='utf-8') as f:
                        json.dump(pack, f, indent=2, ensure_ascii=False)
                    print(f"âœ… Saved pack_{patch}_{queue_id}.json ({queue_name}): {pack['total_games']} games")

            # Save individual match files to global pool (shared across players)
            global_matches_dir = Path("data/matches")
            global_matches_dir.mkdir(parents=True, exist_ok=True)

            saved_count = 0
            skipped_count = 0
            match_ids_list = []
            verified_matches_data = []  # Only matches where player is present

            for match in matches_data:
                match_id = match['metadata']['matchId']

                # Verify player is in this match before adding to match_ids
                player_in_match = False
                for participant in match['info']['participants']:
                    if participant.get('puuid') == puuid:
                        player_in_match = True
                        break

                if not player_in_match:
                    print(f"âš ï¸  Skipping {match_id}: Player not found in match")
                    continue

                # Add to verified lists
                match_ids_list.append(match_id)
                verified_matches_data.append(match)

                match_file = global_matches_dir / f"{match_id}.json"

                # Only save if not already exists (avoid duplicate writes)
                if not match_file.exists():
                    try:
                        with open(match_file, 'w', encoding='utf-8') as f:
                            json.dump(match, f, indent=2, ensure_ascii=False)
                        saved_count += 1
                    except Exception as e:
                        print(f"âš ï¸  Failed to save {match_id}.json: {e}")
                else:
                    skipped_count += 1

            print(f"âœ… Match files: {saved_count} saved, {skipped_count} already cached (global pool)")

            # Save match ID list for this player
            match_ids_file = player_dir / "match_ids.json"
            try:
                with open(match_ids_file, 'w', encoding='utf-8') as f:
                    json.dump(match_ids_list, f, indent=2)
                print(f"âœ… Saved match_ids.json: {len(match_ids_list)} verified match IDs")
            except Exception as e:
                print(f"âš ï¸  Failed to save match_ids.json: {e}")

            # Save verified matches_data.json (only matches where player is present)
            matches_file = player_dir / "matches_data.json"
            try:
                with open(matches_file, 'w', encoding='utf-8') as f:
                    json.dump(verified_matches_data, f, indent=2, ensure_ascii=False)
                print(f"âœ… Saved matches_data.json: {len(verified_matches_data)} verified match details")
            except Exception as e:
                print(f"âš ï¸  Failed to save matches_data.json: {e}")

            print(f"âœ… Data preparation complete (phase 1): {game_name}#{tag_line}")
            print(f"   Total games: {total_games}")
            print(f"   Patches: {total_patches}")
            print(f"   Cache location: {player_dir}")
            print(f"   âš¡ 65% of agents can now use the data")

            # âš¡ é˜¶æ®µ2-B: åå°æ‹‰å–timelineså¹¶æ›´æ–°time_to_core
            print(f"\nğŸ”„ Starting background task: Fetching timelines...")
            asyncio.create_task(
                self._fetch_timelines_background(
                    match_ids=match_ids,
                    region=job.region,
                    puuid=job.puuid,
                    player_dir=player_dir
                )
            )

        except Exception as e:
            print(f"âŒ Data preparation failed: {e}")
            job.status = DataStatus.FAILED
            job.error = str(e)
            job.completed_at = datetime.utcnow()

    async def _fetch_all_match_ids(self, puuid: str, platform: str, days: int = None) -> List[str]:
        """ä»patch 14.1å¼€å§‹è·å–åˆ°ä»Šå¤©çš„æ‰€æœ‰æ¯”èµ›æ•°æ®ï¼ˆåŒ…æ‹¬æ‰€æœ‰queueç±»å‹ï¼‰

        Args:
            puuid: Player PUUID
            platform: Platform code (e.g., 'na1')
            days: å·²å¼ƒç”¨ï¼Œä¿ç•™ä»¥å…¼å®¹æ—§ä»£ç 

        Returns:
            List of match IDs (åŒ…å«æ‰€æœ‰queueç±»å‹: 420=Solo/Duo, 440=Flex, 400=Normal)
        """
        # Patch 14.1 start date: 2024-01-09
        patch_14_1_start = datetime(2024, 1, 9, tzinfo=timezone.utc)
        start_timestamp = int(patch_14_1_start.timestamp() * 1000)  # Riot API uses milliseconds
        end_timestamp = int(datetime.now(timezone.utc).timestamp() * 1000)  # Today

        print(f"   ğŸ“Š Fetching all matches from patch 14.1 (2024-01-09) to today (all queue types)")

        all_match_ids = []
        queue_types = [
            (420, "Ranked Solo/Duo"),
            (440, "Ranked Flex"),
            (400, "Normal")
        ]

        for queue_id, queue_name in queue_types:
            print(f"   ğŸ“¥ Fetching {queue_name} matches...")
            start_index = 0
            batch_size = 100  # Riot APIå•æ¬¡æœ€å¤šè¿”å›100åœº
            queue_match_ids = []

            while True:
                print(f"      Fetching {queue_name} matches {start_index}-{start_index + batch_size}...")

                # Fetch with time filter: from patch 14.1 to today
                batch = await riot_client.get_match_history(
                    puuid=puuid,
                    platform=platform,
                    count=batch_size,
                    start=start_index,
                    start_time=start_timestamp,
                    end_time=end_timestamp,
                    queue_id=queue_id
                )

                if not batch or len(batch) == 0:
                    # No more matches available for this queue type
                    print(f"      âœ… All {queue_name} matches fetched: {len(queue_match_ids)} matches")
                    break

                queue_match_ids.extend(batch)
                print(f"      âœ… Batch retrieved {len(batch)} {queue_name} matches, total {len(queue_match_ids)} matches")

                # If returned less than requested, we've reached the end
                if len(batch) < batch_size:
                    print(f"      â„¹ï¸  Reached end of {queue_name} match history")
                    break

                start_index += len(batch)

            all_match_ids.extend(queue_match_ids)
            print(f"   âœ… Total {queue_name} matches: {len(queue_match_ids)}")

        print(f"   âœ… All queue types fetched: {len(all_match_ids)} total matches")
        return all_match_ids

    async def _fetch_match(self, match_id: str, platform: str):
        """åªæ‹‰å–å•åœºmatch details (pipelineä¼˜åŒ–ç¬¬ä¸€é˜¶æ®µ)

        Args:
            match_id: Match ID
            platform: Platform code (e.g., 'na1'), will be converted to regional routing internally
        """
        try:
            # Convert platform to regional routing
            PLATFORM_TO_REGION = {
                "na1": "americas", "br1": "americas", "la1": "americas", "la2": "americas",
                "euw1": "europe", "eun1": "europe", "tr1": "europe", "ru": "europe",
                "kr": "asia", "jp1": "asia",
                "oc1": "sea", "ph2": "sea", "sg2": "sea", "th2": "sea", "tw2": "sea", "vn2": "sea",
            }
            region = PLATFORM_TO_REGION.get(platform.lower(), "americas")

            # âš¡ ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            async with self.semaphore:
                match_data = await riot_client.get_match_details(match_id=match_id, region=region)
                return match_data

        except Exception as e:
            print(f"âš ï¸  Failed to fetch match {match_id}: {e}")
            return None

    async def _fetch_timeline(self, match_id: str, platform: str):
        """åªæ‹‰å–å•åœºtimeline (pipelineä¼˜åŒ–ç¬¬äºŒé˜¶æ®µ)

        Args:
            match_id: Match ID
            platform: Platform code (e.g., 'na1'), will be converted to regional routing internally
        """
        try:
            # Convert platform to regional routing
            PLATFORM_TO_REGION = {
                "na1": "americas", "br1": "americas", "la1": "americas", "la2": "americas",
                "euw1": "europe", "eun1": "europe", "tr1": "europe", "ru": "europe",
                "kr": "asia", "jp1": "asia",
                "oc1": "sea", "ph2": "sea", "sg2": "sea", "th2": "sea", "tw2": "sea", "vn2": "sea",
            }
            region = PLATFORM_TO_REGION.get(platform.lower(), "americas")

            # âš¡ ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘
            async with self.semaphore:
                timeline_data = await riot_client.get_match_timeline(match_id=match_id, region=region)
                return timeline_data

        except Exception as e:
            print(f"âš ï¸  Failed to fetch timeline {match_id}: {e}")
            return None

    def _generate_player_pack(
        self,
        puuid: str,
        game_name: str,
        tag_line: str,
        matches_data: List[Dict],
        timelines_data: List[Dict]
    ) -> List[Dict[str, Any]]:
        """
        ä»matchå’Œtimelineæ•°æ®ç”ŸæˆPlayer-Pack

        Returns:
            {
                "puuid": str,
                "generation_timestamp": str,
                "total_games": int,
                "by_cr": [
                    {
                        "champ_id": int,
                        "role": str,
                        "games": int,
                        "wins": int,
                        "losses": int,
                        "p_hat": float,
                        "p_hat_ci": [lower, upper],
                        "kda_adj": float,
                        "obj_rate": float,
                        "cp_25": float,
                        "build_core": [item_ids],
                        "avg_time_to_core": float,
                        "rune_keystone": int,
                        "effective_n": int,
                        "governance_tag": str
                    }
                ]
            }
        """
        import time
        t0 = time.time()

        # åˆ›å»ºtimelineæ˜ å°„
        timelines_map = {t['metadata']['matchId']: t for t in timelines_data}
        print(f"     â±ï¸  Creating timeline mapping: {time.time()-t0:.3f}s")

        # âœ… æŒ‰(patch, queue_id, champ_id, role)èšåˆæ•°æ®
        # Structure: patch_cr_data[patch][queue_id][(champ_id, role)] = [game_stats]
        patch_cr_data = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))

        # ğŸ“Š æ·»åŠ è¿‡æ»¤ç»Ÿè®¡
        filter_stats = {
            'total_matches': len(matches_data),
            'player_not_found': 0,
            'invalid_role': 0,
            'processed': 0
        }

        # ğŸ” è®°å½•å‰3ä¸ªè¢«è¿‡æ»¤çš„matchï¼ˆç”¨äºè°ƒè¯•ï¼‰
        filtered_matches_debug = []

        t1 = time.time()
        earliest_match_date = None
        latest_match_date = None
        
        # Past Season date range: patch 14.1 (2024-01-09) to patch 14.25 (2025-01-06)
        past_season_start = datetime(2024, 1, 9, tzinfo=timezone.utc)
        past_season_end = datetime(2025, 1, 6, 23, 59, 59, 999000, tzinfo=timezone.utc)
        
        # Past 365 Days: from today - 365 days to today
        today = datetime.now(timezone.utc)
        past_365_days_start = today - timedelta(days=365)
        
        # Track games count for each patch in time ranges
        patch_past_season_games = defaultdict(int)
        patch_past_365_games = defaultdict(int)
        
        for match in matches_data:
            match_id = match['metadata']['matchId']
            timeline = timelines_map.get(match_id)
            
            # Extract queue_id from match
            queue_id = match['info'].get('queueId', 420)  # Default to Solo/Duo if not specified

            # Extract match date from gameCreation timestamp
            game_creation = match['info'].get('gameCreation', 0)
            if game_creation:
                match_date = datetime.fromtimestamp(game_creation / 1000, tz=timezone.utc)
                if earliest_match_date is None or match_date < earliest_match_date:
                    earliest_match_date = match_date
                if latest_match_date is None or match_date > latest_match_date:
                    latest_match_date = match_date

            # âœ… æå–patchç‰ˆæœ¬
            game_version = match['info'].get('gameVersion', '0.0.0.0')
            patch = '.'.join(game_version.split('.')[:2])  # "15.1.123.456" â†’ "15.1"

            # ğŸ”„ æ”¹ç”¨gameName#tagLineåŒ¹é…ï¼ˆæ›´å¯é ï¼‰
            player_data = None
            for p in match['info']['participants']:
                # åŒæ—¶æ”¯æŒPUUIDå’ŒgameName#tagLineåŒ¹é…
                puuid_match = p.get('puuid') == puuid
                # Case-insensitive name matching (Riot API may return different casing)
                name_match = (p.get('riotIdGameName', '').lower() == game_name.lower() and
                             p.get('riotIdTagline', '').lower() == tag_line.lower())

                if puuid_match or name_match:
                    player_data = p
                    break

            if not player_data:
                filter_stats['player_not_found'] += 1
                # ğŸ” è®°å½•å‰3ä¸ªè¢«è¿‡æ»¤çš„matchç”¨äºè°ƒè¯•
                if len(filtered_matches_debug) < 3:
                    filtered_matches_debug.append({
                        'match_id': match_id,
                        'reason': 'player_not_found',
                        'queue_id': match['info'].get('queueId'),
                        'target': f'{game_name}#{tag_line}',
                        'participants_names': [f"{p.get('riotIdGameName', '?')}#{p.get('riotIdTagline', '?')}"
                                              for p in match['info']['participants'][:3]]
                    })
                continue

            champ_id = player_data['championId']
            role = player_data['teamPosition']

            if not role or role == 'Invalid':
                filter_stats['invalid_role'] += 1
                continue

            # Check if this match is in Past Season or Past 365 Days (only count if player is in match)
            if game_creation:
                match_date = datetime.fromtimestamp(game_creation / 1000, tz=timezone.utc)
                # Check Past Season (2024-01-09 to 2025-01-06)
                if past_season_start <= match_date <= past_season_end:
                    patch_past_season_games[patch] += 1
                # Check Past 365 Days
                if match_date >= past_365_days_start:
                    patch_past_365_games[patch] += 1

            # æå–å•åœºç»Ÿè®¡
            game_stats = self._extract_game_stats(
                player_data=player_data,
                match_data=match,
                timeline_data=timeline
            )

            # âœ… æŒ‰(patch, queue_id, champ_id, role)èšåˆæ•°æ®
            key = (champ_id, role, queue_id)
            patch_cr_data[patch][key].append(game_stats)
            filter_stats['processed'] += 1

        print(f"     â±ï¸  Data extraction loop ({len(matches_data)} matches): {time.time()-t1:.3f}s")
        print(f"     ğŸ“Š Filter statistics:")
        print(f"        - Total matches: {filter_stats['total_matches']}")
        print(f"        - Player not found: {filter_stats['player_not_found']}")
        print(f"        - Invalid role: {filter_stats['invalid_role']}")
        print(f"        - âœ… Successfully processed: {filter_stats['processed']}")

        # ğŸ” è¾“å‡ºç©å®¶åŒ¹é…è°ƒè¯•ä¿¡æ¯
        if filtered_matches_debug:
            print(f"     ğŸ” Debug: First {len(filtered_matches_debug)} filtered matches player name comparison:")
            print(f"        Target player: {game_name}#{tag_line}")
            for i, fm in enumerate(filtered_matches_debug, 1):
                print(f"        Match {i} (ID: {fm['match_id'][:20]}..., QueueID: {fm['queue_id']}):")
                print(f"          Participant sample: {fm['participants_names']}")

        # âœ… ä¸ºæ¯ä¸ªpatchç”Ÿæˆä¸€ä¸ªpack
        t2 = time.time()
        packs = []
        
        # Create a mapping of patch to match dates for efficient lookup
        patch_match_dates = defaultdict(lambda: {'earliest': None, 'latest': None})
        for match in matches_data:
            game_version = match['info'].get('gameVersion', '0.0.0.0')
            patch = '.'.join(game_version.split('.')[:2])
            game_creation = match['info'].get('gameCreation', 0)
            if game_creation:
                match_date = datetime.fromtimestamp(game_creation / 1000, tz=timezone.utc)
                if patch_match_dates[patch]['earliest'] is None or match_date < patch_match_dates[patch]['earliest']:
                    patch_match_dates[patch]['earliest'] = match_date
                if patch_match_dates[patch]['latest'] is None or match_date > patch_match_dates[patch]['latest']:
                    patch_match_dates[patch]['latest'] = match_date

        # Generate packs for each patch and queue_id combination
        for patch in sorted(patch_cr_data.keys()):
            patch_queue_data = patch_cr_data[patch]
            
            # Generate a pack for each queue_id
            for queue_id in sorted(patch_queue_data.keys()):
                cr_data = patch_queue_data[queue_id]
                
                # Calculate aggregated metrics for each (champ_id, role)
            by_cr = []

            for (champ_id, role), games_stats in cr_data.items():
                if not games_stats:
                    continue

                games = len(games_stats)
                wins = sum(1 for g in games_stats if g['win'])
                losses = games - wins

                # Win rate with Wilson CI
                p_hat = wins / games if games > 0 else 0.0
                _, ci_lower, ci_upper = wilson_confidence_interval(wins, games)

                # KDA adjusted (winsorized)
                kda_values = [g['kda_adj'] for g in games_stats]
                kda_winsorized = winsorize(kda_values)
                kda_adj = np.mean(kda_winsorized) if kda_winsorized else 0.0

                # Objective rate
                obj_rate = np.mean([g['obj_rate'] for g in games_stats])

                # Combat power at 25min
                cp_25 = np.mean([g['cp_25'] for g in games_stats])

                # Build core: most common items
                item_counts = defaultdict(int)
                for g in games_stats:
                    for item_id in g['items_at_25']:
                        item_counts[item_id] += 1
                build_core = sorted(item_counts.keys(), key=lambda x: item_counts[x], reverse=True)[:3]

                # Average time to core
                avg_time_to_core = np.mean([g['time_to_core'] for g in games_stats])

                # Most common rune keystone
                rune_counts = defaultdict(int)
                for g in games_stats:
                    rune_counts[g['rune_keystone']] += 1
                rune_keystone = max(rune_counts.keys(), key=lambda x: rune_counts[x]) if rune_counts else 0

                # Governance tag
                if games >= 100:
                    governance_tag = "CONFIDENT"
                elif games >= 30:
                    governance_tag = "CAUTION"
                else:
                    governance_tag = "CONTEXT"

                by_cr.append({
                    "champ_id": champ_id,
                    "role": role,
                    "games": games,
                    "wins": wins,
                    "losses": losses,
                    "p_hat": round(p_hat, 4),
                    "p_hat_ci": [round(ci_lower, 4), round(ci_upper, 4)],
                    "kda_adj": round(kda_adj, 2),
                    "obj_rate": round(obj_rate, 3),
                    "cp_25": round(cp_25, 1),
                    "build_core": build_core,
                    "avg_time_to_core": round(avg_time_to_core, 2),
                    "rune_keystone": rune_keystone,
                    "effective_n": games,
                    "governance_tag": governance_tag
                })

                # âœ… Create pack for this patch and queue_id
            pack = {
                "puuid": puuid,
                    "patch": patch,
                    "queue_id": queue_id,  # Store queue_id in pack
                "generation_timestamp": datetime.utcnow().isoformat(),
                "total_games": sum(entry['games'] for entry in by_cr),
                "by_cr": by_cr
            }
            
            # Add match date range for this patch (filtered by queue_id)
            # Calculate date range for this specific queue_id
            queue_earliest = None
            queue_latest = None
            for match in matches_data:
                match_queue_id = match['info'].get('queueId', 420)
                if match_queue_id == queue_id:
                    game_version = match['info'].get('gameVersion', '0.0.0.0')
                    match_patch = '.'.join(game_version.split('.')[:2])
                    if match_patch == patch:
                        game_creation = match['info'].get('gameCreation', 0)
                        if game_creation:
                            match_date = datetime.fromtimestamp(game_creation / 1000, tz=timezone.utc)
                            if queue_earliest is None or match_date < queue_earliest:
                                queue_earliest = match_date
                            if queue_latest is None or match_date > queue_latest:
                                queue_latest = match_date
            
            if queue_earliest:
                pack["earliest_match_date"] = queue_earliest.isoformat()
            if queue_latest:
                pack["latest_match_date"] = queue_latest.isoformat()
            
            # Add games count for Past Season and Past 365 Days (for this queue_id)
                queue_past_season_games = 0
                queue_past_365_games = 0
                for match in matches_data:
                    match_queue_id = match['info'].get('queueId', 420)
                    if match_queue_id == queue_id:
                        game_version = match['info'].get('gameVersion', '0.0.0.0')
                        match_patch = '.'.join(game_version.split('.')[:2])
                        if match_patch == patch:
                            game_creation = match['info'].get('gameCreation', 0)
                            if game_creation:
                                match_date = datetime.fromtimestamp(game_creation / 1000, tz=timezone.utc)
                                # Check Past Season
                                if past_season_start <= match_date <= past_season_end:
                                    queue_past_season_games += 1
                                # Check Past 365 Days
                                if match_date >= past_365_days_start:
                                    queue_past_365_games += 1
                
                pack["past_season_games"] = queue_past_season_games
                pack["past_365_days_games"] = queue_past_365_games
            
            packs.append(pack)

        print(f"     â±ï¸  Aggregation calculation + Pack generation: {time.time()-t2:.3f}s")

        return packs

    def _extract_game_stats(
        self,
        player_data: Dict,
        match_data: Dict,
        timeline_data: Optional[Dict]
    ) -> Dict[str, Any]:
        """ä»å•åœºæ¯”èµ›æå–ç»Ÿè®¡æ•°æ®"""
        # Basic stats
        win = player_data['win']
        kills = player_data['kills']
        deaths = player_data['deaths']
        assists = player_data['assists']

        # KDA adjusted
        kda_adj = (kills + 0.7 * assists) / (deaths + 1)

        # Objective participation rate
        team_baron = player_data.get('challenges', {}).get('teamBaronKills', 1)
        obj_rate = (
            player_data.get('baronKills', 0) +
            player_data.get('dragonKills', 0) +
            player_data.get('turretKills', 0)
        ) / max(1, team_baron)

        # Combat power at 25min (proxy)
        game_duration_min = match_data['info']['gameDuration'] / 60.0
        gold_earned = player_data['goldEarned']
        cp_25 = (gold_earned / game_duration_min * 25) if game_duration_min > 0 else gold_earned

        # Items at end
        items_at_25 = [
            player_data.get(f'item{i}', 0)
            for i in range(6)
            if player_data.get(f'item{i}', 0) != 0
        ]

        # Time to core
        time_to_core = 30.0  # default
        if timeline_data:
            time_to_core = self._calculate_time_to_core(
                timeline_data,
                player_data['participantId']
            )

        # Rune keystone
        rune_keystone = player_data['perks']['styles'][0]['selections'][0]['perk']

        return {
            'win': win,
            'kills': kills,
            'deaths': deaths,
            'assists': assists,
            'kda_adj': kda_adj,
            'obj_rate': obj_rate,
            'cp_25': cp_25,
            'items_at_25': items_at_25,
            'time_to_core': time_to_core,
            'rune_keystone': rune_keystone,
            'game_duration': game_duration_min
        }

    def _calculate_time_to_core(self, timeline_data: Dict, participant_id: int) -> float:
        """è®¡ç®—time to core (minutes)"""
        core_items_found = []

        for frame in timeline_data.get('info', {}).get('frames', []):
            timestamp_min = frame['timestamp'] / 60000.0

            for event in frame.get('events', []):
                if event.get('type') == 'ITEM_PURCHASED' and event.get('participantId') == participant_id:
                    item_id = event.get('itemId', 0)

                    if item_id not in self.boots_ids and item_id > 2000:
                        if item_id not in [item['id'] for item in core_items_found]:
                            core_items_found.append({'id': item_id, 'time': timestamp_min})

                            if len(core_items_found) >= 2:
                                return core_items_found[1]['time']

        # æœªæ‰¾åˆ°2ä»¶æ ¸å¿ƒè£…å¤‡
        if timeline_data['info']['frames']:
            return timeline_data['info']['frames'][-1]['timestamp'] / 60000.0
        return 30.0

    def get_status(self, puuid: str) -> Dict[str, Any]:
        """è·å–æ•°æ®å‡†å¤‡çŠ¶æ€"""
        if puuid not in self.jobs:
            return {"status": DataStatus.NOT_STARTED}

        return self.jobs[puuid].to_dict()

    async def wait_for_data(self, puuid: str, timeout: int = 120) -> Optional[Dict[str, Any]]:
        """
        ç­‰å¾…æ•°æ®å‡†å¤‡å®Œæˆ

        Args:
            puuid: ç©å®¶PUUID
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            Player-Packæ•°æ®ï¼Œæˆ–Noneï¼ˆå¦‚æœå¤±è´¥/è¶…æ—¶ï¼‰
        """
        if puuid not in self.jobs:
            return None

        job = self.jobs[puuid]

        # å¦‚æœå·²ç»å®Œæˆï¼Œç›´æ¥è¿”å›
        if job.status == DataStatus.COMPLETED and job.player_pack:
            return job.player_pack

        # ç­‰å¾…å®Œæˆ
        for _ in range(timeout):
            if job.status == DataStatus.COMPLETED:
                return job.player_pack
            elif job.status == DataStatus.FAILED:
                return None

            await asyncio.sleep(1)

        # è¶…æ—¶
        print(f"âš ï¸  Data wait timeout: {puuid}")
        return None

    def get_data(self, puuid: str) -> Optional[Dict[str, Any]]:
        """
        è·å–å‡†å¤‡å¥½çš„æ•°æ®ï¼ˆåŒæ­¥ï¼Œä¸ç­‰å¾…ï¼‰

        Returns:
            Player-Packæ•°æ®ï¼Œæˆ–None
        """
        if puuid not in self.jobs:
            # å°è¯•ä»ç£ç›˜ç¼“å­˜åŠ è½½
            cache_file = self.cache_dir / puuid / "pack_current.json"
            if cache_file.exists():
                with open(cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            return None

        job = self.jobs[puuid]
        return job.player_pack if job.status == DataStatus.COMPLETED else None

    def get_packs_dir(self, puuid: str) -> Optional[str]:
        """
        è·å–player packsç›®å½•è·¯å¾„ï¼ˆç»™agentä½¿ç”¨ï¼‰

        Returns:
            packs_dirè·¯å¾„ï¼Œæˆ–None
        """
        player_dir = self.cache_dir / puuid
        if player_dir.exists():
            return str(player_dir)
        return None

    def get_role_stats(self, puuid: str, time_range: str = None, queue_id: int = None) -> List[Dict[str, Any]]:
        """
        ä»Player-Packä¸­æå–roleç»Ÿè®¡æ•°æ®ï¼ˆä¼˜å…ˆä»summary.jsonï¼Œå¦åˆ™èšåˆæ‰€æœ‰packæ–‡ä»¶ï¼‰
        
        Args:
            puuid: Player PUUID
            time_range: Time range filter (optional)
            queue_id: Queue ID filter (optional)

        Returns:
            [
                {"role": "TOP", "games": 10, "wins": 6, "win_rate": 60.0, "avg_kda": 3.5},
                ...
            ]
        """
        player_dir = self.cache_dir / puuid

        if not player_dir.exists():
            return []

        try:
            # Calculate time filter if needed
            cutoff_timestamp = None
            cutoff_end_timestamp = None
            
            if time_range == "2024-01-01":
                cutoff_timestamp = datetime(2024, 1, 9, tzinfo=timezone.utc).timestamp()
                cutoff_end_timestamp = datetime(2025, 1, 6, 23, 59, 59, 999000, tzinfo=timezone.utc).timestamp()
            elif time_range == "past-365":
                cutoff_timestamp = (datetime.now(timezone.utc) - timedelta(days=365)).timestamp()
            
            # Build file pattern based on queue_id
            if queue_id is not None:
                pack_pattern = f"pack_*_{queue_id}.json"
            else:
                pack_pattern = "pack_*.json"
            
            pack_files = sorted(player_dir.glob(pack_pattern))
            print(f"ğŸ” [get_role_stats] Looking for packs with pattern: {pack_pattern}, found {len(pack_files)} files")
            print(f"ğŸ” [get_role_stats] Filter params: queue_id={queue_id}, time_range={time_range}")
            
            by_cr_data = []
            for pack_file in pack_files:
                with open(pack_file, 'r', encoding='utf-8') as f:
                    pack = json.load(f)
                
                # Verify queue_id matches if specified
                if queue_id is not None:
                    pack_queue_id = pack.get('queue_id', 420)
                    if pack_queue_id != queue_id:
                        continue
                
                # Apply time range filter
                if cutoff_timestamp:
                    has_match_in_range = False
                    pack_earliest = pack.get("earliest_match_date")
                    pack_latest = pack.get("latest_match_date")
                    
                    print(f"ğŸ” [get_role_stats] Pack {pack_file.name}: earliest={pack_earliest}, latest={pack_latest}, cutoff={cutoff_timestamp}, cutoff_end={cutoff_end_timestamp}")
                    
                    if pack_earliest or pack_latest:
                        if pack_earliest:
                            try:
                                earliest_dt = datetime.fromisoformat(pack_earliest.replace('Z', '+00:00'))
                                if earliest_dt.tzinfo:
                                    earliest_dt = earliest_dt.replace(tzinfo=None)
                                earliest_ts = earliest_dt.timestamp()
                            except Exception as e:
                                print(f"âš ï¸  [get_role_stats] Failed to parse earliest_match_date: {e}")
                                earliest_ts = None
                        else:
                            earliest_ts = None
                            
                        if pack_latest:
                            try:
                                latest_dt = datetime.fromisoformat(pack_latest.replace('Z', '+00:00'))
                                if latest_dt.tzinfo:
                                    latest_dt = latest_dt.replace(tzinfo=None)
                                latest_ts = latest_dt.timestamp()
                            except Exception as e:
                                print(f"âš ï¸  [get_role_stats] Failed to parse latest_match_date: {e}")
                                latest_ts = None
                        else:
                            latest_ts = None
                        
                        if earliest_ts and latest_ts:
                            if cutoff_end_timestamp:
                                if earliest_ts <= cutoff_end_timestamp and latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                                    print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (earliest={earliest_ts}, latest={latest_ts})")
                            else:
                                if latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                                    print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (latest={latest_ts} >= cutoff={cutoff_timestamp})")
                        elif latest_ts:
                            if cutoff_end_timestamp:
                                if latest_ts <= cutoff_end_timestamp and latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                                    print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (latest={latest_ts})")
                            else:
                                if latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                                    print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (latest={latest_ts} >= cutoff={cutoff_timestamp})")
                    else:
                        # Fallback: if pack has no match date info, include it if it has past_365_days_games count
                        # This handles old packs that don't have earliest_match_date/latest_match_date
                        if "past_365_days_games" in pack and pack["past_365_days_games"] > 0:
                            has_match_in_range = True
                            print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (has {pack['past_365_days_games']} past_365_days_games)")
                        elif "generation_timestamp" in pack:
                            # Last resort: use generation_timestamp, but only if it's recent (within 400 days to be safe)
                            pack_timestamp = pack["generation_timestamp"]
                            if isinstance(pack_timestamp, str):
                                pack_timestamp = datetime.fromisoformat(pack_timestamp.replace('Z', '+00:00')).timestamp()
                            # For past-365, if pack was generated recently (within 400 days), include it
                            # This is a heuristic: if pack is old, it likely doesn't have recent data
                            if cutoff_end_timestamp:
                                if cutoff_timestamp <= pack_timestamp <= cutoff_end_timestamp:
                                    has_match_in_range = True
                                    print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (generation_timestamp={pack_timestamp})")
                            else:
                                # For past-365, check if generation_timestamp is within 400 days (to account for pack generation delay)
                                generation_cutoff = (datetime.now(timezone.utc) - timedelta(days=400)).timestamp()
                                if pack_timestamp >= generation_cutoff:
                                    has_match_in_range = True
                                    print(f"âœ… [get_role_stats] Pack {pack_file.name} matches time range (generation_timestamp={pack_timestamp} >= generation_cutoff={generation_cutoff})")
                                else:
                                    print(f"âš ï¸  [get_role_stats] Pack {pack_file.name} generation_timestamp too old ({pack_timestamp}), excluding")
                        else:
                            # No time info at all - exclude to be safe
                            print(f"âš ï¸  [get_role_stats] Pack {pack_file.name} has no time information, excluding")
                    
                    if not has_match_in_range:
                        print(f"âŒ [get_role_stats] Pack {pack_file.name} does NOT match time range, skipping")
                        continue
                
                        by_cr_data.extend(pack.get("by_cr", []))

            print(f"ğŸ” [get_role_stats] After filtering, found {len(by_cr_data)} by_cr entries")

            if not by_cr_data:
                print(f"âš ï¸  [get_role_stats] No data found for queue_id={queue_id}, time_range={time_range}")
                return []

            # ä» by_cr èšåˆ role ç»Ÿè®¡
            role_stats_dict = defaultdict(lambda: {
                "games": 0,
                "wins": 0,
                "total_kda": 0.0
            })

            for cr in by_cr_data:
                role = cr.get("role", "UNKNOWN")
                role_stats_dict[role]["games"] += cr.get("games", 0)
                role_stats_dict[role]["wins"] += cr.get("wins", 0)

                # ä½¿ç”¨ kda_adj * games ä½œä¸ºåŠ æƒKDA
                if "kda_adj" in cr:
                    role_stats_dict[role]["total_kda"] += cr["kda_adj"] * cr.get("games", 0)

            # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
            role_stats = []
            for role, stats in role_stats_dict.items():
                games = stats["games"]
                wins = stats["wins"]
                win_rate = (wins / games * 100) if games > 0 else 0
                avg_kda = (stats["total_kda"] / games) if games > 0 else 0

                role_stats.append({
                    "role": role,
                    "games": games,
                    "wins": wins,
                    "win_rate": round(win_rate, 1),
                    "avg_kda": round(avg_kda, 2)
                })

            # æŒ‰æ¸¸æˆæ•°æ’åº
            role_stats.sort(key=lambda x: x["games"], reverse=True)
            print(f"âœ… [get_role_stats] Returning {len(role_stats)} role stats")
            return role_stats

        except Exception as e:
            print(f"âš ï¸  Failed to get role stats: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_best_champions(self, puuid: str, limit: int = 5, time_range: str = None, queue_id: int = None) -> List[Dict[str, Any]]:
        """
        ä»Player-Packä¸­æå–æœ€ä½³è‹±é›„æ•°æ®ï¼ˆæŒ‰æ¸¸æˆæ•°æ’åºï¼‰
        
        Args:
            puuid: Player PUUID
            limit: Maximum number of champions to return
            time_range: Time range filter (optional)
            queue_id: Queue ID filter (optional, but Champion Mastery uses all game modes so typically None)

        Returns:
            [
                {"champ_id": 202, "games": 50, "wins": 30, "win_rate": 60.0, "avg_kda": 3.5},
                ...
            ]
        """
        player_dir = self.cache_dir / puuid
        if not player_dir.exists():
            return []

        try:
            # Calculate time filter if needed
            cutoff_timestamp = None
            cutoff_end_timestamp = None
            
            if time_range == "2024-01-01":
                cutoff_timestamp = datetime(2024, 1, 9, tzinfo=timezone.utc).timestamp()
                cutoff_end_timestamp = datetime(2025, 1, 6, 23, 59, 59, 999000, tzinfo=timezone.utc).timestamp()
            elif time_range == "past-365":
                cutoff_timestamp = (datetime.now(timezone.utc) - timedelta(days=365)).timestamp()
            
            # Build file pattern based on queue_id
            # Note: Champion Mastery uses all game modes, so queue_id is typically None
            if queue_id is not None:
                pack_pattern = f"pack_*_{queue_id}.json"
            else:
                pack_pattern = "pack_*.json"
            
            pack_files = sorted(player_dir.glob(pack_pattern))
            
            # èšåˆæ‰€æœ‰packæ–‡ä»¶ä¸­çš„championæ•°æ®
            champion_stats = defaultdict(lambda: {
                "games": 0,
                "wins": 0,
                "total_kda": 0.0
            })

            for pack_file in pack_files:
                with open(pack_file, 'r', encoding='utf-8') as f:
                    pack = json.load(f)
                
                # Verify queue_id matches if specified
                if queue_id is not None:
                    pack_queue_id = pack.get('queue_id', 420)
                    if pack_queue_id != queue_id:
                        continue
                
                # Apply time range filter
                if cutoff_timestamp:
                    has_match_in_range = False
                    pack_earliest = pack.get("earliest_match_date")
                    pack_latest = pack.get("latest_match_date")
                    
                    if pack_earliest or pack_latest:
                        if pack_earliest:
                            try:
                                earliest_dt = datetime.fromisoformat(pack_earliest.replace('Z', '+00:00'))
                                if earliest_dt.tzinfo:
                                    earliest_dt = earliest_dt.replace(tzinfo=None)
                                earliest_ts = earliest_dt.timestamp()
                            except:
                                earliest_ts = None
                        else:
                            earliest_ts = None
                            
                        if pack_latest:
                            try:
                                latest_dt = datetime.fromisoformat(pack_latest.replace('Z', '+00:00'))
                                if latest_dt.tzinfo:
                                    latest_dt = latest_dt.replace(tzinfo=None)
                                latest_ts = latest_dt.timestamp()
                            except:
                                latest_ts = None
                        else:
                            latest_ts = None
                        
                        if earliest_ts and latest_ts:
                            if cutoff_end_timestamp:
                                if earliest_ts <= cutoff_end_timestamp and latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                            else:
                                if latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                        elif latest_ts:
                            if cutoff_end_timestamp:
                                if latest_ts <= cutoff_end_timestamp and latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                            else:
                                if latest_ts >= cutoff_timestamp:
                                    has_match_in_range = True
                    else:
                        # Fallback to generation_timestamp
                        if "generation_timestamp" in pack:
                            pack_timestamp = pack["generation_timestamp"]
                            if isinstance(pack_timestamp, str):
                                pack_timestamp = datetime.fromisoformat(pack_timestamp.replace('Z', '+00:00')).timestamp()
                            if cutoff_end_timestamp:
                                if cutoff_timestamp <= pack_timestamp <= cutoff_end_timestamp:
                                    has_match_in_range = True
                            else:
                                if pack_timestamp >= cutoff_timestamp:
                                    has_match_in_range = True
                    
                    if not has_match_in_range:
                        continue
                
                    by_cr_data = pack.get("by_cr", [])

                    for cr in by_cr_data:
                        champ_id = cr.get("champ_id")
                        if not champ_id:
                            continue

                        games = cr.get("games", 0)
                        wins = cr.get("wins", 0)
                        kda_adj = cr.get("kda_adj", 0)

                        champion_stats[champ_id]["games"] += games
                        champion_stats[champ_id]["wins"] += wins
                        champion_stats[champ_id]["total_kda"] += kda_adj * games

            # è½¬æ¢ä¸ºæ•°ç»„æ ¼å¼
            best_champions = []
            for champ_id, stats in champion_stats.items():
                games = stats["games"]
                wins = stats["wins"]
                win_rate = (wins / games * 100) if games > 0 else 0
                avg_kda = (stats["total_kda"] / games) if games > 0 else 0

                # è·å–è‹±é›„åç§°
                champion_name = get_champion_name(champ_id)

                best_champions.append({
                    "champ_id": champ_id,
                    "name": champion_name,
                    "games": games,
                    "wins": wins,
                    "win_rate": round(win_rate, 1),
                    "avg_kda": round(avg_kda, 2)
                })

            # æŒ‰æ¸¸æˆæ•°æ’åºï¼Œè¿”å›å‰Nä¸ª
            best_champions.sort(key=lambda x: x["games"], reverse=True)
            return best_champions[:limit]

        except Exception as e:
            print(f"âš ï¸  Failed to get best champions: {e}")
            import traceback
            traceback.print_exc()
            return []

    def get_recent_matches(self, puuid: str, limit: int = 20) -> List[Dict[str, Any]]:
        """
        è·å–æœ€è¿‘çš„æ¯”èµ›åˆ—è¡¨ï¼ˆç”¨äºtimelineåˆ†æï¼‰

        ä¼˜å…ˆè¿”å›æœ‰timelineæ–‡ä»¶çš„matches

        Args:
            puuid: ç©å®¶PUUID
            limit: è¿”å›æ•°é‡é™åˆ¶

        Returns:
            List[Dict]: æ¯”èµ›ä¿¡æ¯åˆ—è¡¨
        """
        try:
            # æ£€æŸ¥timelineç›®å½•ï¼Œè·å–æœ‰timelineæ–‡ä»¶çš„match_ids
            player_dir = self.cache_dir / puuid
            timelines_dir = player_dir / "timelines"

            available_match_ids = set()
            if timelines_dir.exists():
                for timeline_file in timelines_dir.glob("*_timeline.json"):
                    match_id = timeline_file.stem.replace("_timeline", "")
                    available_match_ids.add(match_id)
            print(f"ğŸ” Available timeline files: {len(available_match_ids)} matches")
            print(f"   Match IDs: {available_match_ids}")

            # Get matches data from job (memory) or matches_data.json (disk)
            job = self.jobs.get(puuid)
            matches_data = None

            if job and job.matches_data:
                # Use in-memory data if available
                matches_data = job.matches_data
            else:
                # Try to load from disk cache
                matches_file = player_dir / "matches_data.json"
                if matches_file.exists():
                    try:
                        with open(matches_file, 'r', encoding='utf-8') as f:
                            matches_data = json.load(f)
                        print(f"âœ… Loaded matches_data.json from disk: {len(matches_data)} matches")
                    except Exception as e:
                        print(f"âš ï¸  Failed to load matches_data.json: {e}")

            # If no matches data available, return empty list
            if not matches_data:
                print(f"âš ï¸  No matches data available for {puuid}")
                return []

            # Convert to frontend format, only include matches with timeline files
            matches = []
            for match in matches_data:
                try:
                    # æå–åŸºç¡€ä¿¡æ¯
                    match_id = match['metadata']['matchId']
                    print(f"ğŸ” Processing match: {match_id}")

                    # ğŸ” åªè¿”å›æœ‰timelineæ–‡ä»¶çš„matches
                    if match_id not in available_match_ids:
                        print(f"   âŒ Skipped: No timeline file for {match_id}")
                        continue
                    print(f"   âœ… Has timeline file")

                    game_creation = match['info']['gameCreation']
                    game_duration = match['info']['gameDuration']

                    # æ‰¾åˆ°å½“å‰ç©å®¶çš„æ•°æ®
                    participants = match['info']['participants']
                    player_data = None
                    for participant in participants:
                        if participant.get('puuid') == puuid:
                            player_data = participant
                            break

                    if not player_data:
                        print(f"   âŒ Player not found in match {match_id}")
                        print(f"      Looking for PUUID: {puuid}")
                        print(f"      Available PUUIDs: {[p.get('puuid') for p in participants]}")
                        continue
                    print(f"   âœ… Found player data")

                    # æå–ç©å®¶æ•°æ®
                    champion_id = player_data.get('championId', 0)
                    champion_name = get_champion_name(champion_id)
                    role = player_data.get('teamPosition', 'UNKNOWN')
                    win = player_data.get('win', False)
                    kills = player_data.get('kills', 0)
                    deaths = player_data.get('deaths', 0)
                    assists = player_data.get('assists', 0)

                    matches.append({
                        'match_id': match_id,
                        'game_creation': game_creation,
                        'game_duration': game_duration,
                        'champion_id': champion_id,
                        'champion_name': champion_name,
                        'role': role,
                        'win': win,
                        'kills': kills,
                        'deaths': deaths,
                        'assists': assists
                    })

                except Exception as e:
                    print(f"âš ï¸  Failed to parse match: {e}")
                    continue

            print(f"âœ… Returning {len(matches)} matches with timeline files")
            return matches

        except Exception as e:
            print(f"âš ï¸  Failed to get recent matches: {e}")
            import traceback
            traceback.print_exc()
            return []

    async def _fetch_timelines_background(
        self,
        match_ids: List[str],
        region: str,
        puuid: str,
        player_dir: Path
    ):
        """
        åå°ä»»åŠ¡ï¼šæ‹‰å–timelineså¹¶æ›´æ–°time_to_core

        è¿™ä¸ªä»»åŠ¡åœ¨ç¬¬ä¸€é˜¶æ®µå®Œæˆåè¿è¡Œï¼Œä¸é˜»å¡agentä½¿ç”¨æ•°æ®
        """
        try:
            import time
            bg_start = time.time()
            print(f"\nğŸ”„ Background task started: Fetching {len(match_ids)} timelines")

            timelines_data = []

            # åˆ†æ‰¹æ‹‰å–timelines
            batch_size = 50
            total_batches = (len(match_ids) + batch_size - 1) // batch_size

            for batch_idx in range(total_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, len(match_ids))
                batch_match_ids = match_ids[start_idx:end_idx]

                batch_start = time.time()
                print(f"   ğŸ“¦ Background batch {batch_idx + 1}/{total_batches}: Fetching {len(batch_match_ids)} timelines...")

                # å¹¶è¡Œæ‹‰å–æœ¬æ‰¹æ¬¡çš„timelines
                batch_tasks = [self._fetch_timeline(match_id, region) for match_id in batch_match_ids]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                batch_duration = time.time() - batch_start

                # æ”¶é›†ç»“æœ
                batch_success = 0
                for result in batch_results:
                    if isinstance(result, Exception):
                        continue
                    if result:
                        timelines_data.append(result)
                        batch_success += 1

                print(f"      âœ… Background batch successful {batch_success}/{len(batch_match_ids)} timelines (took: {batch_duration:.1f}s)")

            bg_duration = time.time() - bg_start
            print(f"âœ… Background timeline fetch complete: {len(timelines_data)} timelines (total time: {bg_duration:.1f}s)")

            # ä¿å­˜timelineæ•°æ®åˆ°jobï¼ˆä¾›APIä½¿ç”¨ï¼‰
            job = self.jobs.get(puuid)
            if job:
                job.timelines_data = timelines_data
                print(f"ğŸ’¾ Timeline data saved to job: {len(timelines_data)} timelines")

            # ä¿å­˜timelineæ•°æ®åˆ°ç£ç›˜ï¼ˆä¾›timeline_deep_dive agentä½¿ç”¨ï¼‰
            timelines_dir = player_dir / "timelines"
            timelines_dir.mkdir(exist_ok=True)

            saved_count = 0
            skipped_count = 0  # ğŸ›¡ï¸ ç»Ÿè®¡è¢«è¿‡æ»¤çš„timelineæ•°é‡

            for timeline in timelines_data:
                try:
                    match_id = timeline['metadata']['matchId']
                    participants = timeline['metadata']['participants']

                    # ğŸ›¡ï¸ ã€å…³é”®éªŒè¯ã€‘ï¼šåªä¿å­˜åŒ…å«ç›®æ ‡ç©å®¶çš„timeline
                    if puuid not in participants:
                        skipped_count += 1
                        print(f"âš ï¸  Skipping timeline {match_id}: Does not contain target player")
                        print(f"     Target PUUID: {puuid[:40]}...")
                        print(f"     First participant: {participants[0][:40]}...")
                        continue

                    # âœ… éªŒè¯é€šè¿‡ï¼Œä¿å­˜timeline
                    timeline_file = timelines_dir / f"{match_id}_timeline.json"
                    with open(timeline_file, 'w', encoding='utf-8') as f:
                        json.dump(timeline, f, indent=2, ensure_ascii=False)
                    saved_count += 1

                except Exception as e:
                    print(f"âš ï¸  Failed to save timeline: {e}")

            print(f"ğŸ’¾ Timeline files saved to disk: {saved_count}/{len(timelines_data)} timelines")
            if skipped_count > 0:
                print(f"ğŸ›¡ï¸ Data security: Filtered out {skipped_count} timelines not belonging to target player")

            # æ›´æ–°player packsä¸­çš„time_to_core
            print(f"ğŸ”„ Updating time_to_core...")
            await self._update_time_to_core(puuid, player_dir, timelines_data)
            print(f"âœ… Background task complete, timeline_deep_dive agent can now use full data")

        except Exception as e:
            print(f"âš ï¸  Background timeline fetch failed (does not affect other agents): {e}")

    async def _update_time_to_core(
        self,
        puuid: str,
        player_dir: Path,
        timelines_data: List[Dict]
    ):
        """
        æ›´æ–°å·²ä¿å­˜çš„player packsï¼Œç”¨çœŸå®çš„time_to_coreæ›¿æ¢é»˜è®¤å€¼
        """
        try:
            # åˆ›å»ºtimelineæ˜ å°„: match_id -> timeline_data
            timelines_map = {t['metadata']['matchId']: t for t in timelines_data}
            print(f"   ğŸ“Š Available timeline data: {len(timelines_map)} timelines")

            # ä»jobä¸­è·å–åŸå§‹matches_data
            job = self.jobs.get(puuid)
            if not job or not job.matches_data:
                print(f"   âš ï¸  Original match data not found, cannot update time_to_core")
                return

            matches_data = job.matches_data
            print(f"   ğŸ“Š Available match data: {len(matches_data)} matches")

            # ä¸ºæ¯åœºæ¯”èµ›è®¡ç®—çœŸå®çš„time_to_core
            match_time_to_core = {}  # {match_id: {participant_id: time_to_core}}

            for match_data in matches_data:
                match_id = match_data['metadata']['matchId']
                if match_id not in timelines_map:
                    continue  # æ²¡æœ‰timelineæ•°æ®ï¼Œè·³è¿‡

                timeline_data = timelines_map[match_id]

                # ä¸ºè¿™åœºæ¯”èµ›çš„æ¯ä¸ªç©å®¶è®¡ç®—time_to_core
                for participant in match_data['info']['participants']:
                    if participant['puuid'] != puuid:
                        continue  # åªå¤„ç†ç›®æ ‡ç©å®¶

                    participant_id = participant['participantId']
                    time_to_core = self._calculate_time_to_core(timeline_data, participant_id)

                    if match_id not in match_time_to_core:
                        match_time_to_core[match_id] = {}
                    match_time_to_core[match_id][participant_id] = time_to_core

            print(f"   âœ… Calculation complete: {len(match_time_to_core)} matches time_to_core")

            # æ›´æ–°æ¯ä¸ªpackæ–‡ä»¶
            updated_packs = 0
            for pack_file in player_dir.glob("pack_*.json"):
                with open(pack_file, 'r', encoding='utf-8') as f:
                    pack = json.load(f)

                # é‡æ–°èšåˆtime_to_coreï¼ˆæŒ‰champion-roleåˆ†ç»„ï¼‰
                cr_time_to_core = defaultdict(list)  # {(champ_id, role): [time_to_core_values]}

                for match_data in matches_data:
                    match_id = match_data['metadata']['matchId']
                    game_version = match_data['info']['gameVersion']
                    patch = '.'.join(game_version.split('.')[:2])

                    # åªå¤„ç†å½“å‰packçš„patch
                    if patch != pack['patch']:
                        continue

                    if match_id not in match_time_to_core:
                        continue

                    for participant in match_data['info']['participants']:
                        if participant['puuid'] != puuid:
                            continue

                        champ_id = participant['championId']
                        role = participant.get('teamPosition', 'UTILITY')
                        participant_id = participant['participantId']

                        if participant_id in match_time_to_core[match_id]:
                            time_to_core = match_time_to_core[match_id][participant_id]
                            cr_time_to_core[(champ_id, role)].append(time_to_core)

                # æ›´æ–°packä¸­çš„avg_time_to_core
                modified = False
                for cr_entry in pack.get('by_cr', []):
                    champ_id = cr_entry['champ_id']
                    role = cr_entry['role']

                    if (champ_id, role) in cr_time_to_core:
                        times = cr_time_to_core[(champ_id, role)]
                        avg_time = np.mean(times)
                        old_time = cr_entry.get('avg_time_to_core', 30.0)
                        cr_entry['avg_time_to_core'] = round(avg_time, 2)

                        if abs(avg_time - old_time) > 0.1:  # æœ‰å®è´¨æ€§å˜åŒ–
                            modified = True

                # å¦‚æœæœ‰ä¿®æ”¹ï¼Œä¿å­˜packæ–‡ä»¶
                if modified:
                    with open(pack_file, 'w', encoding='utf-8') as f:
                        json.dump(pack, f, indent=2, ensure_ascii=False)
                    updated_packs += 1
                    print(f"   âœ… Updated {pack_file.name}: avg_time_to_core updated")

            print(f"   âœ… Background update complete: {updated_packs} pack files updated")

        except Exception as e:
            print(f"âš ï¸  Failed to update time_to_core: {e}")
            import traceback
            traceback.print_exc()


# å…¨å±€å•ä¾‹
player_data_manager = PlayerDataManager()
