"""
Bedrock LLM Adapter for QuantRift ADK
Adapts AWS Bedrock boto3 client to ADK-compatible LLM interface

Phase 4 Day 4: Added parallel report generation support
Option A Day 1: Integrated structured logging
"""

import json
import os
import asyncio
import time
from typing import Optional, Dict, Any, List
from concurrent.futures import ThreadPoolExecutor
import boto3
from botocore.config import Config

from .structured_logger import get_logger, LogTimer, LogContext
from .metrics_collector import MetricNames  # Keep MetricNames for naming
from .async_metrics_wrapper import get_async_metrics  # Use non-blocking async wrapper
from .llm_cache import get_llm_cache


class BedrockModel:
    """Bedrock æ¨¡å‹é…ç½®"""

    # Anthropic Claude æ¨¡å‹ ID (ä½¿ç”¨ inference profile)
    SONNET_4_5 = "us.anthropic.claude-sonnet-4-5-20250929-v1:0"
    HAIKU_4_5 = "us.anthropic.claude-haiku-4-5-20251001-v1:0"  # Haiku 4.5 inference profile
    HAIKU_3_5 = "us.anthropic.claude-3-5-haiku-20241022-v1:0"
    HAIKU_3 = "us.anthropic.claude-3-haiku-20240307-v1:0"

    # æ¨¡å‹åˆ«åæ˜ å°„
    MODEL_ALIASES = {
        "sonnet": SONNET_4_5,
        "haiku": HAIKU_4_5,  # é»˜è®¤ä½¿ç”¨4.5ï¼ˆæœ€æ–°æœ€å¿«ï¼‰
        "haiku-4.5": HAIKU_4_5,
        "haiku-3.5": HAIKU_3_5,
        "haiku-3": HAIKU_3,
        "claude-sonnet-4-5": SONNET_4_5,
        "claude-haiku-4.5": HAIKU_4_5,
        "claude-3.5-haiku": HAIKU_3_5,
        "claude-3-haiku": HAIKU_3
    }

    @classmethod
    def resolve_model_id(cls, model_name: str) -> str:
        """è§£ææ¨¡å‹åç§°ä¸ºå®Œæ•´æ¨¡å‹ ID"""
        if model_name.startswith("us.anthropic.") or model_name.startswith("anthropic."):
            return model_name
        return cls.MODEL_ALIASES.get(model_name.lower(), cls.SONNET_4_5)


class BedrockLLM:
    """
    ADK-compatible Bedrock LLM adapter

    Adapts boto3 Bedrock Runtime calls to QuantRift ADK Agent interface.
    Supports Claude Sonnet 4.5 and Haiku 4.5 models.

    Example:
        >>> from src.agents.shared.bedrock_adapter import BedrockLLM
        >>> from src.agents.player_analysis.weakness_analysis.agent import WeaknessAnalysisAgent
        >>>
        >>> llm = BedrockLLM(model="haiku")
        >>> agent = WeaknessAnalysisAgent(model="haiku")
        >>> for chunk in agent.run_stream(packs_dir, recent_count=5):
        ...     print(chunk, end="")
    """

    def __init__(
        self,
        model: str = "haiku",
        region: str = None,
        read_timeout: int = 600,
        connect_timeout: int = 60,
        max_retries: int = 3,
        enable_cache: bool = True,
        cache_ttl_hours: int = 24
    ):
        """
        åˆå§‹åŒ– Bedrock LLM é€‚é…å™¨

        Args:
            model: æ¨¡å‹åç§° ("sonnet", "haiku") æˆ–å®Œæ•´æ¨¡å‹ ID
            region: AWS åŒºåŸŸï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            read_timeout: è¯»å–è¶…æ—¶ï¼ˆç§’ï¼‰
            connect_timeout: è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            enable_cache: æ˜¯å¦å¯ç”¨ç»“æœç¼“å­˜ï¼ˆPhase 1.3ï¼‰
            cache_ttl_hours: ç¼“å­˜æœ‰æ•ˆæœŸï¼ˆå°æ—¶ï¼‰
        """
        self.model_id = BedrockModel.resolve_model_id(model)
        self.region = region or os.getenv("AWS_REGION", "us-west-2")  # us-west-2æ›´ç¨³å®š

        # é…ç½® boto3 client
        config = Config(
            read_timeout=read_timeout,
            connect_timeout=connect_timeout,
            retries={'max_attempts': max_retries}
        )

        self.bedrock_runtime = boto3.client(
            service_name='bedrock-runtime',
            region_name=self.region,
            config=config
        )

        # æ¨¡å‹é»˜è®¤å‚æ•°
        self.default_max_tokens = 16000 if "sonnet" in self.model_id else 8000
        self.default_temperature = 0.7

        # ç»“æ„åŒ–æ—¥å¿—ï¼ˆOption A Day 1ï¼‰
        self.logger = get_logger("BedrockLLM", level="INFO")
        self.logger.info("LLMåˆå§‹åŒ–", model=self.model_id, region=self.region, enable_cache=enable_cache)

        # æŒ‡æ ‡æ”¶é›†å™¨ï¼ˆOption A Day 2ï¼‰ - Using async non-blocking wrapper
        self.metrics = get_async_metrics()

        # LLMç¼“å­˜ï¼ˆPhase 1.3ï¼‰ - TEMPORARILY DISABLED
        self.enable_cache = False  # FORCE DISABLE
        self.cache = None  # FORCE DISABLE

    async def generate(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        çœŸæ­£çš„å¼‚æ­¥ç”Ÿæˆæ¥å£ï¼ˆPhase 4 Day 4ï¼‰

        ä½¿ç”¨ asyncio + ThreadPoolExecutor å®ç°çœŸæ­£çš„å¹¶å‘è°ƒç”¨

        Args:
            prompt: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°ï¼ˆ0.0-1.0ï¼‰
            system: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            dict: åŒ…å« text å’Œ usage çš„å­—å…¸
        """
        loop = asyncio.get_event_loop()

        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒåŒæ­¥è°ƒç”¨
        result = await loop.run_in_executor(
            None,  # ä½¿ç”¨é»˜è®¤executor
            lambda: self.generate_sync(
                prompt=prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system,
                **kwargs
            )
        )

        return result

    def generate_sync(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system: Optional[str] = None,
        use_cache: bool = True,
        **kwargs
    ) -> Dict[str, Any]:
        """
        åŒæ­¥ç”Ÿæˆæ¥å£ï¼ˆç”¨äºé async åœºæ™¯ï¼‰

        Args:
            prompt: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            system: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Trueï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Returns:
            dict: åŒ…å« text å’Œ usage çš„å­—å…¸
        """
        start_time = time.time()

        # Phase 1.3: æ£€æŸ¥ç¼“å­˜
        if self.enable_cache and use_cache and self.cache:
            cached_result = self.cache.get(
                prompt=prompt,
                system=system,
                model=self.model_id,
                temperature=temperature or self.default_temperature,
                max_tokens=max_tokens
            )

            if cached_result is not None:
                # ç¼“å­˜å‘½ä¸­
                cache_duration_ms = (time.time() - start_time) * 1000

                self.logger.info(
                    "LLMç¼“å­˜å‘½ä¸­",
                    model=self.model_id,
                    cache_key_preview=prompt[:50],
                    cache_duration_ms=cache_duration_ms
                )

                # æŒ‡æ ‡ï¼šç¼“å­˜å‘½ä¸­
                model_label = "haiku" if "haiku" in self.model_id else "sonnet"
                self.metrics.increment(
                    "llm_cache_hits_total",
                    labels={"model": model_label}
                )

                return cached_result

        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè°ƒç”¨API
        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens or self.default_max_tokens,
            "temperature": temperature or self.default_temperature,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }

        # æ·»åŠ ç³»ç»Ÿæç¤ºï¼ˆå¦‚æœæä¾›ï¼‰
        if system:
            request_body["system"] = system

        # æ—¥å¿—ï¼šLLM è°ƒç”¨å¼€å§‹
        self.logger.debug(
            "LLMè°ƒç”¨å¼€å§‹",
            model=self.model_id,
            prompt_length=len(prompt),
            max_tokens=request_body["max_tokens"],
            temperature=request_body["temperature"],
            has_system=bool(system),
            cache_miss=True
        )

        try:
            response = self.bedrock_runtime.invoke_model(
                modelId=self.model_id,
                body=json.dumps(request_body)
            )

            response_body = json.loads(response['body'].read())
            duration_ms = (time.time() - start_time) * 1000

            result = {
                "text": response_body['content'][0]['text'],
                "usage": response_body.get('usage', {}),
                "model": self.model_id
            }

            # TEMPORARY FIX: Wrap logging/metrics/cache in try-except to prevent hanging
            print(f"ğŸ” DEBUG: Before logger.log_performance")
            import sys
            sys.stdout.flush()

            try:
                # æ—¥å¿—ï¼šLLM è°ƒç”¨æˆåŠŸï¼ˆæ€§èƒ½æŒ‡æ ‡ï¼‰
                self.logger.log_performance(
                    operation="llm_call",
                    duration_ms=duration_ms,
                    success=True,
                    model=self.model_id,
                    input_tokens=result["usage"].get("input_tokens", 0),
                    output_tokens=result["usage"].get("output_tokens", 0),
                    total_tokens=result["usage"].get("input_tokens", 0) + result["usage"].get("output_tokens", 0)
                )
            except Exception as e:
                pass  # Don't let logging block the response

            # æŒ‡æ ‡ï¼šLLM è°ƒç”¨ï¼ˆOption A Day 2ï¼‰ - NOW USING NON-BLOCKING ASYNC WRAPPER
            try:
                model_label = "haiku" if "haiku" in self.model_id else "sonnet"
                self.metrics.increment(
                    MetricNames.LLM_CALLS_TOTAL,
                    labels={"model": model_label, "status": "success"}
                )
                self.metrics.observe(
                    MetricNames.LLM_CALL_DURATION_SECONDS,
                    duration_ms / 1000.0,  # è½¬æ¢ä¸ºç§’
                    labels={"model": model_label}
                )
                self.metrics.increment(
                    MetricNames.LLM_INPUT_TOKENS_TOTAL,
                    labels={"model": model_label},
                    amount=result["usage"].get("input_tokens", 0)
                )
                self.metrics.increment(
                    MetricNames.LLM_OUTPUT_TOKENS_TOTAL,
                    labels={"model": model_label},
                    amount=result["usage"].get("output_tokens", 0)
                )
            except Exception as e:
                pass  # Don't let metrics block the response

            try:
                # Phase 1.3: å­˜å‚¨åˆ°ç¼“å­˜
                if self.enable_cache and use_cache and self.cache:
                    self.cache.set(
                        prompt=prompt,
                        system=system,
                        model=self.model_id,
                        result=result,
                        temperature=temperature or self.default_temperature,
                        max_tokens=max_tokens
                    )

                    # æŒ‡æ ‡ï¼šç¼“å­˜æœªå‘½ä¸­
                    self.metrics.increment(
                        "llm_cache_misses_total",
                        labels={"model": model_label}
                    )
            except Exception as e:
                pass  # Don't let cache block the response

            print(f"ğŸ” DEBUG bedrock_adapter: About to return result, keys: {list(result.keys())}")
            import sys
            sys.stdout.flush()
            return result

        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000
            error_msg = f"Bedrock LLM generation failed: {str(e)}"

            # æ—¥å¿—ï¼šLLM è°ƒç”¨å¤±è´¥
            self.logger.error(
                "LLMè°ƒç”¨å¤±è´¥",
                error=str(e),
                error_type=type(e).__name__,
                model=self.model_id,
                duration_ms=duration_ms
            )

            # æŒ‡æ ‡ï¼šLLM é”™è¯¯ï¼ˆOption A Day 2ï¼‰
            model_label = "haiku" if "haiku" in self.model_id else "sonnet"
            self.metrics.increment(
                MetricNames.LLM_CALLS_TOTAL,
                labels={"model": model_label, "status": "error"}
            )
            self.metrics.increment(
                MetricNames.LLM_ERRORS_TOTAL,
                labels={"model": model_label, "error_type": type(e).__name__}
            )

            return {
                "text": f"# LLM è°ƒç”¨å¤±è´¥\n{error_msg}",
                "usage": {"input_tokens": 0, "output_tokens": 0},
                "model": self.model_id
            }

    async def generate_batch(
        self,
        requests: List[Dict[str, Any]],
        max_concurrent: int = 5
    ) -> List[Dict[str, Any]]:
        """
        æ‰¹é‡å¹¶è¡Œç”Ÿæˆï¼ˆPhase 4 Day 4ï¼‰

        Args:
            requests: è¯·æ±‚åˆ—è¡¨ï¼Œæ¯ä¸ªè¯·æ±‚æ˜¯ä¸€ä¸ªdictåŒ…å« prompt, max_tokens, temperature, systemç­‰
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤5ï¼‰

        Returns:
            ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªç»“æœåŒ…å« text, usage, model

        ä½¿ç”¨ç¤ºä¾‹:
            requests = [
                {"prompt": "åˆ†æè‹±é›„1", "system": "ä½ æ˜¯åˆ†æå¸ˆ"},
                {"prompt": "åˆ†æè‹±é›„2", "system": "ä½ æ˜¯åˆ†æå¸ˆ"},
                {"prompt": "åˆ†æè‹±é›„3", "system": "ä½ æ˜¯åˆ†æå¸ˆ"}
            ]
            results = await llm.generate_batch(requests, max_concurrent=3)
        """
        start_time = time.time()

        # æ—¥å¿—ï¼šæ‰¹é‡è°ƒç”¨å¼€å§‹
        self.logger.info(
            "LLMæ‰¹é‡è°ƒç”¨å¼€å§‹",
            batch_size=len(requests),
            max_concurrent=max_concurrent,
            model=self.model_id
        )

        # ä½¿ç”¨ asyncio.gather å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰è¯·æ±‚
        tasks = [
            self.generate(
                prompt=req.get("prompt", ""),
                max_tokens=req.get("max_tokens"),
                temperature=req.get("temperature"),
                system=req.get("system")
            )
            for req in requests
        ]

        # é™åˆ¶å¹¶å‘æ•°
        if max_concurrent and max_concurrent < len(tasks):
            # åˆ†æ‰¹æ‰§è¡Œ
            results = []
            for i in range(0, len(tasks), max_concurrent):
                batch = tasks[i:i + max_concurrent]
                batch_results = await asyncio.gather(*batch, return_exceptions=True)
                results.extend(batch_results)
        else:
            # å…¨éƒ¨å¹¶è¡Œæ‰§è¡Œ
            results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸
        processed_results = []
        success_count = 0
        error_count = 0

        for i, result in enumerate(results):
            if isinstance(result, Exception):
                error_count += 1
                processed_results.append({
                    "text": f"# ç”Ÿæˆå¤±è´¥\n{str(result)}",
                    "usage": {"input_tokens": 0, "output_tokens": 0},
                    "model": self.model_id,
                    "error": str(result)
                })
            else:
                success_count += 1
                processed_results.append(result)

        duration_ms = (time.time() - start_time) * 1000

        # æ—¥å¿—ï¼šæ‰¹é‡è°ƒç”¨å®Œæˆï¼ˆæ€§èƒ½æŒ‡æ ‡ï¼‰
        total_input_tokens = sum(
            r.get("usage", {}).get("input_tokens", 0)
            for r in processed_results if "error" not in r
        )
        total_output_tokens = sum(
            r.get("usage", {}).get("output_tokens", 0)
            for r in processed_results if "error" not in r
        )

        self.logger.log_performance(
            operation="llm_batch_call",
            duration_ms=duration_ms,
            success=(error_count == 0),
            model=self.model_id,
            batch_size=len(requests),
            success_count=success_count,
            error_count=error_count,
            total_input_tokens=total_input_tokens,
            total_output_tokens=total_output_tokens,
            avg_time_per_request_ms=duration_ms / len(requests) if requests else 0
        )

        return processed_results

    def generate_stream(
        self,
        prompt: str,
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        system: Optional[str] = None,
        on_chunk: Optional[callable] = None,
        enable_thinking: bool = False,
        **kwargs
    ):
        """
        æµå¼ç”Ÿæˆæ¥å£ï¼ˆPhase 1.2ï¼‰

        å®æ—¶è¾“å‡ºLLMç”Ÿæˆçš„tokenï¼Œæ— éœ€ç­‰å¾…å®Œæ•´å“åº”ã€‚
        UXæå‡500%ï¼šç”¨æˆ·å¯ä»¥ç«‹å³çœ‹åˆ°ç”Ÿæˆè¿›åº¦ã€‚

        Args:
            prompt: ç”¨æˆ·è¾“å…¥æ–‡æœ¬
            max_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            temperature: æ¸©åº¦å‚æ•°
            system: ç³»ç»Ÿæç¤ºï¼ˆå¯é€‰ï¼‰
            on_chunk: å›è°ƒå‡½æ•°ï¼Œæ¥æ”¶æ¯ä¸ªchunkçš„æ–‡æœ¬ï¼ˆå¯é€‰ï¼‰
            **kwargs: å…¶ä»–å‚æ•°

        Yields:
            str: æ¯æ¬¡yieldä¸€ä¸ªæ–‡æœ¬chunk

        Returns:
            Dict[str, Any]: æœ€ç»ˆå®Œæ•´ç»“æœ {"text": str, "usage": dict, "model": str}

        Usage:
            # ç®€å•è¿­ä»£
            for chunk in llm.generate_stream(prompt="åˆ†æè‹±é›„"):
                print(chunk, end="", flush=True)

            # ä½¿ç”¨å›è°ƒ
            def show_progress(chunk):
                print(chunk, end="", flush=True)

            result = llm.generate_stream(prompt="åˆ†æè‹±é›„", on_chunk=show_progress)
        """
        start_time = time.time()

        # Extended thinkingè¦æ±‚temperature=1.0
        if enable_thinking and "haiku" in self.model_id.lower():
            final_temperature = 1.0
        else:
            final_temperature = temperature or self.default_temperature

        request_body = {
            "anthropic_version": "bedrock-2023-05-31",
            "max_tokens": max_tokens or self.default_max_tokens,
            "temperature": final_temperature,
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ]
        }

        # æ·»åŠ extended thinkingæ”¯æŒï¼ˆClaude 3.5 Haikuç‰¹æ€§ï¼‰
        if enable_thinking and "haiku" in self.model_id.lower():
            request_body["thinking"] = {
                "type": "enabled",
                "budget_tokens": 2000  # 2K tokensç”¨äºæ€è€ƒ
            }

        # æ·»åŠ ç³»ç»Ÿæç¤º
        if system:
            request_body["system"] = system

        # æ—¥å¿—ï¼šæµå¼è°ƒç”¨å¼€å§‹
        self.logger.info(
            "LLMæµå¼è°ƒç”¨å¼€å§‹",
            model=self.model_id,
            prompt_length=len(prompt),
            max_tokens=request_body["max_tokens"]
        )

        try:
            # ä½¿ç”¨æµå¼API
            response = self.bedrock_runtime.invoke_model_with_response_stream(
                modelId=self.model_id,
                body=json.dumps(request_body)
            )

            # æ”¶é›†å®Œæ•´å“åº”
            full_text = []
            thinking_text = []
            usage_info = {}

            # å¤„ç†æµå¼å“åº”
            print(f"ğŸ” BEDROCK: Starting to process stream response...")
            event_count = 0
            for event in response['body']:
                event_count += 1
                chunk = json.loads(event['chunk']['bytes'].decode())
                if event_count <= 3:
                    print(f"ğŸ” BEDROCK Event {event_count}: type={chunk.get('type')}")

                if chunk['type'] == 'content_block_start':
                    # æ£€æŸ¥æ˜¯å¦æ˜¯thinking block
                    if 'content_block' in chunk:
                        block = chunk['content_block']
                        if block.get('type') == 'thinking':
                            # å¼€å§‹thinking blockï¼Œå‘é€ç‰¹æ®Šæ ‡è®°
                            if on_chunk:
                                on_chunk('__THINKING_START__')
                            yield '__THINKING_START__'

                elif chunk['type'] == 'content_block_delta':
                    # æå–æ–‡æœ¬delta
                    if 'delta' in chunk:
                        delta = chunk['delta']
                        if delta.get('type') == 'thinking_delta':
                            # Thinkingå†…å®¹
                            text_chunk = delta.get('thinking', '')
                            thinking_text.append(text_chunk)
                            if on_chunk:
                                on_chunk(f'__THINKING__{text_chunk}')
                            yield f'__THINKING__{text_chunk}'
                        elif delta.get('type') == 'text_delta' or 'text' in delta:
                            # æ­£å¸¸æ–‡æœ¬å†…å®¹
                            text_chunk = delta.get('text', '')
                            full_text.append(text_chunk)
                            if on_chunk:
                                on_chunk(text_chunk)
                            yield text_chunk

                elif chunk['type'] == 'content_block_stop':
                    # Thinking blockç»“æŸ
                    if on_chunk:
                        on_chunk('__THINKING_END__')
                    yield '__THINKING_END__'

                elif chunk['type'] == 'message_delta':
                    # æå–usageä¿¡æ¯
                    if 'usage' in chunk:
                        usage_info.update(chunk['usage'])

                elif chunk['type'] == 'message_stop':
                    # æµå¼ç»“æŸ
                    if 'amazon-bedrock-invocationMetrics' in chunk:
                        # æ›´æ–°usageä¿¡æ¯
                        metrics = chunk['amazon-bedrock-invocationMetrics']
                        if 'inputTokenCount' in metrics:
                            usage_info['input_tokens'] = metrics['inputTokenCount']
                        if 'outputTokenCount' in metrics:
                            usage_info['output_tokens'] = metrics['outputTokenCount']

            duration_ms = (time.time() - start_time) * 1000

            # ç»„è£…å®Œæ•´ç»“æœ
            result = {
                "text": "".join(full_text),
                "usage": usage_info,
                "model": self.model_id
            }

            # æ—¥å¿—ï¼šæµå¼è°ƒç”¨å®Œæˆï¼ˆæ€§èƒ½æŒ‡æ ‡ï¼‰
            self.logger.log_performance(
                operation="llm_stream",
                duration_ms=duration_ms,
                success=True,
                model=self.model_id,
                input_tokens=usage_info.get("input_tokens", 0),
                output_tokens=usage_info.get("output_tokens", 0),
                total_tokens=usage_info.get("input_tokens", 0) + usage_info.get("output_tokens", 0),
                streaming=True
            )

            # æŒ‡æ ‡ï¼šæµå¼è°ƒç”¨
            model_label = "haiku" if "haiku" in self.model_id else "sonnet"
            self.metrics.increment(
                MetricNames.LLM_CALLS_TOTAL,
                labels={"model": model_label, "status": "success", "streaming": "true"}
            )
            self.metrics.observe(
                MetricNames.LLM_CALL_DURATION_SECONDS,
                duration_ms / 1000.0,
                labels={"model": model_label}
            )

            return result

        except Exception as e:
            duration_ms = (time.time() - start_time) * 1000
            error_msg = f"Bedrock LLM streaming failed: {str(e)}"

            # æ—¥å¿—ï¼šæµå¼è°ƒç”¨å¤±è´¥
            self.logger.error(
                "LLMæµå¼è°ƒç”¨å¤±è´¥",
                error=str(e),
                error_type=type(e).__name__,
                model=self.model_id,
                duration_ms=duration_ms
            )

            # æŒ‡æ ‡ï¼šæµå¼é”™è¯¯
            model_label = "haiku" if "haiku" in self.model_id else "sonnet"
            self.metrics.increment(
                MetricNames.LLM_CALLS_TOTAL,
                labels={"model": model_label, "status": "error", "streaming": "true"}
            )
            self.metrics.increment(
                MetricNames.LLM_ERRORS_TOTAL,
                labels={"model": model_label, "error_type": type(e).__name__}
            )

            return {
                "text": f"# LLM æµå¼è°ƒç”¨å¤±è´¥\n{error_msg}",
                "usage": {"input_tokens": 0, "output_tokens": 0},
                "model": self.model_id
            }

    def __repr__(self) -> str:
        return f"BedrockLLM(model={self.model_id}, region={self.region})"


# ä¾¿æ·å·¥å‚å‡½æ•°
def create_sonnet_llm(**kwargs) -> BedrockLLM:
    """åˆ›å»º Claude Sonnet 4.5 LLM"""
    return BedrockLLM(model="sonnet", **kwargs)


def create_haiku_llm(**kwargs) -> BedrockLLM:
    """åˆ›å»º Claude 3.5 Haiku LLM"""
    return BedrockLLM(model="haiku", **kwargs)
