"""
Stream Helper - é€šç”¨Agent StreamåŒ…è£…å™¨
æ”¯æŒextended thinkingå’Œæ¨¡å‹åˆ‡æ¢
"""

import json
from typing import AsyncGenerator, Dict, Any
from .bedrock_adapter import BedrockLLM


def stream_agent_with_thinking(
    prompt: str,
    system_prompt: str = None,
    model: str = "haiku",
    max_tokens: int = 16000,
    enable_thinking: bool = True
):
    """
    é€šç”¨agent streamç”Ÿæˆå™¨ï¼ˆæ”¯æŒextended thinkingï¼‰

    Args:
        prompt: ç”¨æˆ·prompt
        system_prompt: ç³»ç»Ÿprompt
        model: æ¨¡å‹åç§°ï¼ˆhaiku, haiku-3.5, sonnetç­‰ï¼‰
        max_tokens: æœ€å¤§tokenæ•°
        enable_thinking: æ˜¯å¦å¯ç”¨extended thinking

    Yields:
        SSEæ ¼å¼çš„æ¶ˆæ¯: "data: {JSON}\\n\\n"

    æ¶ˆæ¯ç±»å‹:
        - {"type": "thinking_start"} - Thinkingå¼€å§‹
        - {"type": "thinking", "content": "..."} - Thinkingå†…å®¹
        - {"type": "thinking_end"} - Thinkingç»“æŸ
        - {"type": "chunk", "content": "..."} - æ­£å¸¸æ–‡æœ¬chunk
        - {"type": "complete", "detailed": "..."} - å®Œæˆ
        - {"error": "..."} - é”™è¯¯

    Usage:
        async for message in stream_agent_with_thinking(
            prompt="åˆ†æç©å®¶æ•°æ®",
            model="haiku",
            enable_thinking=True
        ):
            yield message
    """
    try:
        # åˆå§‹åŒ–LLM
        llm = BedrockLLM(model=model)

        # è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” Stream starting: model={model}, prompt_len={len(prompt)}, system_len={len(system_prompt) if system_prompt else 0}")
        print(f"   enable_thinking={enable_thinking}, max_tokens={max_tokens}")

        # Streamç”Ÿæˆdetailed report
        detailed_chunks = []
        thinking_chunks = []

        chunk_count = 0
        print(f"ğŸ” About to call llm.generate_stream...")
        stream_generator = llm.generate_stream(
            prompt=prompt,
            system=system_prompt,
            max_tokens=max_tokens,
            enable_thinking=enable_thinking
        )
        print(f"ğŸ” Stream generator created: {type(stream_generator)}")
        print(f"ğŸ” Starting iteration...")

        try:
            for chunk in stream_generator:
                chunk_count += 1

                # å¤„ç†thinkingæ ‡è®°
                if chunk == '__THINKING_START__':
                    if chunk_count <= 3:
                        print(f"   Chunk {chunk_count}: [THINKING_START]")
                    yield f"data: {json.dumps({'type': 'thinking_start'})}\n\n"
                    continue
                elif chunk == '__THINKING_END__':
                    if chunk_count <= 3:
                        print(f"   Chunk {chunk_count}: [THINKING_END]")
                    yield f"data: {json.dumps({'type': 'thinking_end'})}\n\n"
                    continue
                elif chunk.startswith('__THINKING__'):
                    # Thinkingå†…å®¹
                    thinking_text = chunk[12:]  # å»æ‰__THINKING__å‰ç¼€
                    thinking_chunks.append(thinking_text)
                    if chunk_count <= 3:
                        print(f"   Chunk {chunk_count}: [THINKING] {thinking_text[:50]}")
                    yield f"data: {json.dumps({'type': 'thinking', 'content': thinking_text})}\n\n"
                    continue

                # æ­£å¸¸æ–‡æœ¬å†…å®¹
                detailed_chunks.append(chunk)
                if chunk_count <= 3:
                    print(f"   Chunk {chunk_count}: {chunk[:50]}")
                yield f"data: {json.dumps({'type': 'chunk', 'content': chunk})}\n\n"

            print(f"âœ… Stream complete: {chunk_count} chunks received")
        except Exception as e:
            import traceback
            print(f"âŒ Stream iteration error: {traceback.format_exc()}")
            raise
        detailed = "".join(detailed_chunks)

        # è¿”å›å®Œæ•´ç»“æœ
        yield f"data: {json.dumps({'type': 'complete', 'detailed': detailed})}\n\n"

    except Exception as e:
        import traceback
        error_msg = f"Stream error: {str(e)}\n{traceback.format_exc()}"
        print(f"âŒ {error_msg}")
        yield f"data: {json.dumps({'error': str(e)})}\n\n"
