"""
AgentContext - Agenté—´æ¶ˆæ¯ä¼ é€’ä¸ä¸Šä¸‹æ–‡å…±äº«

æä¾›Agenté—´çš„æ•°æ®å…±äº«ã€å¢é‡åˆ†æå’Œé¿å…é‡å¤è®¡ç®—çš„æœºåˆ¶
Phase 4 Day 3: æ–°å¢ç¼“å­˜é¢„çƒ­åŠŸèƒ½
"""

import json
import sys
import threading
import time
from typing import Dict, Any, List, Optional
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, Future


class AgentContext:
    """
    Agentæ‰§è¡Œä¸Šä¸‹æ–‡ (Phase 4: ä¼˜åŒ–å†…å­˜ç®¡ç†)

    ç®¡ç†Agenté—´çš„æ•°æ®å…±äº«å’Œæ¶ˆæ¯ä¼ é€’ï¼š
    - å­˜å‚¨æ¯ä¸ªAgentçš„æ‰§è¡Œç»“æœ
    - æä¾›ç»Ÿä¸€çš„æ•°æ®è®¿é—®æ¥å£
    - æ”¯æŒå¢é‡åˆ†æ
    - é¿å…é‡å¤è®¡ç®—
    - LRUç¼“å­˜ç®¡ç†ï¼ˆè‡ªåŠ¨é©±é€æœ€ä¹…æœªä½¿ç”¨æ•°æ®ï¼‰
    """

    def __init__(self, user_request: str, packs_dir: str, max_cache_size_mb: int = 500):
        """
        åˆå§‹åŒ–Agentä¸Šä¸‹æ–‡

        Args:
            user_request: åŸå§‹ç”¨æˆ·è¯·æ±‚
            packs_dir: Player Packæ•°æ®ç›®å½•
            max_cache_size_mb: æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆMBï¼‰ï¼Œé»˜è®¤500MB
        """
        self.user_request = user_request
        self.packs_dir = packs_dir
        self.max_cache_size = max_cache_size_mb * 1024 * 1024  # Convert to bytes

        # Agentæ‰§è¡Œç»“æœå­˜å‚¨
        self._results: Dict[str, Dict[str, Any]] = {}

        # æ‰§è¡Œé¡ºåºè®°å½•
        self._execution_order: List[str] = []

        # å…±äº«æ•°æ®ç¼“å­˜
        self._shared_cache: Dict[str, Any] = {}

        # ç¼“å­˜å…ƒæ•°æ® (Phase 4: LRU tracking)
        self._cache_metadata: Dict[str, Dict[str, Any]] = {}

        # å…ƒæ•°æ®
        self._metadata = {
            "created_at": datetime.now().isoformat(),
            "total_agents_executed": 0,
            "cache_evictions": 0,  # Track eviction count
            "peak_cache_size_mb": 0.0  # Track peak memory usage
        }

        # çº¿ç¨‹é” (Phase 2 å¹¶è¡Œæ‰§è¡Œå®‰å…¨ä¿æŠ¤)
        self._lock = threading.Lock()

        # Phase 4 Day 3: ç¼“å­˜é¢„çƒ­ç›¸å…³
        self._preload_futures: Dict[str, Future] = {}  # å­˜å‚¨åå°åŠ è½½ä»»åŠ¡

    def add_agent_result(
        self,
        agent_name: str,
        data: Dict[str, Any],
        report: str,
        execution_time: float = 0.0
    ) -> None:
        """
        æ·»åŠ Agentæ‰§è¡Œç»“æœ (çº¿ç¨‹å®‰å…¨)

        Args:
            agent_name: Agentåç§°
            data: Agentè¿”å›çš„ç»“æ„åŒ–æ•°æ®
            report: Agentç”Ÿæˆçš„æŠ¥å‘Šæ–‡æœ¬
            execution_time: æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰
        """
        with self._lock:
            self._results[agent_name] = {
                "data": data,
                "report": report,
                "execution_time": execution_time,
                "timestamp": datetime.now().isoformat()
            }

            self._execution_order.append(agent_name)
            self._metadata["total_agents_executed"] += 1

    def get_agent_result(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šAgentçš„æ‰§è¡Œç»“æœ

        Args:
            agent_name: Agentåç§°

        Returns:
            Agentç»“æœå­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        return self._results.get(agent_name)

    def get_agent_data(self, agent_name: str) -> Optional[Dict[str, Any]]:
        """
        è·å–æŒ‡å®šAgentçš„æ•°æ®éƒ¨åˆ†

        Args:
            agent_name: Agentåç§°

        Returns:
            Agentæ•°æ®å­—å…¸ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        result = self._results.get(agent_name)
        return result["data"] if result else None

    def get_agent_report(self, agent_name: str) -> Optional[str]:
        """
        è·å–æŒ‡å®šAgentçš„æŠ¥å‘Šæ–‡æœ¬

        Args:
            agent_name: Agentåç§°

        Returns:
            æŠ¥å‘Šæ–‡æœ¬ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å›None
        """
        result = self._results.get(agent_name)
        return result["report"] if result else None

    def has_agent_result(self, agent_name: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å­˜åœ¨æŒ‡å®šAgentçš„ç»“æœ

        Args:
            agent_name: Agentåç§°

        Returns:
            æ˜¯å¦å­˜åœ¨ç»“æœ
        """
        return agent_name in self._results

    def get_previous_agents(self) -> List[str]:
        """
        è·å–å·²æ‰§è¡Œçš„Agentåˆ—è¡¨ï¼ˆæŒ‰æ‰§è¡Œé¡ºåºï¼‰

        Returns:
            Agentåç§°åˆ—è¡¨
        """
        return self._execution_order.copy()

    def set_shared_data(self, key: str, value: Any) -> None:
        """
        è®¾ç½®å…±äº«æ•°æ®

        Args:
            key: æ•°æ®é”®
            value: æ•°æ®å€¼
        """
        self._shared_cache[key] = value

    def add_shared_data(self, key: str, data: Any, summary: str = "") -> None:
        """
        æ·»åŠ å…±äº«æ•°æ®ï¼ˆå¸¦å…ƒæ•°æ®ã€LRUç¼“å­˜ç®¡ç†ï¼‰(çº¿ç¨‹å®‰å…¨)

        Phase 4 ä¼˜åŒ–: è‡ªåŠ¨å†…å­˜ç®¡ç†
        - è®¡ç®—æ•°æ®å¤§å°
        - å¦‚æœè¶…å‡ºé™åˆ¶ï¼Œé©±é€æœ€ä¹…æœªä½¿ç”¨çš„æ•°æ®
        - è¿½è¸ªè®¿é—®æ—¶é—´ç”¨äºLRU

        Args:
            key: æ•°æ®é”®
            data: æ•°æ®å€¼
            summary: æ•°æ®æ‘˜è¦æè¿°ï¼ˆå¯é€‰ï¼‰
        """
        with self._lock:
            # 1. è®¡ç®—æ•°æ®å¤§å°ï¼ˆä½¿ç”¨sys.getsizeofä¼°ç®—ï¼‰
            data_size = sys.getsizeof(data)

            # å¯¹äºå¤æ‚å¯¹è±¡ï¼ˆlist, dictï¼‰ï¼Œé€’å½’è®¡ç®—
            if isinstance(data, (list, dict)):
                data_size = self._calculate_deep_size(data)

            # 2. æ£€æŸ¥æ˜¯å¦éœ€è¦é©±é€æ—§æ•°æ®
            current_size = self._current_cache_size()
            while current_size + data_size > self.max_cache_size and self._shared_cache:
                evicted_key = self._evict_least_recently_used()
                if not evicted_key:
                    break  # æ— æ³•é©±é€æ›´å¤šæ•°æ®
                current_size = self._current_cache_size()

            # 3. æ·»åŠ æ–°æ•°æ®
            self._shared_cache[key] = data
            self._cache_metadata[key] = {
                "size": data_size,
                "last_access": time.time(),
                "summary": summary,
                "access_count": 0
            }

            # 4. æ›´æ–°å³°å€¼å†…å­˜ç»Ÿè®¡
            current_size_mb = self._current_cache_size() / (1024 * 1024)
            if current_size_mb > self._metadata["peak_cache_size_mb"]:
                self._metadata["peak_cache_size_mb"] = current_size_mb

            # 5. ä¿å­˜summaryåˆ°å…ƒæ•°æ®
            if summary:
                self._metadata[f"{key}_summary"] = summary

    def get_shared_data(self, key: str, default: Any = None) -> Any:
        """
        è·å–å…±äº«æ•°æ®ï¼ˆPhase 4: æ›´æ–°LRUè®¿é—®æ—¶é—´ï¼‰

        Args:
            key: æ•°æ®é”®
            default: é»˜è®¤å€¼

        Returns:
            æ•°æ®å€¼
        """
        # Update access time for LRU tracking
        if key in self._cache_metadata:
            with self._lock:
                self._cache_metadata[key]["last_access"] = time.time()
                self._cache_metadata[key]["access_count"] += 1

        return self._shared_cache.get(key, default)

    def has_shared_data(self, key: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦å­˜åœ¨å…±äº«æ•°æ®

        Args:
            key: æ•°æ®é”®

        Returns:
            æ˜¯å¦å­˜åœ¨
        """
        return key in self._shared_cache

    def _calculate_deep_size(self, obj: Any) -> int:
        """
        é€’å½’è®¡ç®—å¯¹è±¡çš„æ·±åº¦å¤§å°ï¼ˆPhase 4 ä¼˜åŒ–ï¼‰

        Args:
            obj: è¦è®¡ç®—çš„å¯¹è±¡

        Returns:
            å¯¹è±¡å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        size = sys.getsizeof(obj)

        if isinstance(obj, dict):
            size += sum(self._calculate_deep_size(k) + self._calculate_deep_size(v)
                       for k, v in obj.items())
        elif isinstance(obj, (list, tuple, set)):
            size += sum(self._calculate_deep_size(item) for item in obj)

        return size

    def _current_cache_size(self) -> int:
        """
        è®¡ç®—å½“å‰ç¼“å­˜æ€»å¤§å°ï¼ˆPhase 4 ä¼˜åŒ–ï¼‰

        Returns:
            ç¼“å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        return sum(meta["size"] for meta in self._cache_metadata.values())

    def _evict_least_recently_used(self) -> Optional[str]:
        """
        é©±é€æœ€ä¹…æœªä½¿ç”¨çš„ç¼“å­˜é¡¹ï¼ˆPhase 4 ä¼˜åŒ–ï¼‰

        Returns:
            è¢«é©±é€çš„keyï¼Œå¦‚æœæ— æ³•é©±é€åˆ™è¿”å›None
        """
        if not self._cache_metadata:
            return None

        # æ‰¾åˆ°æœ€ä¹…æœªè®¿é—®çš„é¡¹
        lru_key = min(
            self._cache_metadata.items(),
            key=lambda x: x[1]["last_access"]
        )[0]

        # é©±é€è¯¥é¡¹
        if lru_key in self._shared_cache:
            evicted_size = self._cache_metadata[lru_key]["size"]
            del self._shared_cache[lru_key]
            del self._cache_metadata[lru_key]
            self._metadata["cache_evictions"] += 1

            # å¯é€‰ï¼šè®°å½•é©±é€æ—¥å¿—
            # print(f"âš ï¸  LRU Cache: é©±é€ '{lru_key}' ({evicted_size / 1024 / 1024:.2f} MB)")

            return lru_key

        return None

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆPhase 4 æ–°å¢ï¼‰

        Returns:
            ç¼“å­˜ç»Ÿè®¡æ•°æ®
        """
        current_size_mb = self._current_cache_size() / (1024 * 1024)
        max_size_mb = self.max_cache_size / (1024 * 1024)

        return {
            "current_size_mb": round(current_size_mb, 2),
            "max_size_mb": round(max_size_mb, 2),
            "usage_percent": round((current_size_mb / max_size_mb) * 100, 1),
            "cached_items": len(self._shared_cache),
            "peak_size_mb": round(self._metadata["peak_cache_size_mb"], 2),
            "total_evictions": self._metadata["cache_evictions"],
            "items_detail": [
                {
                    "key": key,
                    "size_mb": round(meta["size"] / (1024 * 1024), 2),
                    "access_count": meta["access_count"],
                    "last_access_ago_sec": round(time.time() - meta["last_access"], 1)
                }
                for key, meta in self._cache_metadata.items()
            ]
        }

    def get_summary(self) -> Dict[str, Any]:
        """
        è·å–ä¸Šä¸‹æ–‡æ‘˜è¦ (çº¿ç¨‹å®‰å…¨)

        Returns:
            æ‘˜è¦ä¿¡æ¯
        """
        with self._lock:
            return {
                "user_request": self.user_request,
                "total_agents_executed": self._metadata["total_agents_executed"],
                "execution_order": self._execution_order.copy(),
                "agents_results": list(self._results.keys()),
                "shared_cache_keys": list(self._shared_cache.keys()),
                "created_at": self._metadata["created_at"]
            }

    def export_for_agent(self, target_agent: str) -> Dict[str, Any]:
        """
        å¯¼å‡ºä¾›ç‰¹å®šAgentä½¿ç”¨çš„ä¸Šä¸‹æ–‡ä¿¡æ¯

        Args:
            target_agent: ç›®æ ‡Agentåç§°

        Returns:
            ç²¾ç®€çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        """
        # è·å–ä¹‹å‰æ‰§è¡Œçš„Agents
        previous_agents = [a for a in self._execution_order if a != target_agent]

        # æ„å»ºç²¾ç®€çš„ä¸Šä¸‹æ–‡
        context_for_agent = {
            "user_request": self.user_request,
            "previous_agents": previous_agents,
            "available_data": {}
        }

        # æ·»åŠ ä¹‹å‰Agentsçš„å…³é”®æ•°æ®æ‘˜è¦
        for agent_name in previous_agents:
            result = self._results.get(agent_name)
            if result:
                # åªæä¾›æ•°æ®æ‘˜è¦ï¼Œä¸åŒ…æ‹¬å®Œæ•´æŠ¥å‘Šï¼ˆå‡å°‘tokenæ¶ˆè€—ï¼‰
                data = result["data"]
                context_for_agent["available_data"][agent_name] = {
                    "summary": self._extract_data_summary(agent_name, data),
                    "execution_time": result["execution_time"]
                }

        # æ·»åŠ å…±äº«ç¼“å­˜
        if self._shared_cache:
            context_for_agent["shared_cache"] = self._shared_cache.copy()

        return context_for_agent

    def prewarm_cache(self, workflow_name: str) -> None:
        """
        ç¼“å­˜é¢„çƒ­ï¼šåœ¨å·¥ä½œæµå¼€å§‹å‰åå°å¹¶è¡ŒåŠ è½½æ•°æ®ï¼ˆPhase 4 Day 3ï¼‰

        æ ¹æ®å·¥ä½œæµç±»å‹ï¼Œæ™ºèƒ½é¢„æµ‹éœ€è¦çš„æ•°æ®å¹¶åå°åŠ è½½ï¼Œ
        å½“ Agent éœ€è¦æ—¶æ•°æ®å·²ç»å‡†å¤‡å¥½ã€‚

        Args:
            workflow_name: å·¥ä½œæµåç§°

        ä½¿ç”¨ç¤ºä¾‹:
            context = AgentContext("ç”¨æˆ·è¯·æ±‚", "data/packs/player")
            context.prewarm_cache("comprehensive_profile")  # åå°å¼€å§‹åŠ è½½
            # ... æ‰§è¡Œå…¶ä»–åˆå§‹åŒ– ...
            # å½“Agentéœ€è¦æ—¶ï¼Œæ•°æ®å·²åŠ è½½å®Œæˆ
        """
        from src.agents.shared.pack_data_loader import PackDataLoader

        # å®šä¹‰ä¸åŒå·¥ä½œæµçš„æ•°æ®éœ€æ±‚
        workflow_requirements = {
            "quick_diagnosis": ["recent_5_packs"],
            "comprehensive_profile": ["all_packs"],
            "role_mastery": ["all_packs"],
            "seasonal_review": ["all_packs"],
        }

        requirements = workflow_requirements.get(workflow_name, [])

        if not requirements:
            return  # æ— éœ€é¢„çƒ­

        print(f"ğŸ”„ ç¼“å­˜é¢„çƒ­: {workflow_name} å·¥ä½œæµ")

        # åˆ›å»ºåå°åŠ è½½ä»»åŠ¡
        executor = ThreadPoolExecutor(max_workers=1)

        for req in requirements:
            if req == "recent_5_packs":
                # åå°åŠ è½½æœ€è¿‘5ä¸ªç‰ˆæœ¬
                future = executor.submit(self._preload_recent_packs, 5)
                self._preload_futures["recent_packs"] = future
                print(f"   ğŸ”„ åå°åŠ è½½: æœ€è¿‘5ä¸ªç‰ˆæœ¬")

            elif req == "all_packs":
                # åå°åŠ è½½æ‰€æœ‰ç‰ˆæœ¬ï¼ˆä½¿ç”¨å¹¶è¡ŒåŠ è½½å™¨ï¼‰
                future = executor.submit(self._preload_all_packs)
                self._preload_futures["all_packs"] = future
                print(f"   ğŸ”„ åå°åŠ è½½: æ‰€æœ‰ç‰ˆæœ¬")

        executor.shutdown(wait=False)  # ä¸ç­‰å¾…ï¼Œè®©ä»»åŠ¡åœ¨åå°è¿è¡Œ

    def _preload_recent_packs(self, n: int = 5):
        """åå°åŠ è½½æœ€è¿‘Nä¸ªç‰ˆæœ¬"""
        from src.agents.shared.pack_data_loader import PackDataLoader

        try:
            loader = PackDataLoader(self.packs_dir)
            packs = loader.load_recent_n_parallel(n=n, max_workers=3)

            # å°†åŠ è½½çš„æ•°æ®æ·»åŠ åˆ°ç¼“å­˜
            self.add_shared_data(
                key="recent_packs",
                data=list(packs.values()),
                summary=f"{len(packs)}ä¸ªæœ€è¿‘ç‰ˆæœ¬ï¼ˆé¢„çƒ­åŠ è½½ï¼‰"
            )
            print(f"   âœ… é¢„çƒ­å®Œæˆ: recent_packs ({len(packs)} ä¸ªç‰ˆæœ¬)")

        except Exception as e:
            print(f"   âš ï¸  é¢„çƒ­å¤±è´¥: recent_packs - {e}")

    def _preload_all_packs(self):
        """åå°åŠ è½½æ‰€æœ‰ç‰ˆæœ¬"""
        from src.agents.shared.pack_data_loader import PackDataLoader

        try:
            loader = PackDataLoader(self.packs_dir)
            packs = loader.load_all_parallel(max_workers=5)

            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆæŒ‰patchæ’åºï¼‰
            all_packs_list = [packs[patch] for patch in sorted(packs.keys())]

            # å°†åŠ è½½çš„æ•°æ®æ·»åŠ åˆ°ç¼“å­˜
            self.add_shared_data(
                key="all_packs",
                data=all_packs_list,
                summary=f"{len(all_packs_list)}ä¸ªç‰ˆæœ¬ï¼ˆé¢„çƒ­åŠ è½½ï¼‰"
            )
            print(f"   âœ… é¢„çƒ­å®Œæˆ: all_packs ({len(all_packs_list)} ä¸ªç‰ˆæœ¬)")

        except Exception as e:
            print(f"   âš ï¸  é¢„çƒ­å¤±è´¥: all_packs - {e}")

    def wait_for_preload(self, key: str, timeout: float = 30.0) -> bool:
        """
        ç­‰å¾…ç‰¹å®šé¢„çƒ­ä»»åŠ¡å®Œæˆï¼ˆPhase 4 Day 3ï¼‰

        Args:
            key: é¢„çƒ­ä»»åŠ¡çš„keyï¼ˆå¦‚ "all_packs", "recent_packs"ï¼‰
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤30ç§’

        Returns:
            æ˜¯å¦æˆåŠŸå®Œæˆ

        ä½¿ç”¨ç¤ºä¾‹:
            context.prewarm_cache("comprehensive_profile")
            # ... åšå…¶ä»–äº‹æƒ… ...
            if context.wait_for_preload("all_packs"):
                # æ•°æ®å·²å‡†å¤‡å¥½ï¼Œå¯ä»¥ä½¿ç”¨
                packs = context.get_shared_data("all_packs")
        """
        future = self._preload_futures.get(key)

        if not future:
            # æ²¡æœ‰é¢„çƒ­ä»»åŠ¡ï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦å·²åœ¨ç¼“å­˜ä¸­
            return self.has_shared_data(key)

        try:
            # ç­‰å¾…ä»»åŠ¡å®Œæˆ
            future.result(timeout=timeout)
            return True
        except Exception as e:
            print(f"âš ï¸  ç­‰å¾…é¢„çƒ­å¤±è´¥ ({key}): {e}")
            return False

    def get_or_wait_preload(self, key: str, default: Any = None, timeout: float = 30.0) -> Any:
        """
        è·å–é¢„çƒ­æ•°æ®ï¼Œå¦‚æœæ­£åœ¨åŠ è½½åˆ™ç­‰å¾…å®Œæˆï¼ˆPhase 4 Day 3ï¼‰

        è¿™æ˜¯æœ€ä¾¿åˆ©çš„æ–¹æ³•ï¼Œè‡ªåŠ¨å¤„ç†ç­‰å¾…é€»è¾‘ã€‚

        Args:
            key: æ•°æ®key
            default: é»˜è®¤å€¼
            timeout: ç­‰å¾…è¶…æ—¶ï¼ˆç§’ï¼‰

        Returns:
            æ•°æ®å€¼

        ä½¿ç”¨ç¤ºä¾‹:
            context.prewarm_cache("comprehensive_profile")
            # ... Agentæ‰§è¡Œåˆ°éœ€è¦æ•°æ®çš„åœ°æ–¹ ...
            all_packs = context.get_or_wait_preload("all_packs")  # è‡ªåŠ¨ç­‰å¾…åŠ è½½å®Œæˆ
        """
        # å¦‚æœæ•°æ®å·²åœ¨ç¼“å­˜ä¸­ï¼Œç›´æ¥è¿”å›
        if self.has_shared_data(key):
            return self.get_shared_data(key, default)

        # å¦‚æœæœ‰é¢„çƒ­ä»»åŠ¡ï¼Œç­‰å¾…å®Œæˆ
        if key in self._preload_futures:
            success = self.wait_for_preload(key, timeout)
            if success:
                return self.get_shared_data(key, default)

        # è¿”å›é»˜è®¤å€¼
        return default

    def _extract_data_summary(self, agent_name: str, data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æå–æ•°æ®æ‘˜è¦ï¼ˆé¿å…ä¼ é€’å®Œæ•´æ•°æ®ï¼‰

        Args:
            agent_name: Agentåç§°
            data: å®Œæ•´æ•°æ®

        Returns:
            æ•°æ®æ‘˜è¦
        """
        summaries = {
            "annual_summary": lambda d: {
                "total_games": d.get("summary", {}).get("total_games", 0),
                "overall_winrate": d.get("summary", {}).get("overall_winrate", 0),
                "patches_analyzed": len(d.get("patches", [])),
                "top_champions": len(d.get("top_champions", []))
            },

            "weakness_analysis": lambda d: {
                "low_winrate_count": len(d.get("low_winrate_champions", [])),
                "weak_roles_count": len(d.get("weak_roles", [])),
                "patches_analyzed": d.get("total_patches_analyzed", 0)
            },

            "champion_recommendation": lambda d: {
                "recommendations_count": len(d.get("recommendations", [])),
                "core_champions_count": len(d.get("champion_pool", {}).get("core_champions", []))
            },

            "role_specialization": lambda d: {
                "role": d.get("role", "unknown"),
                "total_games": d.get("summary", {}).get("total_games", 0),
                "mastery_score": d.get("summary", {}).get("role_mastery_score", "N/A")
            },

            "multi_version": lambda d: {
                "total_patches": d.get("summary", {}).get("total_patches", 0),
                "unique_champions": d.get("summary", {}).get("unique_champion_roles", 0)
            }
        }

        # ä½¿ç”¨ä¸“é—¨çš„æå–å‡½æ•°ï¼Œå¦‚æœæ²¡æœ‰åˆ™è¿”å›åŸºæœ¬æ‘˜è¦
        extractor = summaries.get(agent_name, lambda d: {"has_data": True})

        try:
            return extractor(data)
        except Exception:
            return {"has_data": True, "error": "Failed to extract summary"}

    def to_dict(self) -> Dict[str, Any]:
        """
        è½¬æ¢ä¸ºå­—å…¸æ ¼å¼ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰

        Returns:
            å®Œæ•´çš„ä¸Šä¸‹æ–‡æ•°æ®
        """
        return {
            "user_request": self.user_request,
            "packs_dir": self.packs_dir,
            "results": self._results,
            "execution_order": self._execution_order,
            "shared_cache": self._shared_cache,
            "metadata": self._metadata
        }

    def save(self, output_path: str) -> None:
        """
        ä¿å­˜ä¸Šä¸‹æ–‡åˆ°æ–‡ä»¶

        Args:
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)

    @classmethod
    def load(cls, input_path: str) -> 'AgentContext':
        """
        ä»æ–‡ä»¶åŠ è½½ä¸Šä¸‹æ–‡

        Args:
            input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„

        Returns:
            AgentContextå®ä¾‹
        """
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        context = cls(
            user_request=data["user_request"],
            packs_dir=data["packs_dir"]
        )

        context._results = data["results"]
        context._execution_order = data["execution_order"]
        context._shared_cache = data["shared_cache"]
        context._metadata = data["metadata"]

        return context


def format_context_for_prompt(context: AgentContext, target_agent: str) -> str:
    """
    æ ¼å¼åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸ºLLMå‹å¥½çš„æ–‡æœ¬

    Args:
        context: Agentä¸Šä¸‹æ–‡
        target_agent: ç›®æ ‡Agentåç§°

    Returns:
        æ ¼å¼åŒ–çš„æ–‡æœ¬
    """
    lines = [f"# Agentæ‰§è¡Œä¸Šä¸‹æ–‡\n"]

    # ç”¨æˆ·è¯·æ±‚
    lines.append(f"**åŸå§‹ç”¨æˆ·è¯·æ±‚**: {context.user_request}\n")

    # å·²æ‰§è¡Œçš„Agents
    previous = context.get_previous_agents()
    if previous:
        lines.append(f"**å·²æ‰§è¡Œçš„Agents**: {', '.join(previous)}\n")

    # ä¸Šä¸‹æ–‡æ•°æ®
    ctx_data = context.export_for_agent(target_agent)

    if ctx_data.get("available_data"):
        lines.append("## å¯ç”¨çš„ä¸Šä¸‹æ–‡æ•°æ®\n")

        for agent_name, summary in ctx_data["available_data"].items():
            lines.append(f"### {agent_name}")
            summary_data = summary.get("summary", {})
            for key, value in summary_data.items():
                lines.append(f"- {key}: {value}")
            lines.append("")

    # å…±äº«ç¼“å­˜
    if ctx_data.get("shared_cache"):
        lines.append("## å…±äº«æ•°æ®ç¼“å­˜\n")
        for key, value in ctx_data["shared_cache"].items():
            lines.append(f"- **{key}**: {value}")

    return "\n".join(lines)
