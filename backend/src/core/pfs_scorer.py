#!/usr/bin/env python3
"""
PFS (Patch Fit Score) è¯„åˆ†æ¨¡å‹
æ ‡å‡†åŒ–+ç¨³å¥ç¼©å°¾+å¯è§£é‡Šæƒé‡
"""
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Tuple
import argparse

from .utils import (
    load_user_mode_config,
    standardize_pfs_inputs,
    format_output_precision,
    save_with_audit_trail
)


class PFSScorer:
    """PFSè¯„åˆ†å™¨"""

    def __init__(self, config_path: str = "configs/user_mode_params.yml"):
        """åˆå§‹åŒ–è¯„åˆ†å™¨"""
        self.config = load_user_mode_config(config_path)
        self.scoring_config = self.config['pfs_scoring']
        self.weights = self.scoring_config['weights']
        self.thresholds = self.scoring_config['thresholds']
        self.robustness = self.scoring_config['robustness']

    def calculate_pfs_scores(self, records: List[Dict[str, Any]]) -> Tuple[List[Dict], Dict]:
        """
        è®¡ç®—PFSè¯„åˆ†

        Args:
            records: è¾“å…¥è®°å½•åˆ—è¡¨

        Returns:
            (å¸¦PFSè¯„åˆ†çš„è®°å½•åˆ—è¡¨, æ ¡å‡†æŠ¥å‘Š)
        """
        print(f"ğŸ¯ è®¡ç®—PFSè¯„åˆ†ï¼Œè¾“å…¥ {len(records)} æ¡è®°å½•")

        if not records:
            return [], {'calibration_stats': {}, 'threshold_stats': {}}

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(records)

        # æ ‡å‡†åŒ–è¾“å…¥
        df = standardize_pfs_inputs(df, self.config)

        # è®¡ç®—å„ç»„ä»¶è¯„åˆ†
        df = self._calculate_skill_score(df)
        df = self._calculate_stability_score(df)
        df = self._calculate_meta_alignment_score(df)
        df = self._calculate_volatility_penalty(df)

        # è®¡ç®—æœ€ç»ˆPFSè¯„åˆ†
        df = self._calculate_final_pfs(df)

        # ç”Ÿæˆæ ¡å‡†æŠ¥å‘Š
        calibration_report = self._generate_calibration_report(df)

        # æ·»åŠ PFSç­‰çº§
        df = self._assign_pfs_levels(df)

        # è½¬æ¢å›è®°å½•åˆ—è¡¨
        scored_records = df.to_dict('records')

        # æ¸…ç†å’Œæ ¼å¼åŒ–è¾“å‡º
        for record in scored_records:
            # æ ¼å¼åŒ–PFSç›¸å…³å­—æ®µ
            for field in ['z_skill', 'stability_score', 'meta_alignment', 'volatility_penalty', 'pfs_score']:
                if field in record:
                    record[field] = format_output_precision(record[field])

            # æ¸…ç†ä¸´æ—¶å­—æ®µ
            temp_fields = ['ci_width', 'z_volatility']
            for field in temp_fields:
                record.pop(field, None)

        print(f"âœ… PFSè¯„åˆ†å®Œæˆ")
        print(f"  å¹³å‡PFS: {df['pfs_score'].mean():.3f}")
        print(f"  å¼ºæ¨è®°å½•: {(df['pfs_level'] == 'strong_recommend').sum()}")
        print(f"  è°¨æ…å°è¯•: {(df['pfs_level'] == 'cautious_try').sum()}")
        print(f"  ä¸å»ºè®®: {(df['pfs_level'] == 'not_recommend').sum()}")

        return scored_records, calibration_report

    def _calculate_skill_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æŠ€èƒ½åˆ†ï¼ˆæ ‡å‡†åŒ–z-scoreï¼‰"""
        if 'z_skill' not in df.columns:
            # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„z_skillï¼Œä½¿ç”¨ç®€å•ç‰ˆæœ¬
            if 'winrate_delta' in df.columns:
                df['z_skill'] = df['winrate_delta']
            else:
                df['z_skill'] = 0

        # ç¨³å¥ç¼©å°¾
        lower_bound = df['z_skill'].quantile(self.robustness['winsorize_lower'])
        upper_bound = df['z_skill'].quantile(self.robustness['winsorize_upper'])
        df['z_skill'] = df['z_skill'].clip(lower=lower_bound, upper=upper_bound)

        return df

    def _calculate_stability_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—ç¨³å®šæ€§è¯„åˆ†"""
        if 'stability' in df.columns:
            df['stability_score'] = df['stability']
        else:
            # åŸºäºCIå®½åº¦è®¡ç®—ç¨³å®šæ€§
            if 'ci_width' in df.columns:
                # CIè¶Šçª„è¶Šç¨³å®šï¼Œå½’ä¸€åŒ–åˆ°[0,1]
                max_ci_width = df['ci_width'].quantile(0.95)  # ä½¿ç”¨95åˆ†ä½æ•°é¿å…æå€¼
                df['stability_score'] = 1 - (df['ci_width'] / max_ci_width).clip(0, 1)
            else:
                df['stability_score'] = 0.5  # é»˜è®¤ä¸­ç­‰ç¨³å®šæ€§

        return df

    def _calculate_meta_alignment_score(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—å…ƒé€‚é…è¯„åˆ†ï¼ˆç‰ˆæœ¬é€‚é…åº¦ï¼‰"""
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåŸºäºpatchå’Œroleçš„é€‚é…åº¦
        # å®é™…åº”è¯¥ç»“åˆshock_adjustç­‰æ›´å¤æ‚æŒ‡æ ‡

        if 'patch_id' in df.columns and 'role' in df.columns:
            # æŒ‰patchå’Œroleåˆ†ç»„ï¼Œè®¡ç®—ç›¸å¯¹è¡¨ç°
            df['meta_alignment'] = 0.0

            for (patch, role), group in df.groupby(['patch_id', 'role']):
                if len(group) > 1:
                    # ç»„å†…ç›¸å¯¹è¡¨ç°
                    group_mean = group['winrate_delta'].mean()
                    group_std = group['winrate_delta'].std()

                    if group_std > 0:
                        # æ ‡å‡†åŒ–åˆ°[-1, 1]èŒƒå›´
                        relative_performance = (group['winrate_delta'] - group_mean) / group_std
                        relative_performance = relative_performance.clip(-1, 1)
                        df.loc[group.index, 'meta_alignment'] = relative_performance
                    else:
                        df.loc[group.index, 'meta_alignment'] = 0.0
                else:
                    # å•è®°å½•ç»„ï¼Œä½¿ç”¨ç»å¯¹è¡¨ç°
                    df.loc[group.index, 'meta_alignment'] = np.sign(group['winrate_delta'].iloc[0]) * 0.5
        else:
            df['meta_alignment'] = 0.0

        return df

    def _calculate_volatility_penalty(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æ³¢åŠ¨é£é™©æƒ©ç½š"""
        if 'z_volatility' in df.columns:
            # å·²æ ‡å‡†åŒ–çš„æ³¢åŠ¨æ€§
            df['volatility_penalty'] = df['z_volatility']
        elif 'ci_width' in df.columns:
            # åŸºäºCIå®½åº¦
            median_ci = df['ci_width'].median()
            if median_ci > 0:
                df['volatility_penalty'] = df['ci_width'] / median_ci
            else:
                df['volatility_penalty'] = 1.0
        else:
            df['volatility_penalty'] = 1.0

        return df

    def _calculate_final_pfs(self, df: pd.DataFrame) -> pd.DataFrame:
        """è®¡ç®—æœ€ç»ˆPFSè¯„åˆ†"""
        # ä½¿ç”¨é…ç½®ä¸­çš„æƒé‡
        pfs_formula = (
            self.weights['skill_score'] * df['z_skill'] +
            self.weights['stability'] * df['stability_score'] +
            self.weights['meta_alignment'] * df['meta_alignment'] +
            self.weights['volatility_penalty'] * df['volatility_penalty']  # æ³¨æ„ï¼šè¿™æ˜¯è´Ÿæƒé‡
        )

        df['pfs_score'] = pfs_formula

        return df

    def _assign_pfs_levels(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ†é…PFSç­‰çº§"""
        conditions = [
            df['pfs_score'] >= self.thresholds['strong_recommend'],
            df['pfs_score'] >= self.thresholds['cautious_try'],
            df['pfs_score'] < self.thresholds['not_recommend']
        ]

        choices = ['strong_recommend', 'cautious_try', 'not_recommend']

        df['pfs_level'] = np.select(conditions, choices, default='not_recommend')

        return df

    def _generate_calibration_report(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ç”Ÿæˆæ ¡å‡†æŠ¥å‘Š"""
        if df.empty:
            return {'calibration_stats': {}, 'threshold_stats': {}}

        # åŸºç¡€ç»Ÿè®¡
        calibration_stats = {
            'total_records': len(df),
            'pfs_score_mean': format_output_precision(df['pfs_score'].mean()),
            'pfs_score_std': format_output_precision(df['pfs_score'].std()),
            'pfs_score_min': format_output_precision(df['pfs_score'].min()),
            'pfs_score_max': format_output_precision(df['pfs_score'].max()),
            'pfs_score_percentiles': {
                'p25': format_output_precision(df['pfs_score'].quantile(0.25)),
                'p50': format_output_precision(df['pfs_score'].quantile(0.50)),
                'p75': format_output_precision(df['pfs_score'].quantile(0.75)),
                'p90': format_output_precision(df['pfs_score'].quantile(0.90)),
                'p95': format_output_precision(df['pfs_score'].quantile(0.95))
            }
        }

        # é˜ˆå€¼å‡»ç©¿ç‡ç»Ÿè®¡
        threshold_stats = {}
        for level_name, threshold in self.thresholds.items():
            hit_count = (df['pfs_score'] >= threshold).sum()
            hit_rate = hit_count / len(df)
            threshold_stats[level_name] = {
                'threshold': threshold,
                'hit_count': int(hit_count),
                'hit_rate': format_output_precision(hit_rate, is_probability=True)
            }

        # æŒ‰patchå’Œroleåˆ†ç»„çš„æ ¡å‡†
        groupby_calibration = {}
        if 'patch_id' in df.columns and 'role' in df.columns:
            for (patch, role), group in df.groupby(['patch_id', 'role']):
                if len(group) >= 3:  # è‡³å°‘3ä¸ªè®°å½•æ‰åšæ ¡å‡†
                    group_key = f"{patch}_{role}"
                    groupby_calibration[group_key] = {
                        'count': len(group),
                        'pfs_mean': format_output_precision(group['pfs_score'].mean()),
                        'strong_recommend_rate': format_output_precision(
                            (group['pfs_level'] == 'strong_recommend').mean(), is_probability=True
                        )
                    }

        # æ£€æŸ¥æ ¡å‡†å¥åº·åº¦
        strong_hit_rate = threshold_stats.get('strong_recommend', {}).get('hit_rate', 0)
        calibration_health = 'healthy'

        if strong_hit_rate < 0.20:
            calibration_health = 'too_strict'  # é˜ˆå€¼è¿‡ä¸¥
        elif strong_hit_rate > 0.40:
            calibration_health = 'too_loose'  # é˜ˆå€¼è¿‡æ¾

        calibration_report = {
            'calibration_stats': calibration_stats,
            'threshold_stats': threshold_stats,
            'groupby_calibration': groupby_calibration,
            'calibration_health': calibration_health,
            'generated_at': pd.Timestamp.now().isoformat()
        }

        return calibration_report

    def save_calibration_report(self, calibration_report: Dict[str, Any],
                               output_file: str = "data/user_mode/pfs_calibration.json") -> str:
        """ä¿å­˜æ ¡å‡†æŠ¥å‘Š"""
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(calibration_report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“Š PFSæ ¡å‡†æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return str(output_file)

    def get_recommendations_by_level(self, scored_records: List[Dict[str, Any]],
                                   level: str = 'strong_recommend') -> List[Dict[str, Any]]:
        """æŒ‰PFSç­‰çº§è·å–æ¨è"""
        return [record for record in scored_records if record.get('pfs_level') == level]


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="PFSè¯„åˆ†è®¡ç®—")
    parser.add_argument("--input", required=True, help="è¾“å…¥è®°å½•æ–‡ä»¶")
    parser.add_argument("--output", default="data/user_mode", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--level", choices=['strong_recommend', 'cautious_try', 'not_recommend'],
                       help="ä»…è¾“å‡ºæŒ‡å®šç­‰çº§çš„è®°å½•")
    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½è®°å½•: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        if args.input.endswith('.jsonl'):
            records = [json.loads(line) for line in f if line.strip()]
        else:
            records = json.load(f)

    print(f"  å‘ç° {len(records)} æ¡è®°å½•")

    # è®¡ç®—PFSè¯„åˆ†
    scorer = PFSScorer()
    scored_records, calibration_report = scorer.calculate_pfs_scores(records)

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜è¯„åˆ†è®°å½•
    scored_file = output_dir / "pfs_scored_records.jsonl"
    save_with_audit_trail(scored_records, scored_file, {'type': 'pfs_scored'})

    # ä¿å­˜æ ¡å‡†æŠ¥å‘Š
    calibration_file = scorer.save_calibration_report(calibration_report,
                                                     output_dir / "pfs_calibration.json")

    # å¦‚æœæŒ‡å®šäº†ç‰¹å®šç­‰çº§
    if args.level:
        level_records = scorer.get_recommendations_by_level(scored_records, args.level)
        level_file = output_dir / f"pfs_{args.level}.jsonl"
        save_with_audit_trail(level_records, level_file, {'type': f'pfs_{args.level}'})
        print(f"ğŸ“‹ {args.level}è®°å½•: {len(level_records)} æ¡ â†’ {level_file}")

    print(f"âœ… PFSè¯„åˆ†å®Œæˆï¼")
    print(f"  è¯„åˆ†è®°å½•: {len(scored_records)} â†’ {scored_file}")
    print(f"  æ ¡å‡†æŠ¥å‘Š: {calibration_file}")

    # æ˜¾ç¤ºæ ¡å‡†å¥åº·åº¦
    health = calibration_report.get('calibration_health', 'unknown')
    print(f"  æ ¡å‡†çŠ¶æ€: {health}")


if __name__ == "__main__":
    main()