#!/usr/bin/env python3
"""
User-Modeæ²»ç†å‰ç½®æ£€æŸ¥å™¨
åœ¨è¯„åˆ†å™¨ä¹‹å‰å¼ºåˆ¶æ‰§è¡Œæ²»ç†çº¢çº¿ï¼Œç¡®ä¿æ•°æ®è´¨é‡
"""
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Tuple
import argparse

from .utils import (
    load_user_mode_config,
    apply_governance_tag,
    filter_by_governance,
    format_output_precision
)


class UserGovernanceChecker:
    """User-Modeæ²»ç†æ£€æŸ¥å™¨"""

    def __init__(self, config_path: str = "configs/user_mode_params.yml"):
        """åˆå§‹åŒ–æ£€æŸ¥å™¨"""
        self.config = load_user_mode_config(config_path)
        self.red_lines = self.config['governance']['red_lines']
        self.grading = self.config['governance']['evidence_grading']

    def check_governance(self, records: List[Dict[str, Any]],
                        current_patch: str = None) -> Tuple[List[Dict], List[Dict], Dict]:
        """
        æ‰§è¡Œæ²»ç†æ£€æŸ¥å’Œåˆ†çº§

        Args:
            records: è¾“å…¥è®°å½•åˆ—è¡¨
            current_patch: å½“å‰patchï¼ˆç”¨äº+1 bufferæ£€æŸ¥ï¼‰

        Returns:
            (ç¬¦åˆrecords, è¢«æ‹’ç»records, æ£€æŸ¥æŠ¥å‘Š)
        """
        print(f"ğŸ›¡ï¸ å¼€å§‹æ²»ç†æ£€æŸ¥ï¼Œè¾“å…¥ {len(records)} æ¡è®°å½•")

        compliant_records = []
        rejected_records = []
        check_report = {
            'total_input': len(records),
            'passed_count': 0,
            'rejected_count': 0,
            'rejection_reasons': {},
            'grading_stats': {'CONFIDENT': 0, 'CAUTION': 0, 'CONTEXT': 0}
        }

        for record in records:
            # æ‰§è¡Œçº¢çº¿æ£€æŸ¥
            passed, rejection_reason = self._check_red_lines(record, current_patch)

            if not passed:
                # è®°å½•æ‹’ç»åŸå› 
                if rejection_reason not in check_report['rejection_reasons']:
                    check_report['rejection_reasons'][rejection_reason] = 0
                check_report['rejection_reasons'][rejection_reason] += 1

                rejected_records.append({
                    **record,
                    'rejection_reason': rejection_reason,
                    'governance_tag': 'REJECTED'
                })
                check_report['rejected_count'] += 1
                continue

            # é€šè¿‡çº¢çº¿æ£€æŸ¥ï¼Œè¿›è¡Œè¯æ®åˆ†çº§
            governance_tag = apply_governance_tag(record, self.config)
            record['governance_tag'] = governance_tag

            compliant_records.append(record)
            check_report['passed_count'] += 1
            check_report['grading_stats'][governance_tag] += 1

        print(f"âœ… æ²»ç†æ£€æŸ¥å®Œæˆ:")
        print(f"  é€šè¿‡: {check_report['passed_count']}")
        print(f"  æ‹’ç»: {check_report['rejected_count']}")
        print(f"  åˆ†çº§: {check_report['grading_stats']}")

        if check_report['rejection_reasons']:
            print(f"  æ‹’ç»åŸå› : {check_report['rejection_reasons']}")

        return compliant_records, rejected_records, check_report

    def _check_red_lines(self, record: Dict[str, Any], current_patch: str = None) -> Tuple[bool, str]:
        """æ£€æŸ¥æ²»ç†çº¢çº¿"""

        # 1. æ£€æŸ¥coarseçº§åˆ«
        if self.red_lines['ban_coarse_evidence']:
            aggregation_level = record.get('aggregation_level', '')
            if 'coarse' in aggregation_level.lower():
                return False, "banned_coarse_level"

        # 2. æ£€æŸ¥åˆæˆæ•°æ®å æ¯”
        synthetic_share = record.get('synthetic_share', 0)
        if synthetic_share > self.red_lines['max_synthetic_share']:
            return False, f"synthetic_share_too_high_{synthetic_share:.3f}"

        # 3. æ£€æŸ¥patch bufferï¼ˆ+1 patché˜²æœªæ¥ä¿¡æ¯æ³„æ¼ï¼‰
        if current_patch and self.red_lines['patch_buffer'] > 0:
            record_patch = record.get('patch_id', '')
            if self._is_future_patch(record_patch, current_patch):
                return False, f"future_patch_violation_{record_patch}_{current_patch}"

        # 4. æ£€æŸ¥åŸºæœ¬æ•°æ®å®Œæ•´æ€§
        required_fields = ['n', 'w', 'p_hat', 'ci']
        for field in required_fields:
            if field not in record or record[field] is None:
                return False, f"missing_field_{field}"

        # 5. æ£€æŸ¥æ•°å€¼åˆç†æ€§
        n = record.get('n', 0)
        w = record.get('w', 0)
        p_hat = record.get('p_hat', 0)

        if n <= 0:
            return False, "invalid_sample_size"

        if w < 0 or w > n:
            return False, "invalid_win_count"

        if not (0 <= p_hat <= 1):
            return False, "invalid_probability"

        # 6. æ£€æŸ¥CIåˆç†æ€§
        ci = record.get('ci', {})
        if not isinstance(ci, dict) or 'lo' not in ci or 'hi' not in ci:
            return False, "invalid_ci_format"

        ci_lo = ci.get('lo', 0)
        ci_hi = ci.get('hi', 1)

        if not (0 <= ci_lo <= ci_hi <= 1):
            return False, "invalid_ci_bounds"

        return True, ""

    def _is_future_patch(self, record_patch: str, current_patch: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºæœªæ¥patchï¼ˆç®€å•ç‰ˆæœ¬æ¯”è¾ƒï¼‰"""
        try:
            # ç®€åŒ–çš„patchæ¯”è¾ƒï¼ˆå‡è®¾æ ¼å¼ä¸º "15.13.1"ï¼‰
            record_parts = [int(x) for x in record_patch.split('.')]
            current_parts = [int(x) for x in current_patch.split('.')]

            # æ¯”è¾ƒä¸»ç‰ˆæœ¬ã€æ¬¡ç‰ˆæœ¬ã€ä¿®è®¢ç‰ˆæœ¬
            for i in range(min(len(record_parts), len(current_parts))):
                if record_parts[i] > current_parts[i]:
                    return True
                elif record_parts[i] < current_parts[i]:
                    return False

            # å¦‚æœæ‰€æœ‰å¯¹æ¯”éƒ¨åˆ†ç›¸ç­‰ï¼Œè¾ƒé•¿ç‰ˆæœ¬å·ä¸ºæ–°ç‰ˆæœ¬
            return len(record_parts) > len(current_parts)

        except (ValueError, AttributeError):
            # æ— æ³•è§£æç‰ˆæœ¬å·ï¼Œè°¨æ…èµ·è§è®¤ä¸ºæ˜¯æœªæ¥ç‰ˆæœ¬
            return True

    def filter_for_evidence(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿‡æ»¤å‡ºå¯ç”¨ä½œè¯æ®çš„è®°å½•ï¼ˆä»…CONFIDENTå’ŒCAUTIONï¼‰"""
        return filter_by_governance(records, allowed_tags=['CONFIDENT', 'CAUTION'], config=self.config)

    def filter_for_context(self, records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¿‡æ»¤å‡ºä»…ç”¨ä½œä¸Šä¸‹æ–‡çš„è®°å½•ï¼ˆä»…CONTEXTï¼‰"""
        return filter_by_governance(records, allowed_tags=['CONTEXT'], config=self.config)

    def generate_governance_summary(self, records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ²»ç†æ±‡æ€»æŠ¥å‘Š"""
        if not records:
            return {
                'total_records': 0,
                'governance_distribution': {},
                'quality_metrics': {},
                'compliance_rate': 0.0
            }

        df = pd.DataFrame(records)

        # æ²»ç†æ ‡ç­¾åˆ†å¸ƒ
        governance_dist = df['governance_tag'].value_counts().to_dict()

        # è´¨é‡æŒ‡æ ‡
        confident_count = governance_dist.get('CONFIDENT', 0)
        caution_count = governance_dist.get('CAUTION', 0)
        context_count = governance_dist.get('CONTEXT', 0)
        evidence_count = confident_count + caution_count

        # è®¡ç®—åˆè§„ç‡
        total_count = len(df)
        compliance_rate = evidence_count / total_count if total_count > 0 else 0

        # ç»Ÿè®¡æŒ‡æ ‡
        quality_metrics = {
            'evidence_count': evidence_count,
            'confident_ratio': confident_count / total_count if total_count > 0 else 0,
            'caution_ratio': caution_count / total_count if total_count > 0 else 0,
            'context_ratio': context_count / total_count if total_count > 0 else 0,
            'avg_sample_size': df['n'].mean() if 'n' in df.columns else 0,
            'avg_effective_n': df['effective_n'].mean() if 'effective_n' in df.columns else 0,
            'uses_prior_ratio': df['uses_prior'].mean() if 'uses_prior' in df.columns else 0
        }

        # æ ¼å¼åŒ–è¾“å‡º
        for key, value in quality_metrics.items():
            if isinstance(value, float):
                quality_metrics[key] = format_output_precision(value, is_probability=True)

        summary = {
            'total_records': total_count,
            'governance_distribution': governance_dist,
            'quality_metrics': quality_metrics,
            'compliance_rate': format_output_precision(compliance_rate, is_probability=True)
        }

        return summary

    def save_governance_report(self, check_report: Dict[str, Any],
                              governance_summary: Dict[str, Any],
                              output_file: str = "data/user_mode/governance_report.json") -> str:
        """ä¿å­˜æ²»ç†æŠ¥å‘Š"""
        output_file = Path(output_file)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        combined_report = {
            'check_report': check_report,
            'governance_summary': governance_summary,
            'generated_at': pd.Timestamp.now().isoformat(),
            'config_summary': {
                'red_lines': self.red_lines,
                'grading_thresholds': self.grading
            }
        }

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(combined_report, f, indent=2, ensure_ascii=False)

        print(f"ğŸ“‹ æ²»ç†æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        return str(output_file)


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="User-Modeæ²»ç†æ£€æŸ¥")
    parser.add_argument("--input", required=True, help="è¾“å…¥è®°å½•æ–‡ä»¶")
    parser.add_argument("--output", default="data/user_mode", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--current-patch", help="å½“å‰patchï¼ˆç”¨äº+1 bufferæ£€æŸ¥ï¼‰")
    parser.add_argument("--evidence-only", action="store_true", help="ä»…è¾“å‡ºè¯æ®çº§è®°å½•")
    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½è®°å½•: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        if args.input.endswith('.jsonl'):
            records = [json.loads(line) for line in f if line.strip()]
        else:
            records = json.load(f)

    print(f"  å‘ç° {len(records)} æ¡è®°å½•")

    # æ‰§è¡Œæ²»ç†æ£€æŸ¥
    checker = UserGovernanceChecker()
    compliant_records, rejected_records, check_report = checker.check_governance(
        records, args.current_patch
    )

    # ç”Ÿæˆæ²»ç†æ±‡æ€»
    governance_summary = checker.generate_governance_summary(compliant_records)

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜åˆè§„è®°å½•
    compliant_file = output_dir / "compliant_records.jsonl"
    with open(compliant_file, 'w', encoding='utf-8') as f:
        for record in compliant_records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')

    # ä¿å­˜è¢«æ‹’ç»è®°å½•
    rejected_file = output_dir / "rejected_records.jsonl"
    with open(rejected_file, 'w', encoding='utf-8') as f:
        for record in rejected_records:
            f.write(json.dumps(record, ensure_ascii=False) + '\n')

    # ä¿å­˜æ²»ç†æŠ¥å‘Š
    report_file = checker.save_governance_report(check_report, governance_summary,
                                                output_dir / "governance_report.json")

    # å¦‚æœåªè¦è¯æ®çº§è®°å½•
    if args.evidence_only:
        evidence_records = checker.filter_for_evidence(compliant_records)
        evidence_file = output_dir / "evidence_only.jsonl"
        with open(evidence_file, 'w', encoding='utf-8') as f:
            for record in evidence_records:
                f.write(json.dumps(record, ensure_ascii=False) + '\n')
        print(f"ğŸ“‹ è¯æ®è®°å½•: {len(evidence_records)} æ¡ â†’ {evidence_file}")

    print(f"âœ… æ²»ç†æ£€æŸ¥å®Œæˆï¼")
    print(f"  åˆè§„è®°å½•: {len(compliant_records)} â†’ {compliant_file}")
    print(f"  æ‹’ç»è®°å½•: {len(rejected_records)} â†’ {rejected_file}")
    print(f"  æ²»ç†æŠ¥å‘Š: {report_file}")


if __name__ == "__main__":
    main()