#!/usr/bin/env python3
"""
Rule-basedé™çº§æ¨¡æ¿ç”Ÿæˆå™¨
å½“Bedrockå¼‚å¸¸æˆ–è¯æ®ä¸è¶³æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
"""
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime
import argparse

from .utils import (
    load_user_mode_config,
    format_output_precision,
    safe_float_convert,
    safe_int_convert
)


class RuleBasedFallback:
    """Rule-basedé™çº§ç³»ç»Ÿ"""

    def __init__(self, config_path: str = "configs/user_mode_params.yml"):
        """åˆå§‹åŒ–é™çº§ç³»ç»Ÿ"""
        self.config = load_user_mode_config(config_path)
        self.fallback_templates = self._load_fallback_templates()

    def _load_fallback_templates(self) -> Dict[str, Any]:
        """åŠ è½½é™çº§æ¨¡æ¿"""
        return {
            "observation_card": {
                "type": "observation_only",
                "title": "æ•°æ®è§‚å¯Ÿå¡",
                "description": "åŸºäºç°æœ‰è¯æ®çš„è§‚å¯Ÿæ€§åˆ†æï¼Œä¸æä¾›è¡ŒåŠ¨å»ºè®®",
                "min_evidence_threshold": 5,
                "sections": ["evidence_summary", "patterns", "data_gaps"]
            },
            "rule_based_card": {
                "type": "rule_based_advice",
                "title": "åŸºç¡€æ•™ç»ƒå¡",
                "description": "åŸºäºè§„åˆ™å’ŒCONFIDENTè¯æ®çš„åŸºç¡€å»ºè®®",
                "min_confident_threshold": 2,
                "sections": ["strengths", "improvements", "evidence_citations"]
            },
            "insufficient_data_card": {
                "type": "insufficient_data",
                "title": "æ•°æ®ä¸è¶³æç¤º",
                "description": "æŒ‡å¯¼ç”¨æˆ·å¦‚ä½•è·å¾—æ›´å¤šæœ‰æ•ˆæ•°æ®",
                "sections": ["current_status", "requirements", "recommendations"]
            }
        }

    def generate_fallback_card(self, evidence_records: List[Dict[str, Any]],
                              fallback_reason: str = "bedrock_failure") -> Dict[str, Any]:
        """
        ç”Ÿæˆé™çº§å¡ç‰‡

        Args:
            evidence_records: è¯æ®è®°å½•åˆ—è¡¨
            fallback_reason: é™çº§åŸå›  (bedrock_failure, insufficient_evidence, cost_limit)

        Returns:
            é™çº§å¡ç‰‡æ•°æ®
        """
        print(f"ğŸ›¡ï¸ è§¦å‘é™çº§: {fallback_reason}, è¯æ®è®°å½•: {len(evidence_records)}")

        # åˆ†æè¯æ®è´¨é‡
        evidence_analysis = self._analyze_evidence_quality(evidence_records)

        # é€‰æ‹©é™çº§ç­–ç•¥
        fallback_strategy = self._select_fallback_strategy(evidence_analysis, fallback_reason)

        # ç”Ÿæˆå¯¹åº”å¡ç‰‡
        if fallback_strategy == "insufficient_data":
            return self._generate_insufficient_data_card(evidence_analysis)
        elif fallback_strategy == "observation_only":
            return self._generate_observation_card(evidence_records, evidence_analysis)
        elif fallback_strategy == "rule_based":
            return self._generate_rule_based_card(evidence_records, evidence_analysis)
        else:
            return self._generate_error_card(fallback_reason)

    def _analyze_evidence_quality(self, evidence_records: List[Dict[str, Any]]) -> Dict[str, Any]:
        """åˆ†æè¯æ®è´¨é‡"""
        if not evidence_records:
            return {
                "total_count": 0,
                "confident_count": 0,
                "caution_count": 0,
                "context_count": 0,
                "confident_ratio": 0.0,
                "avg_sample_size": 0,
                "coverage": {},
                "quality_level": "insufficient"
            }

        df = pd.DataFrame(evidence_records)

        # æ²»ç†æ ‡ç­¾ç»Ÿè®¡
        governance_counts = df['governance_tag'].value_counts().to_dict()
        confident_count = governance_counts.get('CONFIDENT', 0)
        caution_count = governance_counts.get('CAUTION', 0)
        context_count = governance_counts.get('CONTEXT', 0)
        total_count = len(df)

        # æ ·æœ¬é‡ç»Ÿè®¡
        avg_sample_size = df['n'].mean() if 'n' in df.columns else 0
        avg_effective_n = df['effective_n'].mean() if 'effective_n' in df.columns else 0

        # è¦†ç›–åº¦åˆ†æ
        coverage = self._analyze_coverage(df)

        # è´¨é‡ç­‰çº§åˆ¤å®š
        quality_level = self._determine_quality_level(confident_count, caution_count, total_count)

        return {
            "total_count": total_count,
            "confident_count": confident_count,
            "caution_count": caution_count,
            "context_count": context_count,
            "confident_ratio": confident_count / total_count if total_count > 0 else 0,
            "caution_ratio": caution_count / total_count if total_count > 0 else 0,
            "avg_sample_size": format_output_precision(avg_sample_size),
            "avg_effective_n": format_output_precision(avg_effective_n),
            "coverage": coverage,
            "quality_level": quality_level
        }

    def _analyze_coverage(self, df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ†ææ•°æ®è¦†ç›–åº¦"""
        coverage = {
            "patch_coverage": [],
            "role_coverage": [],
            "champion_coverage": [],
            "queue_coverage": []
        }

        if 'patch_id' in df.columns:
            coverage["patch_coverage"] = df['patch_id'].unique().tolist()

        if 'role' in df.columns:
            coverage["role_coverage"] = df['role'].unique().tolist()

        if 'champion_id' in df.columns:
            coverage["champion_coverage"] = df['champion_id'].nunique()

        if 'queue' in df.columns:
            coverage["queue_coverage"] = df['queue'].unique().tolist()

        return coverage

    def _determine_quality_level(self, confident_count: int, caution_count: int, total_count: int) -> str:
        """åˆ¤å®šè¯æ®è´¨é‡ç­‰çº§"""
        if total_count < 5:
            return "insufficient"
        elif confident_count >= 10:
            return "high"
        elif confident_count >= 3 or (confident_count + caution_count) >= 8:
            return "medium"
        elif confident_count >= 1 or caution_count >= 3:
            return "low"
        else:
            return "insufficient"

    def _select_fallback_strategy(self, evidence_analysis: Dict[str, Any],
                                 fallback_reason: str) -> str:
        """é€‰æ‹©é™çº§ç­–ç•¥"""
        quality_level = evidence_analysis["quality_level"]
        confident_count = evidence_analysis["confident_count"]
        total_count = evidence_analysis["total_count"]

        # è¯æ®ä¸è¶³ -> æ•°æ®ä¸è¶³å¡
        if quality_level == "insufficient" or total_count < 5:
            return "insufficient_data"

        # æœ‰è¶³å¤ŸCONFIDENTè¯æ® -> Rule-basedå»ºè®®
        elif confident_count >= 2 and quality_level in ["high", "medium"]:
            return "rule_based"

        # è¯æ®è´¨é‡ä¸€èˆ¬ -> è§‚å¯Ÿå¡
        else:
            return "observation_only"

    def _generate_insufficient_data_card(self, evidence_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•°æ®ä¸è¶³å¡ç‰‡"""
        current_count = evidence_analysis["total_count"]
        confident_count = evidence_analysis["confident_count"]

        # è®¡ç®—ç¼ºå£
        min_total_needed = 20
        min_confident_needed = 3

        total_gap = max(0, min_total_needed - current_count)
        confident_gap = max(0, min_confident_needed - confident_count)

        return {
            "card_type": "insufficient_data",
            "status": "data_insufficient",
            "title": "æ•°æ®ç§¯ç´¯ä¸­ - ç»§ç»­æ¯”èµ›è·å¾—æ›´å‡†ç¡®åˆ†æ",
            "summary": f"å½“å‰æ•°æ®é‡ä¸è¶³ä»¥ç”Ÿæˆå¯é çš„æ•™ç»ƒå»ºè®®ã€‚å·²æ”¶é›† {current_count} æ¡è®°å½•ï¼Œéœ€è¦æ›´å¤šæ¯”èµ›æ•°æ®ã€‚",

            "current_status": {
                "total_matches_analyzed": current_count,
                "confident_evidence": confident_count,
                "data_quality": evidence_analysis["quality_level"],
                "coverage": evidence_analysis["coverage"]
            },

            "requirements": {
                "total_matches_needed": min_total_needed,
                "confident_evidence_needed": min_confident_needed,
                "remaining_gap": {
                    "total_matches": total_gap,
                    "confident_evidence": confident_gap
                }
            },

            "recommendations": {
                "immediate_actions": [
                    f"ç»§ç»­è¿›è¡Œæ’ä½èµ›ï¼Œç›®æ ‡å¢åŠ  {total_gap} åœºæ¯”èµ›",
                    "ä¸“æ³¨äºç†Ÿç»ƒè‹±é›„ï¼Œæé«˜èƒœç‡ä»¥è·å¾—æ›´å¤šCONFIDENTçº§è¯æ®",
                    "å°½é‡é¿å…å°è¯•æ–°è‹±é›„ï¼Œä¿æŒè¡¨ç°ç¨³å®šæ€§"
                ],
                "data_quality_tips": [
                    "å•è‹±é›„è‡³å°‘è¿›è¡Œ5åœºæ¯”èµ›æ‰èƒ½è·å¾—å¯é åˆ†æ",
                    "æ¯ä¸ªä½ç½®è‡³å°‘éœ€è¦3-5åœºæ¯”èµ›æ•°æ®",
                    "è¿ç»­å‡ ä¸ªpatchçš„æ•°æ®æ›´æœ‰åŠ©äºè¶‹åŠ¿åˆ†æ"
                ],
                "estimated_time": self._estimate_completion_time(total_gap)
            },

            "next_analysis_threshold": {
                "matches": min_total_needed,
                "estimated_confident_evidence": self._estimate_confident_evidence(min_total_needed),
                "trigger_condition": "æ•°æ®è¾¾åˆ°é˜ˆå€¼åå°†è‡ªåŠ¨è§¦å‘å®Œæ•´åˆ†æ"
            },

            "fallback_info": {
                "reason": "insufficient_evidence",
                "generated_at": datetime.utcnow().isoformat(),
                "version": "rule_based_v1.0"
            }
        }

    def _generate_observation_card(self, evidence_records: List[Dict[str, Any]],
                                  evidence_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆè§‚å¯Ÿå¡ç‰‡"""

        # æå–å…³é”®è§‚å¯Ÿ
        observations = self._extract_key_observations(evidence_records)

        # ç”Ÿæˆæ¨¡å¼è¯†åˆ«
        patterns = self._identify_patterns(evidence_records)

        return {
            "card_type": "observation_only",
            "status": "observation_mode",
            "title": "æ•°æ®è§‚å¯ŸæŠ¥å‘Š - æ¨¡å¼è¯†åˆ«åˆ†æ",
            "summary": f"åŸºäº {evidence_analysis['total_count']} æ¡è¯æ®çš„è§‚å¯Ÿæ€§åˆ†æã€‚å½“å‰æ•°æ®è´¨é‡ï¼š{evidence_analysis['quality_level']}ã€‚",

            "evidence_summary": {
                "total_records": evidence_analysis["total_count"],
                "confident_evidence": evidence_analysis["confident_count"],
                "caution_evidence": evidence_analysis["caution_count"],
                "coverage_analysis": evidence_analysis["coverage"],
                "data_quality_metrics": {
                    "confident_ratio": format_output_precision(evidence_analysis["confident_ratio"], is_probability=True),
                    "avg_sample_size": evidence_analysis["avg_sample_size"],
                    "avg_effective_n": evidence_analysis["avg_effective_n"]
                }
            },

            "key_observations": observations,

            "identified_patterns": patterns,

            "data_gaps": {
                "missing_coverage": self._identify_data_gaps(evidence_analysis["coverage"]),
                "low_confidence_areas": self._identify_low_confidence_areas(evidence_records),
                "recommendations_for_more_data": [
                    "å¢åŠ æ ·æœ¬é‡ä¸è¶³çš„è‹±é›„/ä½ç½®ç»„åˆçš„æ¯”èµ›",
                    "åœ¨å½“å‰patchç»§ç»­ç§¯ç´¯æ•°æ®ä»¥æé«˜ç½®ä¿¡åº¦",
                    "ä¿æŒç¨³å®šè¡¨ç°ä»¥è·å¾—æ›´å¤šCONFIDENTçº§è¯æ®"
                ]
            },

            "limitations": {
                "why_no_advice": "å½“å‰è¯æ®è´¨é‡ä¸è¶³ä»¥æ”¯æŒå…·ä½“çš„è¡ŒåŠ¨å»ºè®®",
                "confidence_threshold": "éœ€è¦è‡³å°‘2æ¡CONFIDENTçº§è¯æ®æ‰èƒ½ç»™å‡ºè¡ŒåŠ¨å»ºè®®",
                "next_steps": "ç»§ç»­ç§¯ç´¯æ•°æ®ï¼Œè¾¾åˆ°é˜ˆå€¼åå°†æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®"
            },

            "fallback_info": {
                "reason": "observation_only",
                "generated_at": datetime.utcnow().isoformat(),
                "version": "rule_based_v1.0"
            }
        }

    def _generate_rule_based_card(self, evidence_records: List[Dict[str, Any]],
                                 evidence_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """ç”ŸæˆåŸºäºè§„åˆ™çš„æ•™ç»ƒå¡ç‰‡"""

        # è¿‡æ»¤å‡ºCONFIDENTè¯æ®
        confident_records = [r for r in evidence_records if r.get('governance_tag') == 'CONFIDENT']

        # è¯†åˆ«ä¼˜åŠ¿
        strengths = self._identify_rule_based_strengths(confident_records)

        # è¯†åˆ«æ”¹è¿›ç‚¹
        improvements = self._identify_rule_based_improvements(evidence_records)

        # ç”ŸæˆåŸºç¡€å»ºè®®
        recommendations = self._generate_rule_based_recommendations(confident_records, improvements)

        return {
            "card_type": "rule_based_advice",
            "status": "basic_recommendations",
            "title": "åŸºç¡€æ•™ç»ƒå»ºè®® - åŸºäºé«˜ç½®ä¿¡åº¦è¯æ®",
            "summary": f"åŸºäº {len(confident_records)} æ¡CONFIDENTçº§è¯æ®ç”Ÿæˆçš„åŸºç¡€å»ºè®®ã€‚",

            "evidence_foundation": {
                "confident_evidence_count": len(confident_records),
                "total_evidence_count": len(evidence_records),
                "confidence_ratio": format_output_precision(len(confident_records) / len(evidence_records), is_probability=True),
                "analysis_scope": evidence_analysis["coverage"]
            },

            "identified_strengths": strengths,

            "improvement_areas": improvements,

            "basic_recommendations": recommendations,

            "evidence_citations": self._generate_evidence_citations(confident_records),

            "limitations": {
                "scope": "åŸºäºè§„åˆ™ç”Ÿæˆï¼Œæœªä½¿ç”¨AIæ·±åº¦åˆ†æ",
                "evidence_requirement": "ä»…åŸºäºCONFIDENTçº§è¯æ®",
                "upgrade_condition": "BedrockæœåŠ¡æ¢å¤åå°†æä¾›æ›´è¯¦ç»†çš„AIåˆ†æ"
            },

            "fallback_info": {
                "reason": "rule_based_fallback",
                "generated_at": datetime.utcnow().isoformat(),
                "version": "rule_based_v1.0",
                "ai_service_status": "degraded"
            }
        }

    def _extract_key_observations(self, evidence_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """æå–å…³é”®è§‚å¯Ÿ"""
        observations = []

        if not evidence_records:
            return observations

        df = pd.DataFrame(evidence_records)

        # èƒœç‡è§‚å¯Ÿ
        if 'p_hat' in df.columns:
            avg_winrate = df['p_hat'].mean()
            observations.append({
                "type": "winrate_analysis",
                "finding": f"å¹³å‡èƒœç‡: {format_output_precision(avg_winrate, is_probability=True)}",
                "confidence": "medium",
                "details": {
                    "sample_size": len(df),
                    "range": f"{df['p_hat'].min():.3f} - {df['p_hat'].max():.3f}"
                }
            })

        # è§’è‰²åˆ†å¸ƒè§‚å¯Ÿ
        if 'role' in df.columns:
            role_dist = df['role'].value_counts()
            primary_role = role_dist.index[0] if len(role_dist) > 0 else "unknown"
            observations.append({
                "type": "role_distribution",
                "finding": f"ä¸»è¦ä½ç½®: {primary_role} ({role_dist.iloc[0]}åœº)",
                "confidence": "high",
                "details": role_dist.to_dict()
            })

        # è‹±é›„å¤šæ ·æ€§è§‚å¯Ÿ
        if 'champion_id' in df.columns:
            unique_champions = df['champion_id'].nunique()
            total_games = len(df)
            diversity_score = unique_champions / total_games
            observations.append({
                "type": "champion_diversity",
                "finding": f"è‹±é›„æ± å¤šæ ·æ€§: {unique_champions}ä¸ªè‹±é›„ï¼Œå¤šæ ·æ€§è¯„åˆ†: {format_output_precision(diversity_score)}",
                "confidence": "high",
                "details": {
                    "unique_champions": unique_champions,
                    "total_games": total_games,
                    "diversity_score": diversity_score
                }
            })

        return observations

    def _identify_patterns(self, evidence_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """è¯†åˆ«æ•°æ®æ¨¡å¼"""
        patterns = []

        if not evidence_records:
            return patterns

        df = pd.DataFrame(evidence_records)

        # CIå®½åº¦æ¨¡å¼
        if 'ci' in df.columns:
            ci_widths = []
            for ci in df['ci']:
                if isinstance(ci, dict) and 'lo' in ci and 'hi' in ci:
                    width = ci['hi'] - ci['lo']
                    ci_widths.append(width)

            if ci_widths:
                avg_ci_width = sum(ci_widths) / len(ci_widths)
                patterns.append({
                    "type": "confidence_pattern",
                    "pattern": "ç½®ä¿¡åŒºé—´å®½åº¦åˆ†æ",
                    "finding": f"å¹³å‡CIå®½åº¦: {format_output_precision(avg_ci_width)}",
                    "interpretation": "CIè¶Šçª„è¡¨ç¤ºä¼°è®¡è¶Šå¯é " if avg_ci_width < 0.3 else "CIè¾ƒå®½ï¼Œéœ€è¦æ›´å¤šæ•°æ®æé«˜å¯é æ€§"
                })

        # Patchåˆ†å¸ƒæ¨¡å¼
        if 'patch_id' in df.columns:
            patch_dist = df['patch_id'].value_counts()
            patterns.append({
                "type": "temporal_pattern",
                "pattern": "ç‰ˆæœ¬åˆ†å¸ƒ",
                "finding": f"è¦†ç›– {len(patch_dist)} ä¸ªç‰ˆæœ¬ï¼Œä¸»è¦ç‰ˆæœ¬: {patch_dist.index[0]}",
                "interpretation": "å¤šç‰ˆæœ¬æ•°æ®æœ‰åŠ©äºè¯†åˆ«è‹±é›„å¼ºåº¦å˜åŒ–è¶‹åŠ¿"
            })

        return patterns

    def _identify_rule_based_strengths(self, confident_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºäºè§„åˆ™è¯†åˆ«ä¼˜åŠ¿"""
        strengths = []

        if not confident_records:
            return strengths

        # é«˜èƒœç‡è¡¨ç°
        high_winrate_records = [r for r in confident_records if r.get('p_hat', 0) > 0.55]
        if high_winrate_records:
            avg_winrate = sum(r['p_hat'] for r in high_winrate_records) / len(high_winrate_records)
            strengths.append({
                "area": "é«˜èƒœç‡è¡¨ç°",
                "description": f"{len(high_winrate_records)}ä¸ªè‹±é›„/ä½ç½®ç»„åˆæ˜¾ç¤ºå‡ºä¼˜å¼‚è¡¨ç°",
                "metric": f"å¹³å‡èƒœç‡: {format_output_precision(avg_winrate, is_probability=True)}",
                "supporting_evidence": len(high_winrate_records),
                "confidence": "high"
            })

        # ç¨³å®šæ€§è¡¨ç°
        stable_records = [r for r in confident_records if r.get('ci', {}).get('hi', 1) - r.get('ci', {}).get('lo', 0) < 0.25]
        if stable_records:
            strengths.append({
                "area": "è¡¨ç°ç¨³å®šæ€§",
                "description": f"{len(stable_records)}ä¸ªç»„åˆæ˜¾ç¤ºç¨³å®šçš„è¡¨ç°",
                "metric": "ç½®ä¿¡åŒºé—´è¾ƒçª„ï¼Œè¡¨ç°å¯é¢„æµ‹",
                "supporting_evidence": len(stable_records),
                "confidence": "medium"
            })

        return strengths

    def _identify_rule_based_improvements(self, evidence_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŸºäºè§„åˆ™è¯†åˆ«æ”¹è¿›ç‚¹"""
        improvements = []

        if not evidence_records:
            return improvements

        df = pd.DataFrame(evidence_records)

        # ä½èƒœç‡åŒºåŸŸ
        low_winrate_records = df[df['p_hat'] < 0.45] if 'p_hat' in df.columns else pd.DataFrame()
        if not low_winrate_records.empty:
            improvements.append({
                "area": "èƒœç‡æ”¹è¿›",
                "issue": f"{len(low_winrate_records)}ä¸ªè‹±é›„/ä½ç½®ç»„åˆèƒœç‡åä½",
                "target_metric": "èƒœç‡ < 45%",
                "priority": "high",
                "evidence_count": len(low_winrate_records)
            })

        # æ•°æ®ä¸è¶³åŒºåŸŸ
        low_sample_records = df[df['n'] < 10] if 'n' in df.columns else pd.DataFrame()
        if not low_sample_records.empty:
            improvements.append({
                "area": "æ•°æ®ç§¯ç´¯",
                "issue": f"{len(low_sample_records)}ä¸ªç»„åˆæ ·æœ¬é‡ä¸è¶³",
                "target_metric": "æ ·æœ¬é‡ < 10",
                "priority": "medium",
                "evidence_count": len(low_sample_records)
            })

        return improvements

    def _generate_rule_based_recommendations(self, confident_records: List[Dict[str, Any]],
                                           improvements: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”ŸæˆåŸºäºè§„åˆ™çš„å»ºè®®"""
        recommendations = []

        # ä¼˜åŠ¿æ”¾å¤§å»ºè®®
        if confident_records:
            high_winrate_records = [r for r in confident_records if r.get('p_hat', 0) > 0.55]
            if high_winrate_records:
                best_combo = max(high_winrate_records, key=lambda x: x.get('p_hat', 0))
                recommendations.append({
                    "type": "leverage_strength",
                    "priority": "high",
                    "title": "å‘æŒ¥ä¼˜åŠ¿ç»„åˆ",
                    "description": f"ç»§ç»­ä½¿ç”¨è¡¨ç°ä¼˜å¼‚çš„è‹±é›„/ä½ç½®ç»„åˆ",
                    "specific_action": f"ä¼˜å…ˆé€‰æ‹© {best_combo.get('champion_id', 'unknown')} åœ¨ {best_combo.get('role', 'unknown')} ä½ç½®",
                    "supporting_evidence": {
                        "row_id": best_combo.get('row_id', ''),
                        "winrate": format_output_precision(best_combo.get('p_hat', 0), is_probability=True),
                        "sample_size": best_combo.get('n', 0),
                        "confidence_level": "CONFIDENT"
                    }
                })

        # æ”¹è¿›å»ºè®®
        for improvement in improvements[:2]:  # åªå–å‰2ä¸ªæ”¹è¿›ç‚¹
            if improvement["area"] == "èƒœç‡æ”¹è¿›":
                recommendations.append({
                    "type": "improve_performance",
                    "priority": "medium",
                    "title": "æ”¹è¿›ä½è¡¨ç°ç»„åˆ",
                    "description": "é‡ç‚¹æå‡èƒœç‡åä½çš„è‹±é›„/ä½ç½®ç»„åˆ",
                    "specific_action": "é€šè¿‡ç»ƒä¹ ã€è§‚çœ‹å›æ”¾æˆ–æš‚æ—¶é¿å…ä½¿ç”¨è¿™äº›ç»„åˆ",
                    "supporting_evidence": {
                        "affected_combinations": improvement["evidence_count"],
                        "threshold": "èƒœç‡ < 45%"
                    }
                })

        # æ•°æ®ç§¯ç´¯å»ºè®®
        if any(imp["area"] == "æ•°æ®ç§¯ç´¯" for imp in improvements):
            recommendations.append({
                "type": "data_collection",
                "priority": "low",
                "title": "å¢åŠ æ•°æ®æ ·æœ¬",
                "description": "ä¸ºæ ·æœ¬é‡ä¸è¶³çš„ç»„åˆå¢åŠ æ›´å¤šæ¯”èµ›æ•°æ®",
                "specific_action": "æ¯ä¸ªè‹±é›„/ä½ç½®ç»„åˆè‡³å°‘è¿›è¡Œ10åœºæ¯”èµ›ä»¥è·å¾—å¯é åˆ†æ",
                "supporting_evidence": {
                    "min_sample_requirement": 10,
                    "current_insufficient_combinations": sum(1 for imp in improvements if imp["area"] == "æ•°æ®ç§¯ç´¯")
                }
            })

        return recommendations

    def _generate_evidence_citations(self, confident_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆè¯æ®å¼•ç”¨"""
        citations = []

        for record in confident_records[:5]:  # åªæ˜¾ç¤ºå‰5æ¡
            citation = {
                "row_id": record.get('row_id', ''),
                "champion_id": record.get('champion_id', 0),
                "role": record.get('role', ''),
                "patch_id": record.get('patch_id', ''),
                "sample_size": record.get('n', 0),
                "effective_n": format_output_precision(record.get('effective_n', 0)),
                "winrate": format_output_precision(record.get('p_hat', 0), is_probability=True),
                "confidence_interval": {
                    "lower": format_output_precision(record.get('ci', {}).get('lo', 0), is_probability=True),
                    "upper": format_output_precision(record.get('ci', {}).get('hi', 1), is_probability=True)
                },
                "uses_prior": record.get('uses_prior', False),
                "governance_tag": record.get('governance_tag', ''),
                "synthetic_share": format_output_precision(record.get('synthetic_share', 0), is_probability=True)
            }
            citations.append(citation)

        return citations

    def _generate_error_card(self, fallback_reason: str) -> Dict[str, Any]:
        """ç”Ÿæˆé”™è¯¯å¡ç‰‡"""
        return {
            "card_type": "error",
            "status": "system_error",
            "title": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨",
            "summary": "åˆ†ææœåŠ¡é‡åˆ°æŠ€æœ¯é—®é¢˜ï¼Œè¯·ç¨åé‡è¯•ã€‚",
            "error_info": {
                "reason": fallback_reason,
                "timestamp": datetime.utcnow().isoformat(),
                "support_contact": "æŠ€æœ¯æ”¯æŒå›¢é˜Ÿæ­£åœ¨å¤„ç†æ­¤é—®é¢˜"
            },
            "fallback_info": {
                "reason": "system_error",
                "generated_at": datetime.utcnow().isoformat(),
                "version": "rule_based_v1.0"
            }
        }

    def _identify_data_gaps(self, coverage: Dict[str, Any]) -> List[str]:
        """è¯†åˆ«æ•°æ®ç¼ºå£"""
        gaps = []

        # æ£€æŸ¥è§’è‰²è¦†ç›–
        role_coverage = coverage.get("role_coverage", [])
        all_roles = ["top", "jungle", "mid", "adc", "support"]
        missing_roles = [role for role in all_roles if role not in role_coverage]
        if missing_roles:
            gaps.append(f"ç¼ºå°‘ä½ç½®æ•°æ®: {', '.join(missing_roles)}")

        # æ£€æŸ¥ç‰ˆæœ¬è¦†ç›–
        patch_coverage = coverage.get("patch_coverage", [])
        if len(patch_coverage) < 2:
            gaps.append("ç‰ˆæœ¬è¦†ç›–ä¸è¶³ï¼Œå»ºè®®æ¶µç›–å¤šä¸ªpatchæ•°æ®")

        # æ£€æŸ¥è‹±é›„å¤šæ ·æ€§
        champion_count = coverage.get("champion_coverage", 0)
        if champion_count < 5:
            gaps.append(f"è‹±é›„æ± åçª„ï¼Œä»…æœ‰ {champion_count} ä¸ªè‹±é›„")

        return gaps

    def _identify_low_confidence_areas(self, evidence_records: List[Dict[str, Any]]) -> List[str]:
        """è¯†åˆ«ä½ç½®ä¿¡åº¦åŒºåŸŸ"""
        low_confidence_areas = []

        context_records = [r for r in evidence_records if r.get('governance_tag') == 'CONTEXT']
        if context_records:
            low_confidence_areas.append(f"{len(context_records)}æ¡è®°å½•ç½®ä¿¡åº¦è¾ƒä½ï¼ˆCONTEXTçº§ï¼‰")

        wide_ci_records = [r for r in evidence_records
                          if isinstance(r.get('ci', {}), dict) and
                          (r['ci'].get('hi', 1) - r['ci'].get('lo', 0)) > 0.4]
        if wide_ci_records:
            low_confidence_areas.append(f"{len(wide_ci_records)}æ¡è®°å½•ç½®ä¿¡åŒºé—´è¿‡å®½")

        return low_confidence_areas

    def _estimate_completion_time(self, remaining_matches: int) -> str:
        """ä¼°ç®—å®Œæˆæ—¶é—´"""
        if remaining_matches <= 0:
            return "æ•°æ®å·²è¶³å¤Ÿ"
        elif remaining_matches <= 10:
            return "çº¦1-2å‘¨ï¼ˆæ¯å¤©2-3åœºï¼‰"
        elif remaining_matches <= 30:
            return "çº¦2-4å‘¨ï¼ˆæ¯å¤©2-3åœºï¼‰"
        else:
            return "çº¦1-2ä¸ªæœˆï¼ˆæ¯å¤©2-3åœºï¼‰"

    def _estimate_confident_evidence(self, total_matches: int) -> int:
        """ä¼°ç®—å¯è·å¾—çš„CONFIDENTè¯æ®æ•°"""
        # åŸºäºç»éªŒï¼šçº¦30-40%çš„è®°å½•èƒ½è¾¾åˆ°CONFIDENTçº§
        return max(1, int(total_matches * 0.35))


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="Rule-basedé™çº§å¡ç‰‡ç”Ÿæˆ")
    parser.add_argument("--input", required=True, help="è¾“å…¥è¯æ®æ–‡ä»¶")
    parser.add_argument("--output", default="data/user_mode", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--fallback-reason",
                       choices=["bedrock_failure", "insufficient_evidence", "cost_limit"],
                       default="bedrock_failure", help="é™çº§åŸå› ")
    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½è¯æ®è®°å½•: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        if args.input.endswith('.jsonl'):
            evidence_records = [json.loads(line) for line in f if line.strip()]
        else:
            evidence_records = json.load(f)

    print(f"  å‘ç° {len(evidence_records)} æ¡è¯æ®è®°å½•")

    # ç”Ÿæˆé™çº§å¡ç‰‡
    fallback = RuleBasedFallback()
    fallback_card = fallback.generate_fallback_card(evidence_records, args.fallback_reason)

    # ä¿å­˜ç»“æœ
    output_dir = Path(args.output)
    output_dir.mkdir(parents=True, exist_ok=True)

    card_file = output_dir / f"fallback_card_{args.fallback_reason}.json"
    with open(card_file, 'w', encoding='utf-8') as f:
        json.dump(fallback_card, f, indent=2, ensure_ascii=False)

    print(f"âœ… é™çº§å¡ç‰‡å·²ç”Ÿæˆ: {card_file}")
    print(f"  å¡ç‰‡ç±»å‹: {fallback_card['card_type']}")
    print(f"  çŠ¶æ€: {fallback_card['status']}")


if __name__ == "__main__":
    main()