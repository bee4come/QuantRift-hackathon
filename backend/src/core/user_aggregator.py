#!/usr/bin/env python3
"""
User-Mode èšåˆå™¨
ä¸“æ³¨å•ç”¨æˆ·æ•°æ®èšåˆï¼ŒBeta-Binomialå…ˆéªŒæ”¶ç¼©ï¼Œè¯æ®åˆ†çº§
"""
import json
import pandas as pd
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
from scipy import stats
import argparse

from .utils import (
    load_user_mode_config,
    generate_row_id,
    validate_evidence_schema,
    apply_governance_tag,
    safe_float_convert,
    safe_int_convert,
    format_output_precision,
    save_with_audit_trail
)


class UserAggregator:
    """å•ç”¨æˆ·æ•°æ®èšåˆå™¨"""

    def __init__(self, config_path: str = "configs/user_mode_params.yml"):
        """åˆå§‹åŒ–èšåˆå™¨"""
        self.config = load_user_mode_config(config_path)
        self.prior_config = self.config['prior_shrinkage']
        self.governance_config = self.config['governance']

    def aggregate_user_data(self, puuid: str, match_records: List[Dict[str, Any]],
                           league_baseline: Optional[pd.DataFrame] = None) -> Tuple[List[Dict], List[Dict], List[Dict]]:
        """
        èšåˆå•ç”¨æˆ·æ•°æ®

        Args:
            puuid: ç”¨æˆ·PUUID
            match_records: æ¯”èµ›è®°å½•åˆ—è¡¨
            league_baseline: è”ç›ŸåŸºçº¿æ•°æ®

        Returns:
            (entity_panel, context_panel, patch_summary)
        """
        print(f"ğŸ¯ èšåˆç”¨æˆ·æ•°æ®: {puuid[:20]}...")

        # è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(match_records)
        if df.empty:
            return [], [], []

        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        required_fields = ['patch', 'champion_id', 'role', 'queue', 'win', 'games']
        missing_fields = [f for f in required_fields if f not in df.columns]
        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦å­—æ®µ: {missing_fields}")

        # æŒ‰patchæ’åºï¼ˆä¸¥ç¦è·¨patchï¼‰
        df = df.sort_values('patch')
        patches = sorted(df['patch'].unique())

        print(f"  å‘ç° {len(patches)} ä¸ªpatch: {patches[:5]}...")
        print(f"  æ€»è®°å½•æ•°: {len(df)}")

        entity_records = []
        context_records = []
        patch_summaries = []

        # æŒ‰patchå¤„ç†ï¼ˆä¸¥ç¦è·¨patchèšåˆï¼‰
        for patch in patches:
            patch_df = df[df['patch'] == patch]
            print(f"  å¤„ç† patch {patch}: {len(patch_df)} æ¡è®°å½•")

            # Patchçº§æ±‡æ€»
            patch_summary = self._create_patch_summary(patch, patch_df, puuid)
            patch_summaries.append(patch_summary)

            # è·å–å†å²æ•°æ®ç”¨äºå…ˆéªŒ
            historical_data = self._get_historical_data(patch, df, league_baseline)

            # æŒ‰ (champion_id, role, queue) åˆ†ç»„èšåˆ
            groupby_cols = ['champion_id', 'role', 'queue']
            groups = patch_df.groupby(groupby_cols)

            for group_keys, group_df in groups:
                champion_id, role, queue = group_keys

                # è®¡ç®—åŸºç¡€ç»Ÿè®¡
                base_stats = self._calculate_base_stats(group_df)

                # åº”ç”¨å…ˆéªŒæ”¶ç¼©
                prior_stats = self._apply_prior_shrinkage(
                    base_stats, champion_id, role, queue, patch, historical_data
                )

                # åˆ›å»ºèšåˆè®°å½•
                record = self._create_aggregate_record(
                    puuid, patch, champion_id, role, queue,
                    base_stats, prior_stats, group_df
                )

                # åº”ç”¨æ²»ç†åˆ†çº§
                governance_tag = apply_governance_tag(record, self.config)
                record['governance_tag'] = governance_tag

                # åˆ†æµåˆ°entityæˆ–context
                if governance_tag in ['CONFIDENT', 'CAUTION']:
                    entity_records.append(record)
                else:
                    context_records.append(record)

        print(f"âœ… èšåˆå®Œæˆ:")
        print(f"  Entity records: {len(entity_records)}")
        print(f"  Context records: {len(context_records)}")
        print(f"  Patch summaries: {len(patch_summaries)}")

        return entity_records, context_records, patch_summaries

    def _create_patch_summary(self, patch: str, patch_df: pd.DataFrame, puuid: str) -> Dict[str, Any]:
        """åˆ›å»ºpatchçº§æ±‡æ€»"""
        total_games = len(patch_df)
        total_wins = patch_df['win'].sum()
        winrate = total_wins / total_games if total_games > 0 else 0.5

        # ä½ç½®åˆ†å¸ƒ
        role_dist = patch_df['role'].value_counts().to_dict()

        # é˜Ÿåˆ—åˆ†å¸ƒ
        queue_dist = patch_df['queue'].value_counts().to_dict()

        # è‹±é›„ä½¿ç”¨
        champion_usage = patch_df['champion_id'].value_counts().head(10).to_dict()

        # èŠ‚å¥æŒ‡æ ‡ï¼ˆæ¨¡æ‹Ÿï¼Œå®é™…åº”ä»matchè¯¦æƒ…è®¡ç®—ï¼‰
        avg_game_duration = patch_df.get('game_duration', pd.Series([1800] * len(patch_df))).mean()

        summary = {
            'puuid': puuid,
            'patch_id': patch,
            'total_games': int(total_games),
            'total_wins': int(total_wins),
            'winrate': format_output_precision(winrate, is_probability=True),
            'role_distribution': role_dist,
            'queue_distribution': queue_dist,
            'champion_usage': champion_usage,
            'avg_game_duration': safe_float_convert(avg_game_duration),
            'avg_lp_per_win': 17.0,  # é»˜è®¤å€¼ï¼Œåº”ä»å®é™…æ•°æ®è®¡ç®—
            'avg_lp_per_loss': 17.0,
            'tempo_indicators': {
                'cs10': 80.0,  # 10åˆ†é’Ÿè¡¥åˆ€ï¼Œé»˜è®¤å€¼
                'exp10': 120.0,  # 10åˆ†é’Ÿç»éªŒä¼˜åŠ¿
                'tempo_score': 0.5  # èŠ‚å¥è¯„åˆ†
            }
        }

        return summary

    def _get_historical_data(self, current_patch: str, full_df: pd.DataFrame,
                           league_baseline: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
        """è·å–å†å²æ•°æ®ç”¨äºå…ˆéªŒè®¡ç®—"""
        # è·å–å†å²patchæ•°æ®ï¼ˆä»…â‰¤t-1ï¼‰
        all_patches = sorted(full_df['patch'].unique())
        try:
            current_idx = all_patches.index(current_patch)
            historical_patches = all_patches[:current_idx]  # ä¸¥æ ¼<å½“å‰patch
        except ValueError:
            historical_patches = []

        # é™åˆ¶å›çœ‹çª—å£
        max_lookback = self.prior_config['personal_history']['max_lookback_patches']
        if len(historical_patches) > max_lookback:
            historical_patches = historical_patches[-max_lookback:]

        # ä¸ªäººå†å²æ•°æ®
        personal_history = {}
        if historical_patches:
            hist_df = full_df[full_df['patch'].isin(historical_patches)]
            decay_lambda = self.prior_config['personal_history']['decay_lambda']

            # æŒ‰(champion_id, role, queue)åˆ†ç»„è®¡ç®—è¡°å‡æƒé‡å†å²
            for (champion_id, role, queue), group in hist_df.groupby(['champion_id', 'role', 'queue']):
                key = f"{champion_id}_{role}_{queue}"

                # è®¡ç®—è¡°å‡æƒé‡
                patch_weights = []
                for patch in historical_patches:
                    patch_idx = len(historical_patches) - 1 - historical_patches[::-1].index(patch)
                    weight = decay_lambda ** patch_idx
                    patch_weights.append(weight)

                # åŠ æƒç»Ÿè®¡
                patch_groups = group.groupby('patch')
                weighted_wins = 0
                weighted_games = 0

                for i, patch in enumerate(historical_patches):
                    if patch in patch_groups.groups:
                        patch_data = patch_groups.get_group(patch)
                        weight = patch_weights[i]
                        weighted_wins += patch_data['win'].sum() * weight
                        weighted_games += len(patch_data) * weight

                if weighted_games > 0:
                    personal_history[key] = {
                        'weighted_wins': weighted_wins,
                        'weighted_games': weighted_games,
                        'effective_winrate': weighted_wins / weighted_games
                    }

        # è”ç›ŸåŸºçº¿ï¼ˆå¦‚æœæä¾›ï¼‰
        league_baseline_data = {}
        if league_baseline is not None:
            # æŒ‰patchå’Œroleåˆ†ç»„çš„åŸºçº¿èƒœç‡
            if 'patch' in league_baseline.columns and 'role' in league_baseline.columns:
                baseline_groups = league_baseline.groupby(['patch', 'role'])
                for (patch, role), group in baseline_groups:
                    if patch == current_patch:
                        key = f"baseline_{role}"
                        league_baseline_data[key] = {
                            'avg_winrate': group.get('avg_winrate', group.get('winrate', [0.5]))[0] if len(group) > 0 else 0.5,
                            'sample_size': len(group)
                        }

        return {
            'personal_history': personal_history,
            'league_baseline': league_baseline_data
        }

    def _calculate_base_stats(self, group_df: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—åŸºç¡€ç»Ÿè®¡"""
        n = len(group_df)
        w = group_df['win'].sum()
        p_hat_raw = w / n if n > 0 else 0.5

        # è®¡ç®—Wilsonç½®ä¿¡åŒºé—´
        z = 1.96  # 95%ç½®ä¿¡åŒºé—´
        ci_lo, ci_hi = self._wilson_confidence_interval(w, n, z)

        base_stats = {
            'n': int(n),
            'w': int(w),
            'p_hat_raw': p_hat_raw,
            'ci_lo': ci_lo,
            'ci_hi': ci_hi,
            'ci_width': ci_hi - ci_lo
        }

        return base_stats

    def _apply_prior_shrinkage(self, base_stats: Dict[str, Any],
                              champion_id: int, role: str, queue: str, patch: str,
                              historical_data: Dict[str, Any]) -> Dict[str, Any]:
        """åº”ç”¨Beta-Binomialå…ˆéªŒæ”¶ç¼©"""
        n = base_stats['n']
        w = base_stats['w']

        # æŒ‰ä¼˜å…ˆçº§åº”ç”¨å…ˆéªŒ
        alpha_prior = 0.5  # Jeffreysé»˜è®¤
        beta_prior = 0.5

        # 1. LeagueåŸºçº¿å…ˆéªŒ
        baseline_key = f"baseline_{role}"
        if baseline_key in historical_data['league_baseline']:
            league_data = historical_data['league_baseline'][baseline_key]
            league_winrate = league_data['avg_winrate']
            league_confidence = min(50, league_data['sample_size'])  # é™åˆ¶æœ€å¤§æƒé‡

            alpha_prior = league_confidence * league_winrate
            beta_prior = league_confidence * (1 - league_winrate)

        # 2. ä¸ªäººå†å²å…ˆéªŒï¼ˆä¼˜å…ˆçº§æ›´é«˜ï¼‰
        personal_key = f"{champion_id}_{role}_{queue}"
        if personal_key in historical_data['personal_history']:
            personal_data = historical_data['personal_history'][personal_key]
            personal_winrate = personal_data['effective_winrate']
            personal_confidence = min(100, personal_data['weighted_games'])  # é™åˆ¶æœ€å¤§æƒé‡

            # ä¸LeagueåŸºçº¿çš„åŠ æƒç»„åˆ
            total_confidence = league_confidence + personal_confidence
            if total_confidence > 0:
                combined_winrate = (
                    league_confidence * league_winrate + personal_confidence * personal_winrate
                ) / total_confidence

                alpha_prior = total_confidence * combined_winrate
                beta_prior = total_confidence * (1 - combined_winrate)

        # 3. è®¡ç®—åéªŒ
        alpha_posterior = alpha_prior + w
        beta_posterior = beta_prior + (n - w)

        # åéªŒä¼°è®¡
        p_hat_posterior = alpha_posterior / (alpha_posterior + beta_posterior)

        # æœ‰æ•ˆæ ·æœ¬æ•°
        effective_n = alpha_prior + beta_prior + n

        # åéªŒç½®ä¿¡åŒºé—´
        z = 1.96
        posterior_var = (alpha_posterior * beta_posterior) / (
            (alpha_posterior + beta_posterior) ** 2 * (alpha_posterior + beta_posterior + 1)
        )
        posterior_std = np.sqrt(posterior_var)

        ci_lo_posterior = max(0, p_hat_posterior - z * posterior_std)
        ci_hi_posterior = min(1, p_hat_posterior + z * posterior_std)

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†å…ˆéªŒ
        uses_prior = (alpha_prior > 0.5 or beta_prior > 0.5)

        prior_stats = {
            'uses_prior': uses_prior,
            'alpha_prior': alpha_prior,
            'beta_prior': beta_prior,
            'effective_n': effective_n,
            'p_hat': p_hat_posterior,
            'ci_lo': ci_lo_posterior,
            'ci_hi': ci_hi_posterior,
            'n0': alpha_prior + beta_prior,  # å…ˆéªŒæ ·æœ¬æ•°
            'w0': alpha_prior,  # å…ˆéªŒèƒœåˆ©æ•°
            'decay': self.prior_config['personal_history']['decay_lambda']
        }

        return prior_stats

    def _wilson_confidence_interval(self, w: int, n: int, z: float = 1.96) -> Tuple[float, float]:
        """Wilsonç½®ä¿¡åŒºé—´è®¡ç®—"""
        if n == 0:
            return 0.0, 1.0

        p_hat = w / n
        denominator = 1 + z**2 / n
        center = (p_hat + z**2 / (2*n)) / denominator
        margin = z * np.sqrt((p_hat * (1 - p_hat) + z**2 / (4*n)) / n) / denominator

        ci_lo = max(0, center - margin)
        ci_hi = min(1, center + margin)

        return ci_lo, ci_hi

    def _create_aggregate_record(self, puuid: str, patch: str, champion_id: int,
                               role: str, queue: str, base_stats: Dict[str, Any],
                               prior_stats: Dict[str, Any], group_df: pd.DataFrame) -> Dict[str, Any]:
        """åˆ›å»ºèšåˆè®°å½•"""
        # ç”Ÿæˆç»Ÿä¸€row_id
        row_id = generate_row_id(patch, champion_id, role, queue, "entity_id:role:queue")

        # åŸºç¡€ä¿¡æ¯
        record = {
            'row_id': row_id,
            'patch_id': patch,
            'champion_id': int(champion_id),
            'champion_name': f"Champion_{champion_id}",  # ç®€åŒ–ï¼Œå®é™…åº”æŸ¥è¡¨
            'role': role,
            'queue': queue,
            'n': base_stats['n'],
            'w': base_stats['w'],
            'uses_prior': prior_stats['uses_prior'],
            'effective_n': format_output_precision(prior_stats['effective_n']),
            'p_hat': format_output_precision(prior_stats['p_hat'], is_probability=True),
            'ci': {
                'lo': format_output_precision(prior_stats['ci_lo'], is_probability=True),
                'hi': format_output_precision(prior_stats['ci_hi'], is_probability=True)
            }
        }

        # è®¡ç®—ç›¸å¯¹åŸºçº¿çš„å·®å¼‚ï¼ˆç®€åŒ–ä¸ºç»å¯¹èƒœç‡-0.5ï¼‰
        winrate_delta = prior_stats['p_hat'] - 0.5
        record['winrate_delta'] = format_output_precision(winrate_delta, is_probability=True)

        # ç¨³å®šæ€§è¯„åˆ†ï¼ˆåŸºäºCIå®½åº¦ï¼‰
        ci_width = prior_stats['ci_hi'] - prior_stats['ci_lo']
        stability = max(0, 1 - ci_width)  # CIè¶Šçª„è¶Šç¨³å®š
        record['stability'] = format_output_precision(stability, is_probability=True)

        # åˆæˆæ•°æ®å æ¯”ï¼ˆå…ˆéªŒæƒé‡æ¯”ä¾‹ï¼‰
        if prior_stats['uses_prior']:
            synthetic_share = prior_stats['n0'] / prior_stats['effective_n']
        else:
            synthetic_share = 0.0
        record['synthetic_share'] = format_output_precision(synthetic_share, is_probability=True)

        # å…¶ä»–å¿…å¡«å­—æ®µ
        record['aggregation_level'] = "entity_id:role:queue"
        record['k_selected'] = 5  # é»˜è®¤å€¼
        record['oot_pass'] = True  # é»˜è®¤é€šè¿‡

        # å…ˆéªŒç›¸å…³å­—æ®µ
        record['n0'] = format_output_precision(prior_stats['n0'])
        record['w0'] = format_output_precision(prior_stats['w0'])
        record['decay'] = format_output_precision(prior_stats['decay'])

        return record

    def save_results(self, entity_records: List[Dict], context_records: List[Dict],
                    patch_summaries: List[Dict], output_dir: str = "data/user_mode") -> Dict[str, str]:
        """ä¿å­˜èšåˆç»“æœ"""
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # æ–‡ä»¶è·¯å¾„
        entity_file = output_dir / "user_entity_panel.jsonl"
        context_file = output_dir / "user_context_panel.jsonl"
        summary_file = output_dir / "user_patch_summary.jsonl"
        parquet_file = output_dir / "user_panel.parquet"

        # ä¿å­˜JSONLæ–‡ä»¶
        save_with_audit_trail(entity_records, entity_file, {'type': 'entity_panel'})
        save_with_audit_trail(context_records, context_file, {'type': 'context_panel'})
        save_with_audit_trail(patch_summaries, summary_file, {'type': 'patch_summary'})

        # ä¿å­˜Parquetï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
        if entity_records:
            df_entity = pd.DataFrame(entity_records)
            df_entity.to_parquet(parquet_file, index=False)

        file_paths = {
            'entity_panel': str(entity_file),
            'context_panel': str(context_file),
            'patch_summary': str(summary_file),
            'parquet': str(parquet_file)
        }

        print(f"ğŸ“ ç»“æœå·²ä¿å­˜:")
        for name, path in file_paths.items():
            print(f"  {name}: {path}")

        return file_paths


def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    parser = argparse.ArgumentParser(description="User-Modeæ•°æ®èšåˆ")
    parser.add_argument("--puuid", required=True, help="ç”¨æˆ·PUUID")
    parser.add_argument("--input", required=True, help="è¾“å…¥matchæ•°æ®æ–‡ä»¶")
    parser.add_argument("--output", default="data/user_mode", help="è¾“å‡ºç›®å½•")
    parser.add_argument("--baseline", help="è”ç›ŸåŸºçº¿æ•°æ®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰")
    args = parser.parse_args()

    # åŠ è½½æ•°æ®
    print(f"ğŸ“Š åŠ è½½matchæ•°æ®: {args.input}")
    with open(args.input, 'r', encoding='utf-8') as f:
        if args.input.endswith('.jsonl'):
            match_records = [json.loads(line) for line in f if line.strip()]
        else:
            match_records = json.load(f)

    print(f"  å‘ç° {len(match_records)} æ¡è®°å½•")

    # åŠ è½½åŸºçº¿æ•°æ®ï¼ˆå¯é€‰ï¼‰
    league_baseline = None
    if args.baseline:
        print(f"ğŸ“ˆ åŠ è½½åŸºçº¿æ•°æ®: {args.baseline}")
        if args.baseline.endswith('.parquet'):
            league_baseline = pd.read_parquet(args.baseline)
        else:
            league_baseline = pd.read_csv(args.baseline)

    # èšåˆæ•°æ®
    aggregator = UserAggregator()
    entity_records, context_records, patch_summaries = aggregator.aggregate_user_data(
        args.puuid, match_records, league_baseline
    )

    # ä¿å­˜ç»“æœ
    file_paths = aggregator.save_results(entity_records, context_records, patch_summaries, args.output)

    print(f"âœ… èšåˆå®Œæˆï¼")
    print(f"  Entity panel: {len(entity_records)} è¡Œ")
    print(f"  Context panel: {len(context_records)} è¡Œ")


if __name__ == "__main__":
    main()