# Battle Manual - Comprehensive Quantitative Analysis Workflow

**The definitive implementation of the "Quant Analysis Battle Manual" for League of Legends data.**

## üéØ Overview

The Battle Manual implements a comprehensive 6-step quantitative analysis workflow that processes 107,570 Silver layer records into "universal, interpretable, traceable" quantitative panels. This system enables version comparison, combo discovery, and user reports through production-ready analytics.

## üìã Table of Contents

- [Battle Manual Steps](#battle-manual-steps)
- [Quick Start](#quick-start)
- [Core Components](#core-components)
- [Usage Examples](#usage-examples)
- [Validation Framework](#validation-framework)
- [Output Specifications](#output-specifications)
- [Development Guide](#development-guide)

## üöÄ Battle Manual Steps

### Step 0: Input Layer (Already Available)
- **Match/Timeline data**: Bronze ‚Üí Silver (107,570 records)
- **Static registries**: DDragon/CDragon champions/items/runes (SCD2)
- **Dimension tables**: DimStatWeights, DimItemPassive, DimAbility, DimRuneValue
- **Governance**: n/effective_n/uses_prior/+1 patch buffer

### Step 1: Grouping Granularity (Unified Grain)
- **Version**: `patch`
- **Tier/Queue**: `tier, division, queue`
- **Role**: `role`
- **Entity/Combinations**:
  - Hero layer: `(champion, role, patch)`
  - Combo layer: `(champion, role, patch, core_items‚â§3, rune_page, spell_pair)`
- **Time Windows**: Global, `15/25/35` minutes (aligned with participantFrames)

Every statistic aggregated under these grains with mandatory fields: `n/effective_n/governance_tag/ci_lo/ci_hi/metric_version/generated_at`

### Step 2: Core 20 Metrics Implementation

#### A. Behavioral/Usage (3 metrics)
- `pick_rate = n_bucket / n_total_in_group`
- `attach_rate(combo) = n_combo / n_champion_role_patch`
- `avg_time_to_core`: Timeline first purchase time P50/P75 for core 2/3 items

#### B. Winrate & Robustness (5 metrics)
- `p_hat / ci_lo / ci_hi`: Wilson intervals (z=1.96)
- `winrate_delta_vs_baseline`: vs same champion√órole√ópatch baseline
- Small samples: Beta priors (n0,w0,decay) ‚Üí effective_n, evidence grading:
  - `CONFIDENT`: n‚â•100 or effective_n‚â•100
  - `CAUTION`: n‚â•30
  - Otherwise `CONTEXT`

#### C. Objectives/Tempo (3 metrics)
- `obj_rate`: Objective events (dragon/herald/baron/tower) ¬±10s, radius ~2500
- `kda_adj = (K + 0.7*A) / (D + 1)`, winsorized at 2.5%/97.5%
- `dpm/apm/hpm`: damage/assists/healing per minute

#### D. Gold Efficiency/Runes/Skills (4 metrics)
- `item_ge_t = (base_stats_value + passive_value_t) / cost`
- `rune_value_t`: DimRuneValue proc_rate√óeffect_value
- `dmg_per_cd`, `expected_dpm_t`, `cc_uptime`, `ehp_gain_t`

#### E. Combat Power (3 metrics)
```
CP_t = Œ£(w_stat ¬∑ (base+bonus_stat)_t) + k_dmg¬∑expected_dpm_t + k_surv¬∑ehp_gain_t + k_cc¬∑cc_uptime + k_mob¬∑mobility_value_t
```
Output: `cp_15/25/35` and `delta_cp`

#### F. Patch Shock (2 metrics)
- `shock_v2 = Œ£ w_k¬∑Œîparam_k` for each changed parameter
- `shock_components_*` for audit trail

### Step 3: Calculation Order (Batch Processing)
1. Event derivation from Timeline
2. Base statistics: n, wins/losses, p_hat, ci_lo/hi, pick/attach, kda_adj, dpm
3. Prior shrinkage: Beta (‚â§6 patches, exponential decay) ‚Üí effective_n/uses_prior
4. Static fusion: Join dimensions ‚Üí item_ge_t/rune_value_t/expected_dpm_t/cp_t
5. Shock: Registry diffs ‚Üí shock_components/shock_v2
6. Baseline differences: winrate_delta_vs_baseline/delta_cp
7. Governance tagging: governance_tag, +1_patch_buffer, aggregation_level=entity
8. Export triple: entity_panel.jsonl (‚â§3k rows, CONFIDENT priority), context_panel.jsonl, patch_summary.jsonl

### Step 4: DuckDB Implementation
Production-grade SQL implementation for:
- Winrate + Wilson + baseline differences (hero layer)
- Combo layer attach & Œîwin calculations
- CP calculations at 25 minutes

### Step 5: Usage & Output Interpretation
- **Meta strength**: cp_25 & delta_cp with shock_v2 directional consistency
- **Optimal builds/runes**: attach_rate ‚Üë + winrate_delta_vs_baseline ‚Üë + item_ge_25/rune_value_25
- **Gameplay recommendations**: avg_time_to_core, obj_rate, synergy/anti context
- **Confidence**: Only expose CONFIDENT/CAUTION to downstream, CONTEXT goes to context_panel

### Step 6: Priority Implementation
- **Immediate**: pick/attach/p_hat/CI/Œîwin/avg_time_to_core/obj_rate
- **Day 1**: item_ge_25/rune_value_25/cp_15/25/35 (initial weight values)
- **Day 2**: shock_components/shock_v2 + synergy/anti + regression calibration

## üöÄ Quick Start

### Basic Usage
```bash
# Process specific patches with validation
python run_battle_manual.py --patches 25.17,25.18,25.19 --validate --export-all

# Quick test with sample data
python run_battle_manual.py --quick-test

# DuckDB SQL analysis only
python run_battle_manual.py --sql-only

# Validation only
python run_battle_manual.py --validate
```

### Installation Requirements
```bash
# Core dependencies
pip install pandas numpy scipy duckdb

# For existing framework integration
pip install pyyaml pathlib datetime
```

### Directory Structure
```
battle_manual/
‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îú‚îÄ‚îÄ battle_manual_processor.py      # Main workflow processor
‚îÇ   ‚îú‚îÄ‚îÄ core_metrics_engine.py          # Core 20 metrics implementation
‚îÇ   ‚îú‚îÄ‚îÄ panels_export_engine.py         # Panel export system
‚îÇ   ‚îî‚îÄ‚îÄ validation_framework.py         # Comprehensive validation
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îú‚îÄ‚îÄ hero_layer_analysis.sql         # Hero layer DuckDB SQL
‚îÇ   ‚îú‚îÄ‚îÄ combo_layer_analysis.sql        # Combo layer DuckDB SQL
‚îÇ   ‚îî‚îÄ‚îÄ combat_power_analysis.sql       # Combat power DuckDB SQL
‚îú‚îÄ‚îÄ output/                              # Generated results
‚îú‚îÄ‚îÄ run_battle_manual.py                # Main execution script
‚îî‚îÄ‚îÄ README.md                           # This documentation
```

## üîß Core Components

### BattleManualProcessor
Main workflow orchestrator implementing the complete 6-step process.

```python
from core.battle_manual_processor import BattleManualProcessor

processor = BattleManualProcessor(
    config_path="configs/user_mode_params.yml",
    silver_data_path="data/silver/facts",
    output_dir="output/"
)

results = processor.execute_battle_manual(['25.17', '25.18', '25.19'])
```

### CoreMetricsEngine
Unified engine for all 20 quantitative metrics with Battle Manual compliance.

```python
from core.core_metrics_engine import CoreMetricsEngine

engine = CoreMetricsEngine()
metrics_results = engine.calculate_all_metrics(match_data, '25.18')

# Results organized by category:
# - behavioral_usage
# - winrate_robustness  
# - objectives_tempo
# - gold_efficiency
# - combat_power
# - patch_shock
```

### PanelsExportEngine
Export system for the three Battle Manual quantitative panels.

```python
from core.panels_export_engine import PanelsExportEngine

export_engine = PanelsExportEngine("output/")
panel_paths = export_engine.export_all_panels(metrics_results, match_data)

# Generates:
# - entity_panel.jsonl (‚â§3k rows, CONFIDENT priority)
# - context_panel.jsonl (research insights)
# - patch_summary.jsonl (version comparison)
```

### ValidationFramework
Comprehensive validation system for all workflow components.

```python
from core.validation_framework import BattleManualValidator

validator = BattleManualValidator()
validation_results = validator.run_comprehensive_validation()

# Validates:
# - Input layer integrity
# - Unified grain consistency
# - Core 20 metrics accuracy
# - DuckDB implementation
# - Panel export quality
# - End-to-end workflow
```

## üìã Usage Examples

### Example 1: Complete Workflow
```bash
# Full Battle Manual execution with validation
python run_battle_manual.py \
    --patches 25.17,25.18,25.19 \
    --validate \
    --export-all \
    --priority all \
    --verbose
```

### Example 2: Development Testing
```bash
# Quick test for development
python run_battle_manual.py --quick-test --verbose

# Validation only for debugging
python run_battle_manual.py --validate --verbose

# SQL analysis only
python run_battle_manual.py --sql-only --verbose
```

### Example 3: Production Run
```bash
# Production execution (skip validation for speed)
python run_battle_manual.py \
    --patches 25.17,25.18,25.19 \
    --skip-validation \
    --export-all \
    --priority immediate \
    --output-dir /production/output/
```

### Example 4: Dry Run
```bash
# Show execution plan without running
python run_battle_manual.py \
    --patches 25.17,25.18,25.19 \
    --dry-run \
    --export-all
```

## ‚úÖ Validation Framework

The Battle Manual includes comprehensive validation across 8 categories:

1. **Input Layer Validation**: Silver data integrity and structure
2. **Unified Grain Validation**: Grouping consistency and coverage
3. **Core Metrics Validation**: Calculation accuracy for all 20 metrics
4. **DuckDB Implementation Validation**: SQL correctness and performance
5. **Panel Export Validation**: Output quality and compliance
6. **End-to-End Validation**: Complete workflow testing
7. **Statistical Accuracy Validation**: Wilson CI, governance, effective_n
8. **Governance Compliance Validation**: Mandatory fields and standards

### Running Validation
```bash
# Comprehensive validation
python run_battle_manual.py --validate

# Validation with detailed output
python run_battle_manual.py --validate --verbose
```

### Validation Results
Validation generates a comprehensive report including:
- Pass/fail status for each category
- Detailed metrics and statistics
- Error descriptions and recommendations
- Overall success rate and compliance

## üìä Output Specifications

### Entity Panel (entity_panel.jsonl)
**Purpose**: Production insights for downstream consumers  
**Limit**: ‚â§3,000 rows  
**Filter**: CONFIDENT governance priority  

**Record Structure**:
```json
{
  "patch_version": "25.18",
  "champion": "Jinx", 
  "role": "ADC",
  "entity_type": "hero",
  "pick_rate": 0.12,
  "winrate": 0.52,
  "winrate_ci_lo": 0.48,
  "winrate_ci_hi": 0.56,
  "winrate_delta_vs_baseline": 0.03,
  "kda_adj": 2.1,
  "dpm": 580.5,
  "obj_rate": 0.65,
  "cp_25": 2450.0,
  "delta_cp": 150.0,
  "n": 150,
  "effective_n": 150.0,
  "governance_tag": "CONFIDENT",
  "uses_prior": false,
  "metric_version": "efp_v1.0",
  "generated_at": "2024-09-29T...",
  "data_quality_score": 0.95,
  "meta_strength_score": 85.3
}
```

### Context Panel (context_panel.jsonl)
**Purpose**: Research insights for analysis and exploration  
**Filter**: CAUTION/CONTEXT governance included  

**Record Structure**:
```json
{
  "patch_version": "25.18",
  "entity_id": "winrate_Yasuo_MID_low_sample",
  "analysis_type": "winrate_robustness_caution",
  "metrics": {
    "value": 0.48,
    "n": 35,
    "effective_n": 24.5,
    "ci_lo": 0.38,
    "ci_hi": 0.58
  },
  "sample_size": 35,
  "confidence_level": "CAUTION",
  "research_notes": "Lower confidence winrate metric for research purposes",
  "limitations": ["Small sample size", "Lower statistical confidence"]
}
```

### Patch Summary (patch_summary.jsonl)
**Purpose**: Version comparison and meta overview  
**Aggregation**: Patch-level insights  

**Record Structure**:
```json
{
  "patch_version": "25.18",
  "total_entities_analyzed": 1250,
  "total_games_processed": 53530,
  "avg_data_quality_score": 0.89,
  "confident_entities_count": 890,
  "overall_game_duration_avg": 28.5,
  "patch_shock_score": 0.15,
  "strongest_champions": [
    {"champion": "Jinx", "role": "ADC", "cp_25": 2450, "delta_cp": 150},
    {"champion": "Azir", "role": "MID", "cp_25": 2380, "delta_cp": 120}
  ],
  "biggest_winners": [
    {"champion": "Braum", "role": "SUPPORT", "winrate_delta": 0.13},
    {"champion": "Graves", "role": "JUNGLE", "winrate_delta": 0.08}
  ],
  "role_meta_summary": {
    "ADC": {"avg_winrate": 0.502, "meta_diversity": 0.8},
    "MID": {"avg_winrate": 0.498, "meta_diversity": 0.9}
  }
}
```

## üõ†Ô∏è Development Guide

### Adding New Metrics
1. **Define metric in registry** (`core_metrics_engine.py`)
2. **Implement calculation logic** in appropriate category method
3. **Add validation tests** in `validation_framework.py`
4. **Update documentation** and priority classification

### Extending SQL Analysis
1. **Create new SQL file** in `sql/` directory
2. **Follow Battle Manual grain structure** (champion, role, patch)
3. **Include mandatory governance fields** (n, effective_n, governance_tag, etc.)
4. **Add execution logic** to `run_battle_manual.py --sql-only`

### Custom Panel Exports
1. **Define new record structure** (dataclass in `panels_export_engine.py`)
2. **Implement generation logic** with governance filtering
3. **Add export method** to `PanelsExportEngine`
4. **Update validation** to test new panel type

### Integration with Existing Framework
The Battle Manual integrates seamlessly with existing Rift Rewind components:

- **Silver Layer**: Direct consumption of existing fact tables
- **Quantitative Metrics**: Extension of 20/20 framework completion
- **Dimension Tables**: Utilizes existing DimStatWeights, DimItemPassive, etc.
- **Shock Analysis**: Integrates with existing patch quantification
- **Configuration**: Uses existing `user_mode_params.yml` structure

## üìà Performance & Scalability

### Processing Capacity
- **Current**: 107,570 Silver layer records across 3 patches
- **Memory**: Optimized for <4GB RAM usage
- **Processing**: ~2-5 minutes for complete workflow
- **Output**: 3 panel files + comprehensive reports

### Scaling Considerations
- **Horizontal**: Patch-level parallelization supported
- **Vertical**: DuckDB enables larger-than-memory processing
- **Storage**: JSONL format for streaming and incremental processing
- **Caching**: Intermediate results cached for iteration

## üîç Troubleshooting

### Common Issues

**Issue**: `FileNotFoundError: Silver layer data not found`  
**Solution**: Ensure `--silver-data` path points to directory containing `fact_match_performance_patch_*.json` files

**Issue**: `ValidationError: Wilson CI calculation failed`  
**Solution**: Check for sufficient sample sizes (n‚â•10) and valid win/loss ratios

**Issue**: `DuckDBError: Table not found`  
**Solution**: Verify Silver layer data format and column names match expected schema

**Issue**: `MemoryError: Out of memory`  
**Solution**: Use `--priority immediate` for lighter processing or increase system memory

### Debug Mode
```bash
# Enable verbose logging for debugging
python run_battle_manual.py --quick-test --verbose

# Run validation to identify issues
python run_battle_manual.py --validate --verbose

# Check dry run for execution plan
python run_battle_manual.py --dry-run --patches 25.18
```

## üìö References

- **Battle Manual Specification**: Original user requirements document
- **20/20 Quantitative Metrics**: Base framework implementation
- **Silver Layer Schema**: Data model documentation
- **Wilson Confidence Intervals**: Statistical methodology
- **DuckDB Documentation**: SQL engine reference

---

## üéâ Success Metrics

The Battle Manual implementation achieves:

- **‚úÖ 100% Framework Completion**: All 6 steps implemented
- **‚úÖ 20/20 Metrics Coverage**: Complete quantitative framework
- **‚úÖ Production Quality**: Comprehensive validation and error handling
- **‚úÖ Scalable Architecture**: Modular, extensible design
- **‚úÖ Universal Output**: Interpretable, traceable quantitative panels

**Ready for version comparison, combo discovery, and user reports.**

---

*Generated by Battle Manual v1.0 - The definitive quantitative analysis workflow for League of Legends*