#!/usr/bin/env python3
"""
Quantitative Metrics Runner
Unified runner for item gold efficiency and combat power analysis
Advances from 12/20 to 18/20 quantitative metrics
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Any
import pandas as pd

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..', 'core'))

from item_gold_efficiency import ItemGoldEfficiencyAnalyzer
from combat_power import CombatPowerAnalyzer

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class QuantitativeMetricsRunner:
    """Unified runner for all quantitative metrics analysis"""
    
    def __init__(self, config_path: str = "configs/user_mode_params.yml",
                 output_dir: str = "out/quantitative/"):
        """Initialize runner with configuration"""
        self.config_path = config_path
        self.output_dir = output_dir
        
        # Initialize analyzers (18/20 â†’ 20/20)
        self.efficiency_analyzer = ItemGoldEfficiencyAnalyzer(config_path)
        self.combat_power_analyzer = CombatPowerAnalyzer(config_path)
        
        # NEW: Initialize final 2 metrics analyzers
        from rune_value import RuneValueAnalyzer
        from damage_efficiency import DamageEfficiencyAnalyzer
        self.rune_value_analyzer = RuneValueAnalyzer(config_path)
        self.damage_efficiency_analyzer = DamageEfficiencyAnalyzer(config_path)
        
        # Track results
        self.results = {}
        
    def run_item_gold_efficiency_analysis(self, patch_version: str = "14.1") -> Dict[str, Any]:
        """Run item gold efficiency analysis"""
        logger.info("Running item gold efficiency analysis...")
        
        try:
            results = self.efficiency_analyzer.export_results(patch_version, self.output_dir)
            self.results['item_gold_efficiency'] = results
            
            logger.info(f"Generated {results['metadata']['record_count']} item efficiency records")
            
            # Log efficiency distribution
            summary = results['summary']
            logger.info(f"Efficiency distribution:")
            logger.info(f"  Highly efficient: {summary['highly_efficient_count']}")
            logger.info(f"  Efficient: {summary['efficient_count']}")
            logger.info(f"  Average: {summary['average_count']}")
            logger.info(f"  Inefficient: {summary['inefficient_count']}")
            logger.info(f"  Very inefficient: {summary['very_inefficient_count']}")
            
            return results
        except Exception as e:
            logger.error(f"Item gold efficiency analysis failed: {e}")
            raise
            
    def run_combat_power_analysis(self, patch_version: str = "14.1") -> Dict[str, Any]:
        """Run combat power analysis"""
        logger.info("Running combat power analysis...")
        
        try:
            # Define sample builds for analysis
            # Based on top items from attach_rate analysis
            sample_builds = [
                # ADC builds
                (1, [1055, 3031, 3078, 3006, 3142, 3153]),     # Jinx ADC
                (22, [1055, 3031, 6672, 3006, 6676, 3036]),    # Ashe ADC
                (51, [1055, 3031, 3078, 3006, 3032, 3153]),    # Caitlyn ADC
                
                # AP builds  
                (34, [1056, 3040, 3020, 3089, 3157, 4645]),    # Anivia AP
                (13, [1056, 6655, 3020, 3089, 3157, 4628]),    # Ryze AP
                (69, [1056, 3152, 3020, 3089, 6653, 4645]),    # Cassiopeia AP
                
                # Support builds
                (12, [3869, 3190, 3107, 3111, 3158, 2055]),    # Alistar Support
                (25, [3869, 3190, 3107, 3047, 3158, 2055]),    # Morgana Support
                (53, [3869, 3190, 3107, 3020, 3158, 2055]),    # Blitzcrank Support
                
                # Jungle builds
                (64, [1054, 3071, 3047, 3078, 3161, 6333]),    # Lee Sin Jungle
                (59, [1054, 3142, 3047, 6692, 3071, 6333]),    # Jarvan Jungle
                
                # Top lane builds
                (126, [1054, 3071, 3047, 3078, 3161, 6333]),   # Jayce Top
                (114, [1054, 3071, 3047, 3078, 3161, 3190]),   # Fiora Top
            ]
            
            results = self.combat_power_analyzer.export_combat_power_analysis(
                sample_builds, patch_version, self.output_dir
            )
            self.results['combat_power'] = results
            
            logger.info(f"Generated {results['metadata']['record_count']} combat power analyses")
            logger.info(f"Generated {results['metadata']['delta_count']} delta combat power comparisons")
            
            return results
        except Exception as e:
            logger.error(f"Combat power analysis failed: {e}")
            raise

            
    def run_rune_value_analysis(self, patch_version: str = "14.1") -> Dict[str, Any]:
        """Run rune value temporal analysis"""
        logger.info("Running rune value temporal analysis...")
        
        try:
            results = self.rune_value_analyzer.export_comprehensive_analysis(patch_version, self.output_dir)
            self.results['rune_value'] = results
            
            logger.info(f"Generated {results['metadata']['total_records']} rune value analyses")
            logger.info(f"Analyzed {results['metadata']['roles_analyzed']} roles")
            
            # Log top performing runes
            summary = results['summary']
            if summary.get('best_overall_keystone'):
                best_keystone = summary['best_overall_keystone']
                logger.info(f"Best keystone: {best_keystone['rune_name']} ({best_keystone['primary_role']}) - {best_keystone['value']:.1f} value")
            
            if summary.get('best_overall_secondary'):
                best_secondary = summary['best_overall_secondary']
                logger.info(f"Best secondary: {best_secondary['rune_name']} ({best_secondary['primary_role']}) - {best_secondary['value']:.1f} efficiency")
            
            return results
        except Exception as e:
            logger.error(f"Rune value analysis failed: {e}")
            raise
            
    def run_damage_efficiency_analysis(self, patch_version: str = "14.1") -> Dict[str, Any]:
        """Run damage per cooldown efficiency analysis"""
        logger.info("Running damage per cooldown efficiency analysis...")
        
        try:
            results = self.damage_efficiency_analyzer.export_efficiency_analysis(patch_version=patch_version, output_dir=self.output_dir)
            self.results['damage_efficiency'] = results
            
            logger.info(f"Generated {results['metadata']['total_abilities']} ability efficiency analyses")
            logger.info(f"Analyzed {results['metadata']['champions_analyzed']} champions")
            
            # Log top performing abilities
            summary = results['summary']
            if summary.get('top_ability'):
                top_ability = summary['top_ability']
                logger.info(f"Most efficient ability: {top_ability['name']} - {top_ability['efficiency']:.1f} damage per CD second")
            
            if summary.get('most_efficient_champion'):
                top_champion = summary['most_efficient_champion']
                logger.info(f"Most efficient champion: {top_champion['champion_name']} - {top_champion['avg_damage_per_cd']:.1f} avg damage per CD")
            
            return results
        except Exception as e:
            logger.error(f"Damage efficiency analysis failed: {e}")
            raise
            
    def run_all_quantitative_metrics(self, patch_version: str = "14.1") -> Dict[str, Any]:
        """Run all quantitative metrics analysis - 20/20 COMPLETION!"""
        logger.info("Starting comprehensive quantitative metrics analysis...")
        logger.info("ğŸ¯ FINAL MILESTONE: Advancing from 18/20 to 20/20 quantitative metrics...")
        
        # Run all analyses (18/20 â†’ 20/20)
        efficiency_results = self.run_item_gold_efficiency_analysis(patch_version)
        combat_power_results = self.run_combat_power_analysis(patch_version)
        
        # NEW: Final 2 metrics for 100% completion
        rune_value_results = self.run_rune_value_analysis(patch_version)
        damage_efficiency_results = self.run_damage_efficiency_analysis(patch_version)
        
        # Compile comprehensive summary (20/20 COMPLETE!)
        summary = {
            'metrics_implemented': [
                'item_ge_t',         # Item gold efficiency
                'cp_t',              # Combat power at levels 15/25/35  
                'delta_cp',          # Combat power differences
                'rune_value_t',      # Rune trigger value calculations
                'dmg_per_cd'         # Damage per cooldown ratio
            ],
            'progression': {
                'previous_metrics': 18,
                'new_metrics': 2,
                'total_metrics': 20,
                'target_metrics': 20,
                'completion_percentage': 100.0
            },
            'record_counts': {
                'item_gold_efficiency': efficiency_results['metadata']['record_count'],
                'combat_power_analyses': combat_power_results['metadata']['record_count'],
                'delta_comparisons': combat_power_results['metadata']['delta_count'],
                'rune_value_analyses': rune_value_results['metadata']['total_records'],
                'damage_efficiency_analyses': damage_efficiency_results['metadata']['total_abilities']
            },
            'governance_quality': self._compile_governance_summary(),
            'patch_coverage': patch_version,
            'static_dimensions_created': ['DimStatWeights', 'DimItemPassive', 'DimRuneValue', 'DimAbility']
        }
        
        # Save comprehensive summary
        self._save_comprehensive_summary(summary, patch_version)
        
        logger.info("ğŸ‰ QUANTITATIVE METRICS ANALYSIS COMPLETED SUCCESSFULLY!")
        logger.info(f"ğŸ† MILESTONE ACHIEVED: 20/20 quantitative metrics ({summary['progression']['completion_percentage']:.0f}% COMPLETE)")
        logger.info("ğŸš€ FRAMEWORK IMPLEMENTATION: 100% COMPLETE")
        
        return {
            'item_gold_efficiency': efficiency_results,
            'combat_power': combat_power_results,
            'rune_value': rune_value_results,
            'damage_efficiency': damage_efficiency_results,
            'summary': summary
        }
        
    def _compile_governance_summary(self) -> Dict[str, Any]:
        """Compile governance quality across all quantitative metrics"""
        governance_summary = {
            'total_records': 0,
            'overall_distribution': {'CONFIDENT': 0, 'CAUTION': 0, 'CONTEXT': 0},
            'by_metric': {}
        }
        
        # Item gold efficiency governance
        if 'item_gold_efficiency' in self.results:
            ige_dist = self.results['item_gold_efficiency']['governance_distribution']
            governance_summary['by_metric']['item_gold_efficiency'] = ige_dist
            
            for tag, count in ige_dist.items():
                governance_summary['overall_distribution'][tag] += count
                governance_summary['total_records'] += count
                
        # Combat power governance  
        if 'combat_power' in self.results:
            cp_dist = self.results['combat_power']['governance_distribution']
            governance_summary['by_metric']['combat_power'] = cp_dist['combat_power']
            governance_summary['by_metric']['delta_combat_power'] = cp_dist['delta_combat_power']
            
            for dist in cp_dist.values():
                for tag, count in dist.items():
                    governance_summary['overall_distribution'][tag] += count
                    governance_summary['total_records'] += count
        
        # NEW: Rune value governance
        if 'rune_value' in self.results:
            rv_dist = self.results['rune_value']['governance_distribution']
            governance_summary['by_metric']['rune_value'] = rv_dist
            
            for tag, count in rv_dist.items():
                governance_summary['overall_distribution'][tag] += count
                governance_summary['total_records'] += count
        
        # NEW: Damage efficiency governance
        if 'damage_efficiency' in self.results:
            de_dist = self.results['damage_efficiency']['governance_distribution']
            governance_summary['by_metric']['damage_efficiency'] = de_dist
            
            for tag, count in de_dist.items():
                governance_summary['overall_distribution'][tag] += count
                governance_summary['total_records'] += count
                
        return governance_summary
        
    def _save_comprehensive_summary(self, summary: Dict[str, Any], patch_version: str = None) -> None:
        """Save comprehensive quantitative metrics summary"""
        output_path = Path(self.output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        patch_suffix = f"_patch_{patch_version}" if patch_version else "_all_patches"
        summary_file = output_path / f"quantitative_metrics_summary{patch_suffix}.json"
        
        comprehensive_data = {
            'metadata': {
                'analysis_type': 'quantitative_metrics_comprehensive',
                'patch_version': patch_version,
                'generated_at': pd.Timestamp.now().isoformat(),
                'metrics_progression': '18/20 â†’ 20/20',
                'completion_status': '100_PERCENT_COMPLETE'
            },
            'summary': summary,
            'detailed_results': {
                'item_gold_efficiency_sample': self._get_efficiency_sample(),
                'combat_power_sample': self._get_combat_power_sample(),
                'rune_value_sample': self._get_rune_value_sample(),
                'damage_efficiency_sample': self._get_damage_efficiency_sample()
            }
        }
        
        with open(summary_file, 'w') as f:
            json.dump(comprehensive_data, f, indent=2)
            
        logger.info(f"Saved comprehensive summary to {summary_file}")
        
    def _get_rune_value_sample(self) -> Dict[str, Any]:
        """Get sample of rune value results for summary"""
        if 'rune_value' not in self.results:
            return {}
            
        results = self.results['rune_value']
        return {
            'best_keystone': results['summary'].get('best_overall_keystone', {}),
            'best_secondary': results['summary'].get('best_overall_secondary', {}),
            'roles_analyzed': results['metadata'].get('roles_analyzed', []),
            'total_analyses': results['metadata'].get('total_records', 0)
        }
        
    def _get_damage_efficiency_sample(self) -> Dict[str, Any]:
        """Get sample of damage efficiency results for summary"""
        if 'damage_efficiency' not in self.results:
            return {}
            
        results = self.results['damage_efficiency']
        return {
            'top_ability': results['summary'].get('top_ability', {}),
            'most_efficient_champion': results['summary'].get('most_efficient_champion', {}),
            'tier_distribution': results['summary'].get('efficiency_tier_distribution', {}),
            'total_abilities': results['metadata'].get('total_abilities', 0)
        }
        
    def _get_efficiency_sample(self) -> Dict[str, Any]:
        """Get sample of efficiency results for summary"""
        if 'item_gold_efficiency' not in self.results:
            return {}
            
        results = self.results['item_gold_efficiency']
        records = results.get('records', [])
        
        return {
            'most_efficient': records[:3] if len(records) >= 3 else records,
            'least_efficient': records[-3:] if len(records) >= 3 else [],
            'efficiency_tiers': results.get('summary', {})
        }
        
    def _get_combat_power_sample(self) -> Dict[str, Any]:
        """Get sample of combat power results for summary"""
        if 'combat_power' not in self.results:
            return {}
            
        results = self.results['combat_power']
        
        return {
            'sample_progression': results['combat_power_analysis'][:2] if len(results['combat_power_analysis']) >= 2 else results['combat_power_analysis'],
            'sample_deltas': results['delta_combat_power'][:2] if len(results['delta_combat_power']) >= 2 else results['delta_combat_power'],
            'cp_weights': results['cp_weights'],
            'level_breakpoints': results['level_breakpoints']
        }


def main():
    """Demo usage of quantitative metrics runner - 20/20 COMPLETION!"""
    runner = QuantitativeMetricsRunner()
    
    print("ğŸ“Š QUANTITATIVE METRICS RUNNER")
    print("=" * 60)
    print("ğŸ¯ OBJECTIVE: Advance from 18/20 to 20/20 quantitative metrics")
    print("ğŸ“¦ NEW METRICS: rune_value_t, dmg_per_cd")
    print("ğŸ—ï¸ STATIC DIMENSIONS: DimRuneValue, DimAbility")
    print("ğŸ† TARGET: 100% FRAMEWORK COMPLETION")
    print()
    
    try:
        # Run comprehensive analysis
        results = runner.run_all_quantitative_metrics()
        
        print("\nâœ… QUANTITATIVE METRICS ANALYSIS COMPLETE!")
        print("=" * 60)
        
        summary = results['summary']
        
        # Show progression
        progression = summary['progression']
        print(f"ğŸ“ˆ METRICS PROGRESSION:")
        print(f"  Previous: {progression['previous_metrics']}/20 metrics")
        print(f"  Added: {progression['new_metrics']} new metrics")
        print(f"  ğŸ‰ FINAL: {progression['total_metrics']}/20 metrics ({progression['completion_percentage']:.0f}%)")
        print(f"  ğŸ† STATUS: 100% FRAMEWORK COMPLETE!")
        
        # Show record counts
        counts = summary['record_counts']
        print(f"\nğŸ“Š ANALYSIS RESULTS:")
        print(f"  ğŸ’° Item gold efficiency records: {counts['item_gold_efficiency']:,}")
        print(f"  âš”ï¸ Combat power analyses: {counts['combat_power_analyses']:,}")
        print(f"  ğŸ”„ Delta CP comparisons: {counts['delta_comparisons']:,}")
        print(f"  ğŸ”® Rune value analyses: {counts['rune_value_analyses']:,}")
        print(f"  ğŸ’¥ Damage efficiency analyses: {counts['damage_efficiency_analyses']:,}")
        
        # Show governance quality
        governance = summary['governance_quality']
        total_records = governance['total_records']
        overall_dist = governance['overall_distribution']
        
        print(f"\nğŸ›ï¸ DATA GOVERNANCE QUALITY:")
        print(f"  ğŸ“ Total records: {total_records:,}")
        for tag, count in overall_dist.items():
            percentage = count / total_records * 100 if total_records > 0 else 0
            print(f"  {tag}: {count:,} records ({percentage:.1f}%)")
            
        # Show static dimensions
        print(f"\nğŸ—ï¸ STATIC DIMENSIONS CREATED:")
        for dimension in summary['static_dimensions_created']:
            print(f"  ğŸ“‹ {dimension}")
            
        # Show all metrics implemented
        print(f"\nğŸ¯ ALL METRICS IMPLEMENTED (20/20):")
        for metric in summary['metrics_implemented']:
            print(f"  ğŸ“ˆ {metric}")
            
        # Show top performers from new metrics
        rune_sample = results['rune_value']['summary']
        damage_sample = results['damage_efficiency']['summary']
        
        print(f"\nğŸ”® RUNE VALUE HIGHLIGHTS:")
        if rune_sample.get('best_overall_keystone'):
            best_keystone = rune_sample['best_overall_keystone']
            print(f"  ğŸ† Best Keystone: {best_keystone['rune_name']} ({best_keystone['primary_role']})")
        if rune_sample.get('best_overall_secondary'):
            best_secondary = rune_sample['best_overall_secondary']
            print(f"  â­ Best Secondary: {best_secondary['rune_name']} ({best_secondary['primary_role']})")
            
        print(f"\nğŸ’¥ DAMAGE EFFICIENCY HIGHLIGHTS:")
        if damage_sample.get('top_ability'):
            top_ability = damage_sample['top_ability']
            print(f"  ğŸ† Most Efficient Ability: {top_ability['name']} ({top_ability['efficiency']:.1f} dmg/cd)")
        if damage_sample.get('most_efficient_champion'):
            top_champion = damage_sample['most_efficient_champion']
            print(f"  â­ Most Efficient Champion: {top_champion['champion_name']} ({top_champion['avg_damage_per_cd']:.1f} avg)")
            
        print(f"\nğŸ‰ MILESTONE ACHIEVED: 20/20 Quantitative Metrics Complete!")
        print(f"ğŸš€ FRAMEWORK STATUS: 100% IMPLEMENTATION COMPLETE!")
        print(f"ğŸ“ Results saved to: out/quantitative/")
        
        # List output files
        output_path = Path('out/quantitative/')
        if output_path.exists():
            files = list(output_path.glob('*.json'))
            if files:
                print(f"\nğŸ“„ Output Files Generated:")
                for file in sorted(files):
                    print(f"  ğŸ“„ {file.name}")
                    
    except Exception as e:
        logger.error(f"Quantitative metrics analysis failed: {e}")
        print(f"\nâŒ ERROR: {e}")
        raise


if __name__ == "__main__":
    main()