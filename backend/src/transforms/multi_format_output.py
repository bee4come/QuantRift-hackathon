#!/usr/bin/env python3
"""
Multi-Format Output Pipeline
æ„å»ºParquet+DuckDBå¤šæ ¼å¼è¾“å‡ºç®¡é“ï¼Œæ”¯æŒé«˜æ•ˆå­˜å‚¨å’ŒæŸ¥è¯¢
"""

import json
import os
from pathlib import Path
from datetime import datetime, timezone
from typing import Dict, List, Optional, Any
import pandas as pd
import duckdb
import pyarrow as pa
import pyarrow.parquet as pq
from dataclasses import asdict
import shutil

class MultiFormatOutputPipeline:
    """å¤šæ ¼å¼è¾“å‡ºç®¡é“"""

    def __init__(self,
                 silver_dir: str = "data/silver",
                 gold_dir: str = "data/gold",
                 formats: List[str] = None):

        self.silver_dir = Path(silver_dir)
        self.gold_dir = Path(gold_dir)
        self.gold_dir.mkdir(parents=True, exist_ok=True)

        # æ”¯æŒçš„è¾“å‡ºæ ¼å¼
        self.formats = formats or ["parquet", "duckdb", "json", "csv"]

        # åˆ›å»ºæ ¼å¼ç‰¹å®šç›®å½•
        for fmt in self.formats:
            (self.gold_dir / fmt).mkdir(parents=True, exist_ok=True)

        # DuckDBè¿æ¥
        self.db_path = self.gold_dir / "duckdb" / "analytics.duckdb"
        self.conn = None

        print(f"ğŸ”„ åˆå§‹åŒ–å¤šæ ¼å¼è¾“å‡ºç®¡é“")
        print(f"ğŸ“ Silverå±‚: {self.silver_dir}")
        print(f"ğŸ“ Goldå±‚: {self.gold_dir}")
        print(f"ğŸ“¦ æ”¯æŒæ ¼å¼: {self.formats}")

    def _init_duckdb(self):
        """åˆå§‹åŒ–DuckDBè¿æ¥"""
        if self.conn is None:
            self.conn = duckdb.connect(str(self.db_path))
            print(f"ğŸ”— DuckDBè¿æ¥: {self.db_path}")

    def _close_duckdb(self):
        """å…³é—­DuckDBè¿æ¥"""
        if self.conn:
            self.conn.close()
            self.conn = None

    def load_silver_data(self) -> Dict[str, List[Dict]]:
        """åŠ è½½Silverå±‚æ•°æ®"""
        print("ğŸ“Š åŠ è½½Silverå±‚æ•°æ®...")

        data_sources = {}

        # åŠ è½½SCD2ç»´è¡¨æ•°æ®
        dimensions_dir = self.silver_dir / "dimensions"
        if dimensions_dir.exists():
            for dim_file in dimensions_dir.glob("*.json"):
                if "summary" not in dim_file.name:
                    with open(dim_file, 'r') as f:
                        data = json.load(f)

                    table_name = dim_file.stem
                    data_sources[table_name] = data.get('records', [])
                    print(f"  ğŸ“‹ ç»´è¡¨ {table_name}: {len(data_sources[table_name])} æ¡è®°å½•")

        # åŠ è½½äº‹å®è¡¨æ•°æ®
        facts_dir = self.silver_dir / "facts"
        if facts_dir.exists():
            fact_records = []
            for fact_file in facts_dir.glob("*.json"):
                if "summary" not in fact_file.name:
                    with open(fact_file, 'r') as f:
                        data = json.load(f)

                    records = data.get('records', [])
                    fact_records.extend(records)

            if fact_records:
                data_sources['fact_match_performance'] = fact_records
                print(f"  ğŸ“Š äº‹å®è¡¨ fact_match_performance: {len(fact_records)} æ¡è®°å½•")

        # åŠ è½½å¢å¼ºäº‹å®è¡¨æ•°æ®
        enhanced_facts_dir = self.silver_dir / "enhanced_facts"
        if enhanced_facts_dir.exists():
            enhanced_records = []
            for fact_file in enhanced_facts_dir.glob("*.json"):
                if "governance" not in fact_file.name:
                    with open(fact_file, 'r') as f:
                        data = json.load(f)

                    records = data.get('records', [])
                    enhanced_records.extend(records)

            if enhanced_records:
                data_sources['enhanced_fact_match_performance'] = enhanced_records
                print(f"  ğŸ›¡ï¸ å¢å¼ºäº‹å®è¡¨: {len(enhanced_records)} æ¡è®°å½•")

        return data_sources

    def convert_to_parquet(self, data_sources: Dict[str, List[Dict]]):
        """è½¬æ¢ä¸ºParquetæ ¼å¼"""
        print("\nğŸ“¦ è½¬æ¢ä¸ºParquetæ ¼å¼...")

        parquet_dir = self.gold_dir / "parquet"

        for table_name, records in data_sources.items():
            if not records:
                continue

            try:
                # è½¬æ¢ä¸ºDataFrame
                df = pd.DataFrame(records)

                # ä¼˜åŒ–æ•°æ®ç±»å‹
                df = self._optimize_dataframe_types(df, table_name)

                # ä¿å­˜ä¸ºParquet
                parquet_file = parquet_dir / f"{table_name}.parquet"
                df.to_parquet(parquet_file, index=False, compression='snappy')

                # éªŒè¯æ–‡ä»¶
                file_size = parquet_file.stat().st_size / 1024 / 1024  # MB
                print(f"  âœ… {table_name}.parquet: {len(records)} æ¡è®°å½•, {file_size:.1f}MB")

            except Exception as e:
                print(f"  âŒ {table_name} Parquetè½¬æ¢å¤±è´¥: {e}")

    def _optimize_dataframe_types(self, df: pd.DataFrame, table_name: str) -> pd.DataFrame:
        """ä¼˜åŒ–DataFrameæ•°æ®ç±»å‹ä»¥å‡å°‘å­˜å‚¨ç©ºé—´"""

        # é€šç”¨ä¼˜åŒ–è§„åˆ™
        for col in df.columns:
            if df[col].dtype == 'object':
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                if df[col].str.match(r'^\d+$').all() if not df[col].isna().all() else False:
                    df[col] = pd.to_numeric(df[col], errors='ignore')
                # å°è¯•è½¬æ¢ä¸ºç±»åˆ«ç±»å‹ï¼ˆå¯¹äºé‡å¤å€¼å¤šçš„åˆ—ï¼‰
                elif df[col].nunique() / len(df) < 0.5:
                    df[col] = df[col].astype('category')

            # ä¼˜åŒ–æ•´æ•°ç±»å‹
            elif df[col].dtype in ['int64']:
                if df[col].min() >= 0:
                    if df[col].max() <= 255:
                        df[col] = df[col].astype('uint8')
                    elif df[col].max() <= 65535:
                        df[col] = df[col].astype('uint16')
                    elif df[col].max() <= 4294967295:
                        df[col] = df[col].astype('uint32')
                else:
                    if df[col].min() >= -128 and df[col].max() <= 127:
                        df[col] = df[col].astype('int8')
                    elif df[col].min() >= -32768 and df[col].max() <= 32767:
                        df[col] = df[col].astype('int16')

            # ä¼˜åŒ–æµ®ç‚¹ç±»å‹
            elif df[col].dtype == 'float64':
                df[col] = pd.to_numeric(df[col], downcast='float')

        # è¡¨ç‰¹å®šä¼˜åŒ–
        if table_name.startswith('fact_'):
            # äº‹å®è¡¨ç‰¹å®šä¼˜åŒ–
            boolean_cols = ['win', 'game_ended_early', 'surrender',
                           'anonymization_validated', 'pii_detection_passed', 'gdpr_compliant']
            for col in boolean_cols:
                if col in df.columns:
                    df[col] = df[col].astype('bool')

            # ç±»åˆ«åˆ—ä¼˜åŒ–
            category_cols = ['tier', 'position', 'champion_name', 'game_mode', 'risk_level']
            for col in category_cols:
                if col in df.columns:
                    df[col] = df[col].astype('category')

        return df

    def load_into_duckdb(self, data_sources: Dict[str, List[Dict]]):
        """åŠ è½½æ•°æ®åˆ°DuckDB"""
        print("\nğŸ¦† åŠ è½½æ•°æ®åˆ°DuckDB...")

        self._init_duckdb()

        for table_name, records in data_sources.items():
            if not records:
                continue

            try:
                # åˆ›å»ºDataFrame
                df = pd.DataFrame(records)
                df = self._optimize_dataframe_types(df, table_name)

                # åˆ é™¤è¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                self.conn.execute(f"DROP TABLE IF EXISTS {table_name}")

                # åˆ›å»ºè¡¨å¹¶æ’å…¥æ•°æ®
                self.conn.register(f"{table_name}_df", df)
                self.conn.execute(f"CREATE TABLE {table_name} AS SELECT * FROM {table_name}_df")

                # éªŒè¯æ•°æ®
                row_count = self.conn.execute(f"SELECT COUNT(*) FROM {table_name}").fetchone()[0]
                print(f"  âœ… {table_name}: {row_count} æ¡è®°å½•")

            except Exception as e:
                print(f"  âŒ {table_name} DuckDBåŠ è½½å¤±è´¥: {e}")

    def create_analytics_views(self):
        """åˆ›å»ºåˆ†æè§†å›¾"""
        print("\nğŸ“ˆ åˆ›å»ºåˆ†æè§†å›¾...")

        if not self.conn:
            return

        # åˆ›å»ºæ€§èƒ½åˆ†æè§†å›¾
        performance_view = """
        CREATE OR REPLACE VIEW player_performance_summary AS
        SELECT
            tier,
            position,
            champion_name,
            COUNT(*) as games_played,
            AVG(kills) as avg_kills,
            AVG(deaths) as avg_deaths,
            AVG(assists) as avg_assists,
            AVG(kda_ratio) as avg_kda,
            AVG(gold_per_minute) as avg_gpm,
            AVG(cs_per_minute) as avg_cspm,
            AVG(vision_score_per_minute) as avg_vspm,
            AVG(damage_per_minute) as avg_dpm,
            SUM(CASE WHEN win THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as win_rate
        FROM fact_match_performance
        GROUP BY tier, position, champion_name
        HAVING games_played >= 10
        ORDER BY tier, avg_kda DESC
        """

        try:
            self.conn.execute(performance_view)
            print("  âœ… player_performance_summary è§†å›¾")
        except Exception as e:
            print(f"  âŒ æ€§èƒ½åˆ†æè§†å›¾åˆ›å»ºå¤±è´¥: {e}")

        # åˆ›å»ºæ²»ç†è´¨é‡è§†å›¾ï¼ˆå¦‚æœæœ‰å¢å¼ºæ•°æ®ï¼‰
        if self.conn.execute("SELECT COUNT(*) FROM information_schema.tables WHERE table_name = 'enhanced_fact_match_performance'").fetchone()[0] > 0:
            governance_view = """
            CREATE OR REPLACE VIEW data_quality_summary AS
            SELECT
                tier,
                patch_version,
                COUNT(*) as total_records,
                AVG(data_quality_score) as avg_quality_score,
                AVG(completeness_score) as avg_completeness,
                AVG(accuracy_score) as avg_accuracy,
                AVG(consistency_score) as avg_consistency,
                SUM(CASE WHEN gdpr_compliant THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as compliance_rate,
                SUM(CASE WHEN risk_level = 'LOW' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as low_risk_rate
            FROM enhanced_fact_match_performance
            GROUP BY tier, patch_version
            ORDER BY patch_version, tier
            """

            try:
                self.conn.execute(governance_view)
                print("  âœ… data_quality_summary è§†å›¾")
            except Exception as e:
                print(f"  âŒ æ²»ç†è´¨é‡è§†å›¾åˆ›å»ºå¤±è´¥: {e}")

        # åˆ›å»ºè¡¥ä¸åˆ†æè§†å›¾
        patch_analysis_view = """
        CREATE OR REPLACE VIEW patch_performance_analysis AS
        SELECT
            patch_version,
            tier,
            COUNT(*) as games_in_patch,
            AVG(game_duration_minutes) as avg_game_duration,
            AVG(kills + assists) as avg_kp,
            AVG(gold_per_minute) as avg_gpm,
            COUNT(DISTINCT champion_name) as unique_champions,
            SUM(CASE WHEN kills >= 10 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as carry_game_rate
        FROM fact_match_performance
        GROUP BY patch_version, tier
        ORDER BY patch_version, tier
        """

        try:
            self.conn.execute(patch_analysis_view)
            print("  âœ… patch_performance_analysis è§†å›¾")
        except Exception as e:
            print(f"  âŒ è¡¥ä¸åˆ†æè§†å›¾åˆ›å»ºå¤±è´¥: {e}")

    def export_to_csv(self, data_sources: Dict[str, List[Dict]]):
        """å¯¼å‡ºä¸ºCSVæ ¼å¼"""
        print("\nğŸ“„ å¯¼å‡ºä¸ºCSVæ ¼å¼...")

        csv_dir = self.gold_dir / "csv"

        for table_name, records in data_sources.items():
            if not records:
                continue

            try:
                df = pd.DataFrame(records)
                csv_file = csv_dir / f"{table_name}.csv"
                df.to_csv(csv_file, index=False, encoding='utf-8')

                file_size = csv_file.stat().st_size / 1024 / 1024  # MB
                print(f"  âœ… {table_name}.csv: {len(records)} æ¡è®°å½•, {file_size:.1f}MB")

            except Exception as e:
                print(f"  âŒ {table_name} CSVå¯¼å‡ºå¤±è´¥: {e}")

    def export_analytics_results(self):
        """å¯¼å‡ºåˆ†æç»“æœ"""
        print("\nğŸ“Š å¯¼å‡ºåˆ†æç»“æœ...")

        if not self.conn:
            return

        # è·å–æ‰€æœ‰è§†å›¾
        views = self.conn.execute("""
            SELECT table_name FROM information_schema.tables
            WHERE table_type = 'VIEW'
        """).fetchall()

        results_dir = self.gold_dir / "analytics"
        results_dir.mkdir(exist_ok=True)

        for (view_name,) in views:
            try:
                # å¯¼å‡ºä¸ºCSV
                result_df = self.conn.execute(f"SELECT * FROM {view_name}").df()
                csv_file = results_dir / f"{view_name}.csv"
                result_df.to_csv(csv_file, index=False)

                # å¯¼å‡ºä¸ºJSON
                json_file = results_dir / f"{view_name}.json"
                result_dict = result_df.to_dict('records')
                with open(json_file, 'w') as f:
                    json.dump({
                        'view_name': view_name,
                        'exported_at': datetime.now(timezone.utc).isoformat(),
                        'record_count': len(result_dict),
                        'data': result_dict
                    }, f, indent=2, default=str)

                print(f"  âœ… {view_name}: {len(result_dict)} æ¡è®°å½•")

            except Exception as e:
                print(f"  âŒ {view_name} åˆ†æç»“æœå¯¼å‡ºå¤±è´¥: {e}")

    def generate_metadata(self, data_sources: Dict[str, List[Dict]]):
        """ç”Ÿæˆå…ƒæ•°æ®"""
        print("\nğŸ“‹ ç”Ÿæˆå…ƒæ•°æ®...")

        metadata = {
            'pipeline_metadata': {
                'generated_at': datetime.now(timezone.utc).isoformat(),
                'pipeline_version': '1.0',
                'formats_supported': self.formats,
                'source_data_summary': {}
            },
            'table_schemas': {},
            'data_statistics': {},
            'quality_summary': {}
        }

        # æ”¶é›†è¡¨ç»“æ„å’Œç»Ÿè®¡ä¿¡æ¯
        for table_name, records in data_sources.items():
            if not records:
                continue

            # æ•°æ®ç»Ÿè®¡
            metadata['pipeline_metadata']['source_data_summary'][table_name] = {
                'record_count': len(records),
                'sample_record': records[0] if records else {}
            }

            # å­—æ®µç»Ÿè®¡
            df = pd.DataFrame(records)
            field_stats = {}

            for col in df.columns:
                field_stats[col] = {
                    'type': str(df[col].dtype),
                    'non_null_count': int(df[col].count()),
                    'null_count': int(df[col].isnull().sum()),
                    'unique_values': int(df[col].nunique())
                }

                # æ•°å€¼å­—æ®µé¢å¤–ç»Ÿè®¡
                if df[col].dtype in ['int64', 'float64', 'int32', 'float32']:
                    field_stats[col].update({
                        'min': float(df[col].min()) if not df[col].isna().all() else None,
                        'max': float(df[col].max()) if not df[col].isna().all() else None,
                        'mean': float(df[col].mean()) if not df[col].isna().all() else None
                    })

            metadata['table_schemas'][table_name] = field_stats

        # æ•°æ®è´¨é‡æ‘˜è¦ï¼ˆå¦‚æœæœ‰æ²»ç†æ•°æ®ï¼‰
        if 'enhanced_fact_match_performance' in data_sources:
            enhanced_df = pd.DataFrame(data_sources['enhanced_fact_match_performance'])

            metadata['quality_summary'] = {
                'total_records': len(enhanced_df),
                'avg_data_quality_score': float(enhanced_df['data_quality_score'].mean()),
                'gdpr_compliance_rate': float(enhanced_df['gdpr_compliant'].mean() * 100),
                'risk_distribution': enhanced_df['risk_level'].value_counts().to_dict()
            }

        # ä¿å­˜å…ƒæ•°æ®
        metadata_file = self.gold_dir / "metadata.json"
        with open(metadata_file, 'w') as f:
            json.dump(metadata, f, indent=2, default=str)

        print(f"  âœ… å…ƒæ•°æ®: {metadata_file}")

    def run_multi_format_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„å¤šæ ¼å¼è¾“å‡ºç®¡é“"""
        print("ğŸš€ å¼€å§‹å¤šæ ¼å¼è¾“å‡ºç®¡é“...")

        try:
            # 1. åŠ è½½Silverå±‚æ•°æ®
            data_sources = self.load_silver_data()

            if not data_sources:
                print("âŒ æœªæ‰¾åˆ°Silverå±‚æ•°æ®")
                return

            # 2. è½¬æ¢ä¸ºä¸åŒæ ¼å¼
            if "parquet" in self.formats:
                self.convert_to_parquet(data_sources)

            if "duckdb" in self.formats:
                self.load_into_duckdb(data_sources)
                self.create_analytics_views()
                self.export_analytics_results()

            if "csv" in self.formats:
                self.export_to_csv(data_sources)

            # 3. ç”Ÿæˆå…ƒæ•°æ®
            self.generate_metadata(data_sources)

            # 4. ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
            self._generate_summary_report(data_sources)

            print("âœ… å¤šæ ¼å¼è¾“å‡ºç®¡é“å®Œæˆ!")

        except Exception as e:
            print(f"ğŸ’¥ å¤šæ ¼å¼è¾“å‡ºç®¡é“å¤±è´¥: {e}")
            raise
        finally:
            self._close_duckdb()

    def _generate_summary_report(self, data_sources: Dict[str, List[Dict]]):
        """ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š"""
        print("\nğŸ“‹ ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š...")

        total_records = sum(len(records) for records in data_sources.values())

        summary = {
            'multi_format_pipeline_summary': {
                'completed_at': datetime.now(timezone.utc).isoformat(),
                'total_tables_processed': len(data_sources),
                'total_records_processed': total_records,
                'output_formats': self.formats,
                'output_directory': str(self.gold_dir)
            },
            'table_summary': {
                table_name: len(records)
                for table_name, records in data_sources.items()
            },
            'format_outputs': {
                'parquet': list((self.gold_dir / "parquet").glob("*.parquet")) if "parquet" in self.formats else [],
                'duckdb': str(self.db_path) if "duckdb" in self.formats else None,
                'csv': list((self.gold_dir / "csv").glob("*.csv")) if "csv" in self.formats else [],
                'analytics': list((self.gold_dir / "analytics").glob("*")) if "duckdb" in self.formats else []
            }
        }

        # è®¡ç®—æ–‡ä»¶å¤§å°
        total_size = 0
        for fmt_dir in [self.gold_dir / fmt for fmt in self.formats]:
            if fmt_dir.exists():
                for file_path in fmt_dir.rglob("*"):
                    if file_path.is_file():
                        total_size += file_path.stat().st_size

        summary['storage_summary'] = {
            'total_size_mb': round(total_size / 1024 / 1024, 2),
            'avg_compression_ratio': 'N/A'  # éœ€è¦ä¸åŸå§‹JSONæ¯”è¾ƒ
        }

        summary_file = self.gold_dir / "pipeline_summary.json"
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2, default=str)

        print(f"  âœ… ç®¡é“æ‘˜è¦: {summary_file}")
        print(f"  ğŸ“Š å¤„ç†è¡¨æ•°: {len(data_sources)}")
        print(f"  ğŸ“‹ æ€»è®°å½•æ•°: {total_records:,}")
        print(f"  ğŸ’¾ è¾“å‡ºå¤§å°: {summary['storage_summary']['total_size_mb']:.1f}MB")


def main():
    import argparse

    parser = argparse.ArgumentParser(description="Multi-Format Output Pipeline")
    parser.add_argument("--silver-dir", default="data/silver",
                       help="Silverå±‚æ•°æ®ç›®å½•")
    parser.add_argument("--gold-dir", default="data/gold",
                       help="Goldå±‚è¾“å‡ºç›®å½•")
    parser.add_argument("--formats", nargs='+',
                       choices=['parquet', 'duckdb', 'csv', 'json'],
                       default=['parquet', 'duckdb', 'csv'],
                       help="è¾“å‡ºæ ¼å¼")

    args = parser.parse_args()

    try:
        pipeline = MultiFormatOutputPipeline(
            silver_dir=args.silver_dir,
            gold_dir=args.gold_dir,
            formats=args.formats
        )

        pipeline.run_multi_format_pipeline()
        return 0

    except Exception as e:
        print(f"ğŸ’¥ ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
        return 1


if __name__ == "__main__":
    exit(main())